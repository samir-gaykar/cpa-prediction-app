{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "LtlJ5BA1yKzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "category-encoders\n",
        "optuna\n",
        "xgboost\n",
        "scikit-learn\n",
        "optuna_fast_fanova\n",
        "mlflow"
      ],
      "metadata": {
        "id": "xVMwBm6yoRos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f470d587-4218-4cc6-93c3-9571639a80b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU -r requirements.txt"
      ],
      "metadata": {
        "id": "35mVcke9N5cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "nfMg7bPCyNwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Library Imports\n",
        "import gc\n",
        "import os\n",
        "import re\n",
        "import sqlite3\n",
        "import sys\n",
        "from math import sqrt\n",
        "from pathlib import Path\n",
        "\n",
        "# Third-Party Imports\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import xgboost as xgb\n",
        "from category_encoders import BinaryEncoder, LeaveOneOutEncoder\n",
        "from optuna.importance import get_param_importances\n",
        "from optuna.pruners import HyperbandPruner\n",
        "from optuna.visualization import (\n",
        "    plot_optimization_history,\n",
        "    plot_param_importances\n",
        ")\n",
        "from optuna_fast_fanova import FanovaImportanceEvaluator\n",
        "from scipy import stats\n",
        "from scipy.stats import ks_2samp, mstats, zscore\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.feature_selection import (\n",
        "    f_regression,\n",
        "    mutual_info_regression,\n",
        "    SelectKBest,\n",
        "    VarianceThreshold\n",
        ")\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import (\n",
        "    KFold,\n",
        "    RepeatedKFold,\n",
        "    cross_val_score,\n",
        "    train_test_split\n",
        ")\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import (\n",
        "    LabelEncoder,\n",
        "    MaxAbsScaler,\n",
        "    MinMaxScaler,\n",
        "    Normalizer,\n",
        "    PowerTransformer,\n",
        "    QuantileTransformer,\n",
        "    RobustScaler,\n",
        "    StandardScaler\n",
        ")\n",
        "from sklearn.svm import OneClassSVM\n",
        "from statsmodels.stats.diagnostic import linear_reset\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from xgboost import XGBRegressor\n",
        "import yaml"
      ],
      "metadata": {
        "id": "1Zk5d2grs6hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants\n",
        "\n",
        "We define a `config.yaml` file to externalize key parameters and paths."
      ],
      "metadata": {
        "id": "mBtouIzeXDqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_yaml_content = \"\"\"\n",
        "PATH_PARTS:\n",
        "  - /content\n",
        "  - drive\n",
        "  - MyDrive\n",
        "  - Colab\n",
        "  - Machine Learning\n",
        "  - Regression\n",
        "  - Radancy\n",
        "\n",
        "RANDOM_STATE: 42\n",
        "COMPRESS: 9\n",
        "N_JOBS: -1\n",
        "TARGET: \"cpa\"\n",
        "\"\"\"\n",
        "\n",
        "config_yaml_path = os.path.join(\n",
        "    '/content',\n",
        "    'drive',\n",
        "    'MyDrive',\n",
        "    'Colab',\n",
        "    'Machine Learning',\n",
        "    'Regression',\n",
        "    'Radancy',\n",
        "    'config.yaml'\n",
        ")\n",
        "\n",
        "if os.path.exists(config_yaml_path):\n",
        "  os.remove(config_yaml_path)\n",
        "\n",
        "with open(config_yaml_path, \"w\") as f:\n",
        "  f.write(config_yaml_content)\n",
        "\n",
        "# Load config\n",
        "with open(config_yaml_path, \"r\") as file:\n",
        "  config = yaml.safe_load(file)\n",
        "\n",
        "# Cross-platform path assembly\n",
        "PATH = Path().joinpath(*config[\"PATH_PARTS\"])\n",
        "\n",
        "# Other parameters\n",
        "RANDOM_STATE = config[\"RANDOM_STATE\"]\n",
        "COMPRESS = config[\"COMPRESS\"]\n",
        "N_JOBS = config[\"N_JOBS\"]\n",
        "TARGET = config[\"TARGET\"]"
      ],
      "metadata": {
        "id": "I0W-P7F1Ljts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Custom Utilities Safely\n",
        "\n",
        "We add the project root directory to `sys.path` to enable importing of our own modular code (e.g. ml_utils.py) which contains reusable functions."
      ],
      "metadata": {
        "id": "u-T1W18Gz5lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(str(PATH))"
      ],
      "metadata": {
        "id": "7XKZBzSA2K-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ml_utils import *"
      ],
      "metadata": {
        "id": "pGWaQ-AJxR_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Tracking with MLflow\n",
        "\n",
        "We initialize MLflow with a local file-based tracking URI to systematically log and compare model experiments including parameters, metrics, artifacts and models."
      ],
      "metadata": {
        "id": "6soWBRrz79Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_tracking_uri(f\"file://{PATH / 'ml_experiments'}\")"
      ],
      "metadata": {
        "id": "IBb8wgAJ5mXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions\n",
        "\n",
        "This defines `ml_utils.py` - a centralized, reusable module containing essential functions for the entire ML pipeline.\n",
        "\n",
        "Rather than scattering logic across the notebook, we encapsulate key operations into well-documented, testable utilities that promote:\n",
        "\n",
        "- **Code Reusability:**\n",
        "Functions like `evaluate_and_log_metrics`, `shap_analysis` and `log_transformer_to_mlflow` can be reused across experiments and projects.\n",
        "\n",
        "- **Experiment Tracking & Reproducibility:**\n",
        "Full integration with MLflow ensures every model, parameter, metric and artifact (e.g. SHAP values, Optuna search space) is logged systematically.\n",
        "\n",
        "- **Robust Diagnostics:**\n",
        "Includes statistical checks (`VIF`, correlation analysis) to validate assumptions.\n",
        "\n",
        "- **Safe Preprocessing Workflow:**\n",
        "`apply_transformer` and `compare_features_train_validation` ensure consistent transformations across train/validation sets - preventing subtle bugs.\n",
        "\n",
        "- **Config-Driven & Portable:**\n",
        "Loads paths and settings from `config.yaml`, enabling cross-platform compatibility (e.g. local, Colab, CI/CD) without hardcoded paths.\n",
        "\n",
        "- **Model Management:**\n",
        "Functions like `load_model`, `get_best_params` and `save_object` streamline model retrieval and deployment."
      ],
      "metadata": {
        "id": "xq5qlS1C3zj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ml_utils.py\n",
        "\n",
        "# Standard library imports\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Third-party imports\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import shap\n",
        "import yaml\n",
        "from mlflow.tracking import MlflowClient\n",
        "from scipy.stats import ks_2samp, chi2_contingency\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools import add_constant\n",
        "\n",
        "config_yaml_path = os.path.join(\n",
        "    '/content',\n",
        "    'drive',\n",
        "    'MyDrive',\n",
        "    'Colab',\n",
        "    'Machine Learning',\n",
        "    'Regression',\n",
        "    'Radancy',\n",
        "    'config.yaml'\n",
        ")\n",
        "\n",
        "# Load config\n",
        "with open(config_yaml_path, \"r\") as file:\n",
        "  config = yaml.safe_load(file)\n",
        "\n",
        "# Cross-platform path assembly\n",
        "PATH = Path().joinpath(*config[\"PATH_PARTS\"])\n",
        "\n",
        "# Other parameters\n",
        "RANDOM_STATE = config[\"RANDOM_STATE\"]\n",
        "COMPRESS = config[\"COMPRESS\"]\n",
        "N_JOBS = config[\"N_JOBS\"]\n",
        "TARGET = config[\"TARGET\"]\n",
        "\n",
        "def calculate_vif(df, target_col):\n",
        "  \"\"\"\n",
        "  Calculate Variance Inflation Factor (VIF) for numerical features in a\n",
        "  DataFrame.\n",
        "\n",
        "  Parameters:\n",
        "  - df: pandas.DataFrame — the input DataFrame\n",
        "  - target_col: str — name of the target column to exclude from VIF calculation\n",
        "\n",
        "  Returns:\n",
        "  - vif_data: pandas.DataFrame — DataFrame containing features and their VIF\n",
        "  values, sorted in descending order\n",
        "\n",
        "  VIF Interpretation Guide:\n",
        "  -------------------------\n",
        "  VIF Value   Interpretation\n",
        "  ---------   ------------------------------------------------------\n",
        "  1           No multicollinearity\n",
        "  1 - 5       Low to moderate multicollinearity (usually acceptable)\n",
        "  > 5         Potential multicollinearity issue\n",
        "  > 10        Serious multicollinearity — needs attention\n",
        "  \"\"\"\n",
        "\n",
        "  X = df.drop(columns=[target_col]).select_dtypes(include=['float64', 'int64'])\n",
        "  # add intercept term\n",
        "  X = add_constant(X)\n",
        "\n",
        "  vif_data = pd.DataFrame()\n",
        "  vif_data[\"feature\"] = X.columns\n",
        "\n",
        "  vif_data[\"VIF\"] = [\n",
        "      variance_inflation_factor(X.values, i) for i in range(X.shape[1])\n",
        "  ]\n",
        "\n",
        "  return vif_data.sort_values(by='VIF', ascending=False)\n",
        "###############################################################################\n",
        "\n",
        "def plot_and_save_correlation_matrix(\n",
        "    df,\n",
        "    save_path,\n",
        "    filename='corr_matrix.csv',\n",
        "    figsize=(12, 8),\n",
        "    title='Correlation Matrix'\n",
        "):\n",
        "  \"\"\"\n",
        "  Computes the correlation matrix of a DataFrame, saves it as a CSV, and plots\n",
        "  a heatmap.\n",
        "\n",
        "  Parameters:\n",
        "  - df: pandas.DataFrame — the input DataFrame\n",
        "  - save_path: str — directory path to save the CSV file\n",
        "  - filename: str — name of the CSV file to save\n",
        "  - figsize: tuple — size of the heatmap figure\n",
        "  - title: str — title of the heatmap plot\n",
        "  \"\"\"\n",
        "  # Compute correlation matrix\n",
        "  corr_matrix = df.corr()\n",
        "\n",
        "  # Save correlation matrix to CSV\n",
        "  os.makedirs(save_path, exist_ok=True)\n",
        "  corr_matrix.to_csv(os.path.join(save_path, filename))\n",
        "\n",
        "  # Plot heatmap\n",
        "  plt.figure(figsize=figsize)\n",
        "  sns.heatmap(\n",
        "      corr_matrix,\n",
        "      annot=True,\n",
        "      fmt=\".2f\",\n",
        "      cmap='coolwarm',\n",
        "      square=True,\n",
        "      linewidths=0.5\n",
        "  )\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "###############################################################################\n",
        "\n",
        "def apply_transformer(df_train, df_val, transformer, target_column):\n",
        "  \"\"\"\n",
        "  Apply a transformer to train and validation DataFrames, preserving the target\n",
        "  column.\n",
        "\n",
        "  Parameters:\n",
        "  - df_train (pd.DataFrame): Training DataFrame\n",
        "  - df_val (pd.DataFrame): validation DataFrame\n",
        "  - transformer: Transformer object with a transform method\n",
        "  - target_column (str): Name of the target column to preserve\n",
        "\n",
        "  Returns:\n",
        "  tuple: Transformed (df_train, df_val)\n",
        "  \"\"\"\n",
        "  # Transform train data, excluding target column, and reattach target\n",
        "  df_train_transformed = pd.concat(\n",
        "      [\n",
        "          transformer.transform(df_train.drop(target_column, axis=1)),\n",
        "          df_train[target_column]\n",
        "      ],\n",
        "      axis=1\n",
        "  )\n",
        "\n",
        "  # Transform validation data, excluding target column, and reattach target\n",
        "  df_val_transformed = pd.concat(\n",
        "      [\n",
        "          transformer.transform(df_val.drop(target_column, axis=1)),\n",
        "          df_val[target_column]\n",
        "      ],\n",
        "      axis=1\n",
        "  )\n",
        "\n",
        "  return df_train_transformed, df_val_transformed\n",
        "###############################################################################\n",
        "\n",
        "def log_transformer_to_mlflow(\n",
        "    transformer,\n",
        "    experiment_name,\n",
        "    model_name,\n",
        "    df,\n",
        "    target_col=None\n",
        "):\n",
        "  \"\"\"\n",
        "  Fit a transformer, set an MLflow experiment, and log the model and its\n",
        "  parameters.\n",
        "\n",
        "  Args:\n",
        "  - transformer: The transformer object to fit and log.\n",
        "  - experiment_name (str): Name of the MLflow experiment.\n",
        "  - model_name (str): Name for the model artifact in MLflow.\n",
        "  - df: DataFrame to fit the transformer.\n",
        "  - target_col (str, optional): Name of the target column, if applicable.\n",
        "  \"\"\"\n",
        "  # Prepare data for fitting\n",
        "  X = df.drop(target_col, axis=1) if target_col else df\n",
        "  y = df[target_col] if target_col else None\n",
        "\n",
        "  # Fit the transformer\n",
        "  transformer.fit(X, y)\n",
        "\n",
        "  # Set experiment\n",
        "  mlflow.set_experiment(experiment_name)\n",
        "\n",
        "  with mlflow.start_run():\n",
        "    # Log all attributes as parameters\n",
        "    for attr_name, attr_value in vars(transformer).items():\n",
        "      try:\n",
        "        mlflow.log_param(attr_name, str(attr_value))\n",
        "      except Exception as e:\n",
        "        print(f\"Could not log param {attr_name}: {e}\")\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=transformer,\n",
        "        name=model_name\n",
        "    )\n",
        "###############################################################################\n",
        "\n",
        "def save_object(obj, filename, path=PATH, compress=COMPRESS):\n",
        "  \"\"\"\n",
        "  Save any object (e.g., DataFrame, estimator) to a specified path using joblib.\n",
        "\n",
        "  Parameters:\n",
        "  - obj: Object to save (e.g., pandas DataFrame, scikit-learn estimator)\n",
        "  - filename (str): Name of the output file (should end with .pkl)\n",
        "  - path (str): Directory path where file will be saved\n",
        "  - compress (int): Compression level for joblib (default=3)\n",
        "  \"\"\"\n",
        "  # file_path = os.path.join(path, filename)\n",
        "  file_path = PATH / filename\n",
        "\n",
        "  if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "\n",
        "  joblib.dump(obj, file_path, compress=compress)\n",
        "###############################################################################\n",
        "\n",
        "def compare_features_train_validation(df_train, df_val):\n",
        "  \"\"\"\n",
        "  Compare feature columns between training and validation datasets to ensure\n",
        "  consistency. Due to preprocessing operations, the column structure of\n",
        "  df_train and df_val may differ.\n",
        "  This function identifies discrepancies to align the datasets by:\n",
        "  - Adding interaction terms if any\n",
        "  - Adding polynomial features if any\n",
        "  - Dropping features due to multicollinearity if any\n",
        "  - Performing feature selection\n",
        "\n",
        "  Parameters:\n",
        "  - df_train (pandas.DataFrame): Training dataset\n",
        "  - df_val (pandas.DataFrame): validation dataset\n",
        "\n",
        "  Returns:\n",
        "  - set: Columns present in df_val but not in\n",
        "  df_train (to be dropped from df_val)\n",
        "  \"\"\"\n",
        "  # Identify columns in df_train but not in df_val\n",
        "  train_only = set(df_train.columns) - set(df_val.columns)\n",
        "\n",
        "  # Identify columns in df_val but not in df_train\n",
        "  validation_only = set(df_val.columns) - set(df_train.columns)\n",
        "\n",
        "  # Check for columns in df_train but not in df_val\n",
        "  if train_only:\n",
        "    print(\"Add features in df_val:\", train_only)\n",
        "  else:\n",
        "    print(\"No features to be added in df_val.\")\n",
        "\n",
        "  # Check for columns in df_val but not in df_train\n",
        "  if validation_only:\n",
        "    print(\"Drop features from df_val:\", validation_only)\n",
        "  else:\n",
        "    print(\"No features to be dropped from df_val.\")\n",
        "\n",
        "  return validation_only\n",
        "###############################################################################\n",
        "\n",
        "def copy_file_to_destination(source_path: str, destination_path: Path) -> None:\n",
        "  \"\"\"\n",
        "  Copies a file to the specified destination, deleting any existing file at the\n",
        "  destination.\n",
        "\n",
        "  Args:\n",
        "  - source_path (str): Path to the source file\n",
        "  - destination_path (Path): Path to the destination file\n",
        "\n",
        "  Returns:\n",
        "  - None\n",
        "  \"\"\"\n",
        "  # Check if file exists in destination and delete it\n",
        "  if os.path.exists(destination_path):\n",
        "    os.remove(destination_path)\n",
        "    print(\"Existing file deleted from destination.\")\n",
        "\n",
        "  # Copy the file to destination\n",
        "  shutil.copy(source_path, destination_path)\n",
        "  print(\"File copied to destination successfully.\")\n",
        "###############################################################################\n",
        "\n",
        "def log_optuna_best_trial_search_space(study):\n",
        "  \"\"\"\n",
        "  Logs the parameter search space of the best Optuna trial to MLflow as a\n",
        "  JSON artifact.\n",
        "\n",
        "  Args:\n",
        "  - study (optuna.study.Study): The Optuna study object.\n",
        "  \"\"\"\n",
        "  # Check if there are completed trials\n",
        "  if (\n",
        "      study.trials\n",
        "      and\n",
        "      any(t.state == optuna.trial.TrialState.COMPLETE for t in study.trials)\n",
        "  ):\n",
        "    run_id = mlflow.active_run().info.run_id\n",
        "    artifact_name = f\"optuna_best_trial_search_space_{run_id}.json\"\n",
        "\n",
        "    # Log the search space distributions as a JSON artifact\n",
        "    mlflow.log_dict(\n",
        "        {k: str(v) for k, v in study.best_trial.distributions.items()},\n",
        "        artifact_name\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Parameter search space for the best trial logged to MLflow as \"\n",
        "        f\"artifact: {artifact_name}\"\n",
        "    )\n",
        "  else:\n",
        "    print(\"No completed trials available to log search space.\")\n",
        "###############################################################################\n",
        "\n",
        "def evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\"):\n",
        "  \"\"\"\n",
        "  Evaluate model predictions on both log scale and original scale,\n",
        "  log the metrics to MLflow and print them.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  y_val : array-like\n",
        "  - True target values (log-transformed).\n",
        "\n",
        "  y_pred : array-like\n",
        "  - Predicted target values (log-transformed).\n",
        "\n",
        "  prefix : str, optional\n",
        "  - Prefix for MLflow metric names (default: \"val\").\n",
        "  \"\"\"\n",
        "\n",
        "  # --- Log Scale Evaluation ---\n",
        "  rmse_log = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "  mae_log = mean_absolute_error(y_val, y_pred)\n",
        "  r2_log = r2_score(y_val, y_pred)\n",
        "\n",
        "  mlflow.log_metric(f\"{prefix}_rmse_log\", rmse_log)\n",
        "  mlflow.log_metric(f\"{prefix}_mae_log\", mae_log)\n",
        "  mlflow.log_metric(f\"{prefix}_r2_log\", r2_log)\n",
        "\n",
        "  print(f\"\\nFinal Evaluation on Hold-Out Validation Set (Log Scale):\")\n",
        "  print(f\"Hold-Out Validation RMSE (Log Scale): {rmse_log}\")\n",
        "  print(f\"Hold-Out Validation MAE (Log Scale): {mae_log}\")\n",
        "  print(f\"Hold-Out Validation R² Score (Log Scale): {r2_log}\")\n",
        "\n",
        "  # --- Original Scale Evaluation ---\n",
        "  y_pred_original = np.expm1(y_pred)\n",
        "  y_val_original = np.expm1(y_val)\n",
        "\n",
        "  rmse_original = np.sqrt(mean_squared_error(y_val_original, y_pred_original))\n",
        "  mae_original = mean_absolute_error(y_val_original, y_pred_original)\n",
        "  r2_original = r2_score(y_val_original, y_pred_original)\n",
        "\n",
        "  mlflow.log_metric(f\"{prefix}_rmse_original\", rmse_original)\n",
        "  mlflow.log_metric(f\"{prefix}_mae_original\", mae_original)\n",
        "  mlflow.log_metric(f\"{prefix}_r2_original\", r2_original)\n",
        "\n",
        "  print(f\"\\nFinal Evaluation on Hold-Out Validation Set (Original Scale):\")\n",
        "  print(f\"Hold-Out Validation RMSE (Original Scale): {rmse_original}\")\n",
        "  print(f\"Hold-Out Validation MAE (Original Scale): {mae_original}\")\n",
        "  print(f\"Hold-Out Validation R² Score (Original Scale): {r2_original}\")\n",
        "\n",
        "  return y_val_original, y_pred_original\n",
        "###############################################################################\n",
        "\n",
        "def get_best_params(experiment_name):\n",
        "  \"\"\"\n",
        "  Retrieve the best parameters from an MLflow experiment based on the lowest\n",
        "  val_rmse_original.\n",
        "\n",
        "  Parameters:\n",
        "  - experiment_name (str): Name of the MLflow experiment.\n",
        "\n",
        "  Returns:\n",
        "  - dict: Best parameters from the run with the lowest val_rmse_original.\n",
        "  - None: If the experiment is not found or no runs exist.\n",
        "  \"\"\"\n",
        "\n",
        "  client = MlflowClient()\n",
        "\n",
        "  # Get experiment ID\n",
        "  experiment = client.get_experiment_by_name(experiment_name)\n",
        "\n",
        "  if experiment is None:\n",
        "    print(f\"Experiment '{experiment_name}' not found.\")\n",
        "    return None\n",
        "\n",
        "  experiment_id = experiment.experiment_id\n",
        "\n",
        "  # Fetch all runs\n",
        "  runs = client.search_runs(\n",
        "      experiment_ids=[experiment_id],\n",
        "      filter_string=\"\",\n",
        "      run_view_type=mlflow.entities.ViewType.ACTIVE_ONLY,\n",
        "      max_results=1,\n",
        "      order_by=[\"metrics.val_rmse_original ASC\"]\n",
        "  )\n",
        "\n",
        "  # Check if runs exist\n",
        "  if not runs:\n",
        "    print(f\"No runs found for experiment '{experiment_name}'.\")\n",
        "    return None\n",
        "\n",
        "  # Get best run and extract parameters\n",
        "  best_run = runs[0]\n",
        "  return best_run.data.params\n",
        "\n",
        "###############################################################################\n",
        "# Function to convert parameters to appropriate types\n",
        "def convert_param(param, param_type, param_name):\n",
        "  try:\n",
        "    if param_type == 'float':\n",
        "      return float(param)\n",
        "    elif param_type == 'int':\n",
        "      return int(float(param))  # Handle string floats\n",
        "    elif param_type == 'bool':\n",
        "      return param.lower() == 'true' if isinstance(param, str) else bool(param)\n",
        "    else:\n",
        "      return param  # Keep as is for categorical or other types\n",
        "  except (ValueError, TypeError):\n",
        "    raise ValueError(f\"Invalid value for {param_name}: {param}\")\n",
        "\n",
        "###############################################################################\n",
        "def load_model(experiment_name, model_name, criteria='latest'):\n",
        "  \"\"\"\n",
        "  Load a model artifact from a given MLflow experiment based on specified\n",
        "  criteria.\n",
        "\n",
        "  Args:\n",
        "  - experiment_name (str): The name of the MLflow experiment.\n",
        "  - model_name (str): The name used as the artifact path during logging.\n",
        "  - criteria (str): Selection criteria for the model ('latest' or 'min_rmse').\n",
        "    Defaults to 'latest'.\n",
        "\n",
        "  Returns:\n",
        "  - model: The loaded model object.\n",
        "\n",
        "  Raises:\n",
        "  - ValueError: If experiment or runs are not found, or if invalid criteria is\n",
        "    provided.\n",
        "  \"\"\"\n",
        "  client = MlflowClient()\n",
        "\n",
        "  # Get experiment by name\n",
        "  experiment = client.get_experiment_by_name(experiment_name)\n",
        "  if experiment is None:\n",
        "    raise ValueError(f\"Experiment '{experiment_name}' not found.\")\n",
        "\n",
        "  if criteria not in ['latest', 'min_rmse']:\n",
        "    raise ValueError(\"Criteria must be 'latest' or 'min_rmse'.\")\n",
        "\n",
        "  if criteria == 'latest':\n",
        "    # Search for the most recent run\n",
        "    runs = client.search_runs(\n",
        "        experiment_ids=[experiment.experiment_id],\n",
        "        order_by=[\"start_time DESC\"],\n",
        "        max_results=1\n",
        "    )\n",
        "  else:  # criteria == 'min_rmse'\n",
        "    # Search for all runs and sort by rmse metric\n",
        "    runs = client.search_runs(\n",
        "        experiment_ids=[experiment.experiment_id],\n",
        "        order_by=[\"metrics.val_rmse_original ASC\"],\n",
        "        max_results=1\n",
        "    )\n",
        "\n",
        "  if not runs:\n",
        "    raise ValueError(f\"No runs found in experiment '{experiment_name}'.\")\n",
        "\n",
        "  # Get run ID from the selected run\n",
        "  run_id = runs[0].info.run_id\n",
        "\n",
        "  # Construct model URI and load model\n",
        "  model_uri = f\"runs:/{run_id}/{model_name}\"\n",
        "  model = mlflow.sklearn.load_model(model_uri)\n",
        "\n",
        "  return model\n",
        "###############################################################################\n",
        "\n",
        "def shap_analysis(\n",
        "    model,\n",
        "    X,\n",
        "    model_type: str,\n",
        "    random_state: int = RANDOM_STATE,\n",
        "    sample_size: int = 100\n",
        "):\n",
        "  \"\"\"\n",
        "  Run SHAP analysis for different model types.\n",
        "\n",
        "  Parameters:\n",
        "  model : trained model\n",
        "  - Final fitted model.\n",
        "\n",
        "  X : pd.DataFrame\n",
        "  - Input features used for training or evaluation.\n",
        "\n",
        "  model_type : str\n",
        "  - One of [\"linear\", \"tree\", \"kernel\"] depending on algorithm.\n",
        "\n",
        "  random_state : int, optional\n",
        "  - Random state for reproducibility.\n",
        "\n",
        "  sample_size : int, optional\n",
        "  - Sample size for Kernel/Tree explainers to reduce computation.\n",
        "\n",
        "  Returns:\n",
        "  mean_abs_shap : pd.Series\n",
        "  - Mean absolute SHAP values per feature.\n",
        "  \"\"\"\n",
        "\n",
        "  # Choose explainer depending on model type\n",
        "  if model_type == \"linear\":\n",
        "    explainer = shap.LinearExplainer(\n",
        "        model,\n",
        "        X,\n",
        "        feature_perturbation=\"interventional\"\n",
        "    )\n",
        "\n",
        "    shap_values = explainer(X).values\n",
        "    shap_df = pd.DataFrame(shap_values, columns=X.columns)\n",
        "\n",
        "  elif model_type == \"tree\":\n",
        "    X_subset = X.sample(n=min(sample_size, len(X)), random_state=random_state)\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_subset)\n",
        "    shap_df = pd.DataFrame(shap_values, columns=X_subset.columns)\n",
        "\n",
        "  elif model_type == \"kernel\":\n",
        "    X_subset = X.sample(n=min(sample_size, len(X)), random_state=random_state)\n",
        "\n",
        "    explainer = shap.KernelExplainer(\n",
        "        model.predict,\n",
        "        X_subset,\n",
        "        feature_perturbation=\"interventional\"\n",
        "    )\n",
        "\n",
        "    shap_values = explainer.shap_values(X_subset, nsamples=100)\n",
        "    shap_df = pd.DataFrame(shap_values, columns=X_subset.columns)\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
        "\n",
        "  # Get mean absolute SHAP value for each feature\n",
        "  mean_abs_shap = shap_df.abs().mean().sort_values(ascending=False)\n",
        "\n",
        "  # Convert to JSON for MLflow logging\n",
        "  mean_abs_shap_json = json.dumps(mean_abs_shap.to_dict())\n",
        "\n",
        "  # Log in MLflow\n",
        "  mlflow.log_param(\"shap_mean_abs_by_feature\", mean_abs_shap_json)\n",
        "\n",
        "  print(\"Mean Absolute SHAP Values:\")\n",
        "  print(mean_abs_shap)\n",
        "\n",
        "  return mean_abs_shap\n",
        "###############################################################################\n",
        "\n",
        "def check_feature_distribution_mismatches(\n",
        "    X_train,\n",
        "    X_val,\n",
        "    test_type,\n",
        "    p_threshold=0.05\n",
        "):\n",
        "  \"\"\"\n",
        "  Check for feature distribution mismatches between training and validation\n",
        "  datasets.\n",
        "\n",
        "  Parameters:\n",
        "  - X_train (pandas.DataFrame): Training dataset features\n",
        "    (all numerical or all categorical)\n",
        "  - X_val (pandas.DataFrame): Validation dataset features (same type as X_train)\n",
        "  - p_threshold (float): Significance level for p-value (default: 0.05)\n",
        "  - test_type (str): Statistical test to use\n",
        "    (\"Kolmogorov-Smirnov\" for numerical, \"Chi-squared\" for categorical)\n",
        "\n",
        "  Returns:\n",
        "  - dict: Dictionary of features with different distributions and their p-values\n",
        "  \"\"\"\n",
        "  print(\"Checking feature distribution mismatches:\")\n",
        "  mismatched_features = {}\n",
        "\n",
        "  # Validate test_type\n",
        "  if test_type not in [\"Kolmogorov-Smirnov\", \"Chi-squared\"]:\n",
        "    raise ValueError(\"test_type must be 'Kolmogorov-Smirnov' or 'Chi-squared'\")\n",
        "\n",
        "  for col in X_train.columns:\n",
        "    if col not in X_val.columns:\n",
        "      print(f\"Feature {col} not found in validation set, skipping.\")\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      if test_type == \"Chi-squared\":\n",
        "        # For categorical columns, use Chi-squared test\n",
        "        train_counts = X_train[col].value_counts()\n",
        "        val_counts = X_val[col].value_counts()\n",
        "\n",
        "        # Align categories\n",
        "        all_categories = sorted(set(train_counts.index) | set(val_counts.index))\n",
        "        train_freq = [train_counts.get(cat, 0) for cat in all_categories]\n",
        "        val_freq = [val_counts.get(cat, 0) for cat in all_categories]\n",
        "\n",
        "        # Perform Chi-squared test\n",
        "        contingency_table = np.array([train_freq, val_freq])\n",
        "        stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
        "      else:\n",
        "        # For numerical columns, use KS test\n",
        "        stat, p_value = ks_2samp(X_train[col], X_val[col])\n",
        "\n",
        "      if p_value < p_threshold:\n",
        "        print(\n",
        "            f\"Feature {col} has different distributions \"\n",
        "            f\"({test_type} test, p-value: {p_value:.4f})\"\n",
        "        )\n",
        "        mismatched_features[col] = {'p_value': p_value, 'test': test_type}\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing feature {col}: {str(e)}\")\n",
        "      continue\n",
        "\n",
        "  return mismatched_features"
      ],
      "metadata": {
        "id": "YzrAzLPR31Vp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837afa4f-4236-4930-f640-64a565f29117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ml_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = 'ml_utils.py'\n",
        "destination_path = PATH / 'ml_utils.py'\n",
        "\n",
        "# Check if file exists in destination and delete it\n",
        "if os.path.exists(PATH / 'ml_utils.py'):\n",
        "  os.remove(destination_path)\n",
        "  print(\"Existing file deleted from destination.\")\n",
        "\n",
        "# Copy the file to destination\n",
        "shutil.copy(source_path, destination_path)\n",
        "print(\"File copied to destination successfully.\")"
      ],
      "metadata": {
        "id": "OYMPV9zU7K2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9e5654b-7dbb-4504-b8f5-dca3d3bb8631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing file deleted from destination.\n",
            "File copied to destination successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing with Temporal Integrity\n",
        "\n",
        "We define `DataLoader` as a custom scikit-learn transformer that handles:\n",
        "- Loading from CSV\n",
        "- Deterministic temporal splitting\n",
        "- Basic cleaning and standardization\n",
        "\n",
        "### Key Design Decisions:\n",
        "\n",
        "- **Time-Aware Train/Val/Test Split:**  \n",
        "The data is split sequentially (not randomly) by order of appearance - preserving temporal flow. This mimics real-world deployment where models predict future data based on past observations.\n",
        "\n",
        "- **Controlled Data Ingestion:**  \n",
        "`max_rows` enables fast prototyping on large datasets without memory issues - critical for iterative development.\n",
        "\n",
        "- **Consistent Preprocessing:**  \n",
        "\t- Removed rows where `cost = 0`, `clicks = 0`, and `conversions = 1` to exclude invalid campaign data with applications but no associated cost or clicks\n",
        "\t- Fixes common data quality issues (e.g. typo `converions` to `conversions`)\n",
        "\t- Cleans string columns (lowercase, underscores, trim whitespace) for consistency\n",
        "\t- Drops irrelevant identifiers like `campaign_id` to prevent leakage"
      ],
      "metadata": {
        "id": "lnmi9Xb1yQpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_loader.py\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "\n",
        "class DataLoader(BaseEstimator, TransformerMixin):\n",
        "  def __init__(\n",
        "      self,\n",
        "      file_path,\n",
        "      train_size=0.7,\n",
        "      val_size=0.15,\n",
        "      max_rows,\n",
        "      string_cols_to_clean=None\n",
        "  ):\n",
        "    self.file_path = file_path\n",
        "    self.train_size = train_size\n",
        "    self.val_size = val_size\n",
        "    self.max_rows = max_rows\n",
        "    self.string_cols_to_clean = string_cols_to_clean\n",
        "    self.df_train = None\n",
        "    self.df_val = None\n",
        "    self.df_test = None\n",
        "\n",
        "  def fit(self, X=None, y=None):\n",
        "    # Load the dataset\n",
        "    df_original = pd.read_csv(self.file_path)\n",
        "\n",
        "    # Fix misspelled column if it exists\n",
        "    if 'converions' in df_original.columns:\n",
        "      df_original.rename(columns={\"converions\": \"conversions\"}, inplace=True)\n",
        "\n",
        "    # Remove rows where cost and clicks are 0 and conversions is 1\n",
        "    df_original = df_original[\n",
        "        (df_original['cost'] != 0)\n",
        "        | (df_original['clicks'] != 0)\n",
        "        | (df_original['conversions'] != 1)\n",
        "    ]\n",
        "\n",
        "    # Limit to max_rows if specified\n",
        "    if self.max_rows is not None:\n",
        "      df_original = df_original.head(self.max_rows)\n",
        "\n",
        "    # Calculate split indices\n",
        "    total_size = len(df_original)\n",
        "    train_size = int(self.train_size * total_size)\n",
        "    val_size = int(self.val_size * total_size)\n",
        "\n",
        "    # Split the data\n",
        "    self.df_train = df_original[:train_size].reset_index(drop=True)\n",
        "\n",
        "    self.df_val = (\n",
        "        df_original[train_size:train_size + val_size].reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    self.df_test = df_original[train_size + val_size:].reset_index(drop=True)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    df = X.copy()\n",
        "\n",
        "    # Ensure date is in datetime format and sort\n",
        "    if 'date' in df.columns:\n",
        "      df['date'] = pd.to_datetime(\n",
        "          df['date'],\n",
        "          format='%Y-%m-%d'\n",
        "      )\n",
        "\n",
        "      df = df.sort_values(by='date')\n",
        "\n",
        "    # Drop campaign_id if it exists\n",
        "    df.drop(columns=['campaign_id'], inplace=True, errors='ignore')\n",
        "\n",
        "    # Convert and clean selected string columns\n",
        "    for col in self.string_cols_to_clean:\n",
        "      if col in df.columns:\n",
        "        df[col] = (\n",
        "            df[col]\n",
        "            .astype(\"string\")\n",
        "            .str.strip()\n",
        "            .str.replace(r'\\s+', '_', regex=True)\n",
        "            .str.lower()\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "  def get_train_val_test(self):\n",
        "    \"\"\"\n",
        "    - Return the train, validation, and test DataFrames.\n",
        "    - Added this method in case we wish to take a look at df_train, df_val\n",
        "      and df_test before it is transformed.\n",
        "    \"\"\"\n",
        "    if self.df_train is None or self.df_val is None or self.df_test is None:\n",
        "      raise ValueError(\"Must call fit() before get_train_val_test()\")\n",
        "\n",
        "    return self.df_train, self.df_val, self.df_test"
      ],
      "metadata": {
        "id": "XKXbDftyoBFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128b059f-bac6-4db0-e66a-3218565635ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'data_loader.py',\n",
        "    destination_path = PATH / 'data_loader.py'\n",
        ")"
      ],
      "metadata": {
        "id": "7LN3DFQFYbHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4a3606-0a97-4023-df5b-9c013307f093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing file deleted from destination.\n",
            "File copied to destination successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "c900v4KjBWie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data_loader import DataLoader\n",
        "\n",
        "file_path = PATH / \"data.csv\"\n",
        "\n",
        "string_cols_to_clean=[\n",
        "        'category_id',\n",
        "        'industry',\n",
        "        'customer_id',\n",
        "        'publisher',\n",
        "        'market_id'\n",
        "    ]\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    file_path = file_path,\n",
        "    train_size=0.7,\n",
        "    max_rows=1000,\n",
        "    string_cols_to_clean=string_cols_to_clean\n",
        ")\n",
        "\n",
        "# Fit the transformer\n",
        "data_loader.fit()\n",
        "\n",
        "df_train, df_val, df_test = data_loader.get_train_val_test()\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=data_loader,\n",
        "    experiment_name=\"Data_Loader\",\n",
        "    model_name=\"data_loader\",\n",
        "    df=df_train,\n",
        "    target_col=None\n",
        ")"
      ],
      "metadata": {
        "id": "6BImg7g_WTv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Delete Test Data"
      ],
      "metadata": {
        "id": "RnM08oR6W3DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_object(df_test, 'df_test.pkl')"
      ],
      "metadata": {
        "id": "BA-mmdSbWvlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df_test"
      ],
      "metadata": {
        "id": "D-EioQZNQS0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train and Test"
      ],
      "metadata": {
        "id": "qbgbj_czBaRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = data_loader.transform(df_train)\n",
        "df_val = data_loader.transform(df_val)"
      ],
      "metadata": {
        "id": "dqgbWa6aBWN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "E_zOXMmWy3Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "bmTszph2y4tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NAs\n",
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "2l8F0jDg03RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "bTIvxuElVmI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values again\n",
        "df_train['industry'].unique()"
      ],
      "metadata": {
        "id": "HsMI1LKI3IcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['category_id'].unique()"
      ],
      "metadata": {
        "id": "0uHYlwi1y6PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['publisher'].unique()"
      ],
      "metadata": {
        "id": "rm8ft_nazBEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['market_id'].unique()"
      ],
      "metadata": {
        "id": "AyCroXbmy_Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates if any\n",
        "df_train.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "qm8GEBBy3Tof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target Processor\n",
        "\n",
        "We define a custom scikit-learn transformer `CPAProcessor` to compute the **Cost Per Acquisition (CPA)** - a key business metric directly from raw columns `cost` and `conversions`."
      ],
      "metadata": {
        "id": "XBfYmlupU3PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPA Processor"
      ],
      "metadata": {
        "id": "c_FB31sGB0yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile target_processor.py\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "TARGET = 'cpa'\n",
        "\n",
        "class CPAProcessor(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  A class to calculate Cost Per Acquisition (CPA) by dividing cost by\n",
        "  conversions, replacing zero conversions with a specified value to avoid\n",
        "  division by zero.\n",
        "  \"\"\"\n",
        "  def __init__(self, zero_replacement=0.1):\n",
        "    \"\"\"\n",
        "    Initialize the CPAProcessor.\n",
        "\n",
        "    Args:\n",
        "    - zero_replacement (float): Value to replace zero conversions with\n",
        "    (default: 0.1).\n",
        "    \"\"\"\n",
        "    self.zero_replacement = zero_replacement\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit method for compatibility with scikit-learn pipelines. Does nothing.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Input DataFrame with 'cost' and 'conversions' columns.\n",
        "    - y (None): Ignored, included for compatibility.\n",
        "\n",
        "    Returns:\n",
        "    - self: Returns the instance itself.\n",
        "    \"\"\"\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Calculate CPA as cost / (conversions, with zeros replaced).\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Input DataFrame with 'cost' and 'conversions' columns.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: DataFrame with an additional 'cpa' column.\n",
        "\n",
        "    Raises:\n",
        "    - ValueError: If 'cost' or 'conversions' columns are missing.\n",
        "    - TypeError: If X is not a pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise TypeError(\"Input X must be a pandas DataFrame\")\n",
        "\n",
        "    if not all(col in X.columns for col in ['cost', 'conversions']):\n",
        "      raise ValueError(\n",
        "          \"Input DataFrame must contain 'cost' and 'conversions' columns.\"\n",
        "      )\n",
        "\n",
        "    # Create a copy to avoid modifying the original DataFrame\n",
        "    X_transformed = X.copy()\n",
        "\n",
        "    # Calculate CPA: cost / (conversions, with 0 replaced by zero_replacement)\n",
        "    X_transformed[TARGET] = (\n",
        "        X_transformed['cost'] / X_transformed['conversions']\n",
        "        .replace(0, self.zero_replacement)\n",
        "    )\n",
        "\n",
        "    return X_transformed"
      ],
      "metadata": {
        "id": "B8tEZ5prygJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit and Transform Train and Test"
      ],
      "metadata": {
        "id": "9rzGXdQLB--C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from target_processor import CPAProcessor\n",
        "\n",
        "cpa_processor = CPAProcessor(zero_replacement = 0.1)\n",
        "df_train = cpa_processor.fit(df_train).transform(df_train)\n",
        "df_val = cpa_processor.transform(df_val)"
      ],
      "metadata": {
        "id": "xDqK_xbbz11O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Detection\n",
        "\n",
        "Outliers can distort model training and reduce generalization performance. To address this, several outlier detection techniques were systematically evaluated:\n",
        "\n",
        "- **Methods tried:** `IsolationForest`, `Local Outlier Factor (LOF)`, `One-Class SVM`, `DBSCAN`, `Z-Score method`  \n",
        "- **Purpose:** Identify and remove anomalous data points that could negatively affect model performance, while preserving useful information.\n",
        "\n",
        "Evaluation on the validation set:\n",
        "\n",
        "1. **IsolationForest** consistently provided the best results, achieving the lowest cross-validated RMSE.  \n",
        "2. **Followed by:** Local Outlier Factor (LOF), One-Class SVM, DBSCAN, and Z-Score method.  \n",
        "3. The proportion of data removed as outliers was reasonable, ensuring that essential predictive signals were preserved.\n",
        "\n",
        "**Decision:** The final model uses **IsolationForest** for outlier detection.\n",
        "\n",
        "**Takeaway:** By systematically comparing multiple approaches and validating their impact on downstream performance, IsolationForest was selected as the outlier removal method, ensuring an empirically justified, robust, and interpretable preprocessing step.\n"
      ],
      "metadata": {
        "id": "Mt0Bb6MEAjLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = (\n",
        "    df_train\n",
        "    .select_dtypes(include=['int64', 'float64'])\n",
        "    .columns\n",
        "    .tolist()\n",
        ")\n",
        "\n",
        "X_train_full = df_train[numeric_cols]\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_full)\n",
        "X_train_full_scaled = scaler.transform(X_train_full)\n",
        "X_val = df_val[numeric_cols]\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "n_trials = 500\n",
        "n_splits = 5\n",
        "n_repeats = 3\n",
        "mlflow_experiment = \"outlier_detection\"\n",
        "# SQLite storage configuration\n",
        "study_name = f\"{mlflow_experiment}_study\"\n",
        "storage = f\"sqlite:///{PATH}/{study_name}.db\"\n",
        "\n",
        "def objective(trial):\n",
        "  winsorize_upper = trial.suggest_float(\"winsorize_upper\", 0.90, 1)\n",
        "\n",
        "  # Apply winsorization to target\n",
        "  y_train_full = np.log1p(\n",
        "      mstats.winsorize(df_train[TARGET], limits=[0, 1-winsorize_upper])\n",
        "  )\n",
        "\n",
        "  outlier_method = trial.suggest_categorical(\n",
        "      \"outlier_method\",\n",
        "       [\n",
        "           \"IsolationForest\",\n",
        "           \"LocalOutlierFactor\",\n",
        "           \"OneClassSVM\",\n",
        "           \"DBSCAN\",\n",
        "           \"ZScore\"\n",
        "       ]\n",
        "  )\n",
        "\n",
        "  if outlier_method == \"IsolationForest\":\n",
        "    contamination_option = trial.suggest_categorical(\n",
        "        \"if_contamination_option\",\n",
        "         [\"auto\", \"float\"]\n",
        "    )\n",
        "\n",
        "    contamination = (\n",
        "        'auto'\n",
        "        if contamination_option == 'auto'\n",
        "        else trial.suggest_float(\"if_contamination\", 0.01, 0.2)\n",
        "    )\n",
        "\n",
        "    detector = IsolationForest(\n",
        "        n_estimators=trial.suggest_int(\"if_n_estimators\", 50, 300),\n",
        "        max_samples=trial.suggest_float(\"if_max_samples\", 0.1, 1.0),\n",
        "        contamination=contamination,\n",
        "        max_features=trial.suggest_float(\"if_max_features\", 0.1, 1.0),\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    mask = detector.fit_predict(X_train_full_scaled) == 1\n",
        "\n",
        "  elif outlier_method == \"LocalOutlierFactor\":\n",
        "    contamination_option = trial.suggest_categorical(\n",
        "        \"lof_contamination_option\",\n",
        "         [\"auto\", \"float\"]\n",
        "    )\n",
        "\n",
        "    contamination = (\n",
        "        'auto'\n",
        "        if contamination_option == 'auto'\n",
        "        else trial.suggest_float(\"lof_contamination\", 0.01, 0.2)\n",
        "    )\n",
        "\n",
        "    detector = LocalOutlierFactor(\n",
        "        n_neighbors=trial.suggest_int(\"lof_n_neighbors\", 5, 100),\n",
        "        algorithm=trial.suggest_categorical(\n",
        "            \"lof_algorithm\",\n",
        "             [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "        ),\n",
        "        leaf_size=trial.suggest_int(\"lof_leaf_size\", 10, 100),\n",
        "        metric=trial.suggest_categorical(\n",
        "            \"lof_metric\",\n",
        "             [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\n",
        "        ),\n",
        "        p=trial.suggest_int(\"lof_p\", 1, 5),\n",
        "        contamination=contamination,\n",
        "        n_jobs=N_JOBS\n",
        "    )\n",
        "    mask = detector.fit_predict(X_train_full_scaled) == 1\n",
        "\n",
        "  elif outlier_method == \"OneClassSVM\":\n",
        "    detector = OneClassSVM(\n",
        "        kernel=trial.suggest_categorical(\n",
        "            \"ocsvm_kernel\",\n",
        "             [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "        ),\n",
        "        degree=trial.suggest_int(\"ocsvm_degree\", 2, 5),\n",
        "        gamma=trial.suggest_float(\"ocsvm_gamma\", 0.001, 1.0, log=True),\n",
        "        coef0=trial.suggest_float(\"ocsvm_coef0\", 0.0, 1.0),\n",
        "        tol=trial.suggest_float(\"ocsvm_tol\", 1e-4, 1e-2),\n",
        "        nu=trial.suggest_float(\"ocsvm_nu\", 0.01, 0.5),\n",
        "        shrinking=trial.suggest_categorical(\"ocsvm_shrinking\", [True, False]),\n",
        "        max_iter=trial.suggest_int(\"ocsvm_max_iter\", 100, 10000),\n",
        "        verbose=0\n",
        "    )\n",
        "    mask = detector.fit_predict(X_train_full_scaled) == 1\n",
        "\n",
        "  elif outlier_method == \"DBSCAN\":\n",
        "      detector = DBSCAN(\n",
        "          eps=trial.suggest_float(\"dbscan_eps\", 0.1, 5.0),\n",
        "          min_samples=trial.suggest_int(\"dbscan_min_samples\", 3, 20),\n",
        "          algorithm=trial.suggest_categorical(\n",
        "              \"dbscan_algorithm\",\n",
        "               [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "          ),\n",
        "          leaf_size=trial.suggest_int(\"dbscan_leaf_size\", 10, 100),\n",
        "          metric=trial.suggest_categorical(\n",
        "              \"dbscan_metric\",\n",
        "               [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\n",
        "          ),\n",
        "          p=trial.suggest_int(\"dbscan_p\", 1, 5),\n",
        "          n_jobs=N_JOBS\n",
        "      )\n",
        "      cluster_labels = detector.fit_predict(X_train_full_scaled)\n",
        "      mask = cluster_labels != -1\n",
        "\n",
        "  elif outlier_method == \"ZScore\":\n",
        "    threshold = trial.suggest_float(\"zscore_threshold\", 2.0, 4.0)\n",
        "    # This recalculates zscore on already scaled data. It may be misleading.\n",
        "    # Thus commented.\n",
        "    # z_scores = np.abs(zscore(X_train_full_scaled))\n",
        "    # Ideally, following is correct.\n",
        "    z_scores = np.abs(zscore(X_train_full, axis=0))\n",
        "    mask = (z_scores < threshold).all(axis=1)\n",
        "\n",
        "  X_clean = X_train_full[mask]\n",
        "\n",
        "  # Convert to Pandas Series\n",
        "  y_clean = pd.Series(y_train_full[mask])\n",
        "\n",
        "  # Step 4: Repeated K-Fold Cross-Validation\n",
        "  rkf = RepeatedKFold(\n",
        "      n_splits=n_splits,\n",
        "      n_repeats=n_repeats,\n",
        "      random_state=RANDOM_STATE\n",
        "  )\n",
        "\n",
        "  rmse_list = []\n",
        "\n",
        "  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_clean)):\n",
        "    X_train, X_val_fold = X_clean.iloc[train_idx], X_clean.iloc[val_idx]\n",
        "    y_train, y_val_fold = y_clean.iloc[train_idx], y_clean.iloc[val_idx]\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective= trial.suggest_categorical('objective', ['reg:squarederror']),\n",
        "        base_score= trial.suggest_float('base_score', 0.1, 0.9),\n",
        "        booster= trial.suggest_categorical('booster', [\n",
        "            'gbtree',\n",
        "            'dart'\n",
        "        ]),\n",
        "        colsample_bylevel= trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
        "        colsample_bynode= trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
        "        colsample_bytree= trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        gamma= trial.suggest_float('gamma', 0, 5, log=True),\n",
        "        grow_policy= trial.suggest_categorical(\n",
        "            'grow_policy',\n",
        "             ['depthwise', 'lossguide']\n",
        "        ),\n",
        "        learning_rate= trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        max_bin= trial.suggest_int('max_bin', 128, 512),\n",
        "        max_cat_to_onehot= trial.suggest_int('max_cat_to_onehot', 1, 64),\n",
        "        max_depth= trial.suggest_int('max_depth', 3, 10),\n",
        "        max_leaves= trial.suggest_int('max_leaves', 0, 64),\n",
        "        min_child_weight= trial.suggest_float('min_child_weight', 1, 10),\n",
        "        n_estimators= trial.suggest_int('n_estimators', 50, 500),\n",
        "        n_jobs= N_JOBS,\n",
        "        num_parallel_tree= trial.suggest_int('num_parallel_tree', 1, 3),\n",
        "        random_state= RANDOM_STATE,\n",
        "        reg_alpha= trial.suggest_float('reg_alpha', 0.0, 1.0, log=True),\n",
        "        reg_lambda= trial.suggest_float('reg_lambda', 0.0, 1.0, log=True),\n",
        "        sampling_method= trial.suggest_categorical(\n",
        "            'sampling_method',\n",
        "             ['uniform', 'gradient_based']\n",
        "        ),\n",
        "        scale_pos_weight= trial.suggest_float('scale_pos_weight', 0.5, 1.5),\n",
        "        subsample= trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        tree_method= trial.suggest_categorical(\n",
        "            'tree_method',\n",
        "             ['auto', 'approx', 'hist']\n",
        "        ),\n",
        "        validate_parameters=True,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val_fold)\n",
        "\n",
        "    # Step 5: Evaluate using RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n",
        "    gc.collect()\n",
        "\n",
        "    trial.report(np.mean(rmse_list), step=fold_idx)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "  return np.mean(rmse_list)\n",
        "\n",
        "# Start MLflow experiment\n",
        "mlflow.set_experiment(mlflow_experiment)\n",
        "\n",
        "with mlflow.start_run():\n",
        "  # - Ensure to use the same direction,\n",
        "  # pruner (min_resource, max_resource, reduction_factor, etc.) if you are\n",
        "  # loading an existing study\n",
        "  study = optuna.create_study(\n",
        "      study_name=study_name,\n",
        "      storage=storage,\n",
        "      direction=\"minimize\",\n",
        "      pruner=optuna.pruners.HyperbandPruner(\n",
        "          min_resource=max(1, n_splits),\n",
        "          max_resource=n_splits * n_repeats,\n",
        "          reduction_factor=2\n",
        "      ),\n",
        "      sampler=optuna.samplers.TPESampler(\n",
        "          n_startup_trials=max(1, int(n_trials * 0.2)),\n",
        "          seed=RANDOM_STATE,\n",
        "          multivariate=True,\n",
        "          warn_independent_sampling = False\n",
        "      ),\n",
        "      load_if_exists=True\n",
        "  )\n",
        "\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  log_optuna_best_trial_search_space(study)\n",
        "\n",
        "  # Log Optuna storage details\n",
        "  mlflow.log_param(\"optuna_storage\", storage)\n",
        "  mlflow.log_param(\"optuna_study_name\", study_name)\n",
        "\n",
        "  # Log experiment parameters\n",
        "  mlflow.log_param(\"n_trials\", n_trials)\n",
        "  mlflow.log_param(\"n_splits\", n_splits)\n",
        "  mlflow.log_param(\"n_repeats\", n_repeats)\n",
        "\n",
        "  # Fanova Importance is more robust than the Mean Decrease Impurity Importance\n",
        "  fanova_importances = get_param_importances(\n",
        "      study,\n",
        "      evaluator=FanovaImportanceEvaluator(seed = RANDOM_STATE),\n",
        "      target= (\n",
        "          lambda t: t.value\n",
        "          if t.state == optuna.trial.TrialState.COMPLETE\n",
        "          else None\n",
        "      ),\n",
        "      normalize = True\n",
        "  )\n",
        "\n",
        "  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n",
        "\n",
        "  # Log Fanova importances as parameters\n",
        "  for param, importance in fanova_importances.items():\n",
        "    mlflow.log_param(f\"fanova_{param}\", importance)\n",
        "\n",
        "  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n",
        "\n",
        "  # Step 7: Get best IsolationForest params\n",
        "  best_params = study.best_params\n",
        "\n",
        "  for param, value in best_params.items():\n",
        "    mlflow.log_param(param, value)\n",
        "\n",
        "  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n",
        "  print(\"Best Params:\", best_params)\n",
        "  print(\"Best CV RMSE:\", study.best_value)\n",
        "\n",
        "  # Apply best winsorization and compute thresholds from training data\n",
        "  y_train_full = np.log1p(mstats.winsorize(\n",
        "      df_train[TARGET], limits=[0, 1-best_params[\"winsorize_upper\"]]\n",
        "  ))\n",
        "\n",
        "  # Compute winsorization thresholds from training data\n",
        "  # Apply fixed thresholds to validation data\n",
        "  y_val = np.log1p(np.clip(\n",
        "      df_val[TARGET],\n",
        "      0,\n",
        "      np.percentile(df_train[TARGET], best_params[\"winsorize_upper\"] * 100)\n",
        "  ))\n",
        "\n",
        "\n",
        "  # Apply best outlier method on training data\n",
        "  if best_params[\"outlier_method\"] == \"IsolationForest\":\n",
        "    contamination = (\n",
        "        best_params[\"if_contamination\"]\n",
        "        if best_params[\"if_contamination_option\"] == \"float\"\n",
        "        else 'auto'\n",
        "    )\n",
        "    detector = IsolationForest(\n",
        "        n_estimators=best_params[\"if_n_estimators\"],\n",
        "        max_samples=best_params[\"if_max_samples\"],\n",
        "        contamination=contamination,\n",
        "        max_features=best_params[\"if_max_features\"],\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    mask = detector.fit_predict(X_train_full_scaled) == 1\n",
        "    num_outliers = len(X_train_full_scaled) - np.sum(mask)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"LocalOutlierFactor\":\n",
        "    contamination = (\n",
        "        best_params[\"lof_contamination\"]\n",
        "        if best_params[\"lof_contamination_option\"] == \"float\"\n",
        "        else 'auto'\n",
        "    )\n",
        "    detector = LocalOutlierFactor(\n",
        "        n_neighbors=best_params[\"lof_n_neighbors\"],\n",
        "        algorithm=best_params[\"lof_algorithm\"],\n",
        "        leaf_size=best_params[\"lof_leaf_size\"],\n",
        "        metric=best_params[\"lof_metric\"],\n",
        "        p=best_params[\"lof_p\"],\n",
        "        contamination=contamination,\n",
        "        novelty=False\n",
        "    )\n",
        "    mask = detector.fit_predict(X_train_full_scaled) == 1\n",
        "    num_outliers = len(X_train_full_scaled) - np.sum(mask)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"OneClassSVM\":\n",
        "    detector = OneClassSVM(\n",
        "        kernel=best_params[\"ocsvm_kernel\"],\n",
        "        degree=best_params[\"ocsvm_degree\"],\n",
        "        gamma=best_params[\"ocsvm_gamma\"],\n",
        "        coef0=best_params[\"ocsvm_coef0\"],\n",
        "        tol=best_params[\"ocsvm_tol\"],\n",
        "        nu=best_params[\"ocsvm_nu\"],\n",
        "        shrinking=best_params[\"ocsvm_shrinking\"],\n",
        "        max_iter=best_params[\"ocsvm_max_iter\"],\n",
        "        verbose=0\n",
        "    )\n",
        "    mask = detector.fit_predict(X_train_full_scaled) == 1\n",
        "    num_outliers = len(X_train_full_scaled) - np.sum(mask)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"DBSCAN\":\n",
        "    detector = DBSCAN(\n",
        "        eps=best_params[\"dbscan_eps\"],\n",
        "        min_samples=best_params[\"dbscan_min_samples\"],\n",
        "        algorithm=best_params[\"dbscan_algorithm\"],\n",
        "        leaf_size=best_params[\"dbscan_leaf_size\"],\n",
        "        metric=best_params[\"dbscan_metric\"],\n",
        "        p=best_params[\"dbscan_p\"]\n",
        "    )\n",
        "    cluster_labels = detector.fit_predict(X_train_full_scaled)\n",
        "    mask = cluster_labels != -1\n",
        "    num_outliers = np.sum(cluster_labels == -1)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"ZScore\":\n",
        "    zscore_threshold = best_params[\"zscore_threshold\"]\n",
        "    z_scores = np.abs(zscore(X_train_full, axis=0))\n",
        "    mask = (z_scores < zscore_threshold).all(axis=1)\n",
        "    num_outliers = len(X_train_full) - np.sum(mask)\n",
        "\n",
        "  # Log number of outliers and percentage\n",
        "  mlflow.log_metric(\"num_outliers\", num_outliers)\n",
        "  print(f\"Number of outliers detected: {num_outliers}\")\n",
        "  outlier_percentage = (num_outliers / len(X_train_full_scaled)) * 100\n",
        "  mlflow.log_metric(\"outlier_percentage\", outlier_percentage)\n",
        "  print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n",
        "\n",
        "  df_train = df_train[mask]\n",
        "  X_final = X_train_full[mask]\n",
        "  y_final = y_train_full[mask]\n",
        "\n",
        "  valid_params = {\n",
        "      k: v\n",
        "      for k, v in best_params.items()\n",
        "      if k in xgb.XGBRegressor().get_params()\n",
        "  }\n",
        "\n",
        "  # all the constant hyperparameters should be added here manually and are not\n",
        "  # part of study.best_params\n",
        "  final_model = xgb.XGBRegressor(\n",
        "      **valid_params,\n",
        "      n_jobs=N_JOBS,\n",
        "      random_state=RANDOM_STATE,\n",
        "      validate_parameters=True,\n",
        "      verbosity=0\n",
        "  )\n",
        "\n",
        "  X_final = X_final[sorted(X_final.columns)]\n",
        "  X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "  X_val_scaled = X_val_scaled[sorted(X_val_scaled.columns)]\n",
        "  final_model.fit(X_final, y_final)\n",
        "  y_pred = final_model.predict(X_val_scaled)\n",
        "\n",
        "  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")"
      ],
      "metadata": {
        "id": "-aiNUEaY4JD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isolation Forest Transformer"
      ],
      "metadata": {
        "id": "ZErqT9S6CDS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile isolation_forest_transformer.py\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "class IsolationForestTransformer(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  A class to remove outliers from a DataFrame using IsolationForest with\n",
        "  fixed hyperparameters.\n",
        "\n",
        "  Attributes:\n",
        "  - contamination (float): Proportion of outliers in the data.\n",
        "  - n_estimators (int): Number of trees in the IsolationForest.\n",
        "  - max_samples (float): Proportion of samples to draw for each tree.\n",
        "  - max_features (float): Proportion of features to consider for each split.\n",
        "  - random_state (int): Random seed for reproducibility.\n",
        "  - scaler (StandardScaler): Scaler for numerical features.\n",
        "  - model (IsolationForest): Fitted IsolationForest model.\n",
        "  - numerical_cols (list): List of numerical column names.\n",
        "  - fitted (bool): Whether the model has been fitted.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      contamination,\n",
        "      n_estimators,\n",
        "      max_samples,\n",
        "      max_features,\n",
        "      random_state\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Initialize the IsolationForestWrapper with fixed IsolationForest\n",
        "    hyperparameters.\n",
        "\n",
        "    Args:\n",
        "    - contamination (float): Proportion of outliers in the data.\n",
        "    - n_estimators (int): Number of trees in the IsolationForest.\n",
        "    - max_samples (float): Proportion of samples to draw for each tree.\n",
        "    - max_features (float): Proportion of features to consider for each\n",
        "    split.\n",
        "    - random_state (int): Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    self.contamination = contamination\n",
        "    self.n_estimators = n_estimators\n",
        "    self.max_samples = max_samples\n",
        "    self.max_features = max_features\n",
        "    self.random_state = random_state\n",
        "    self.scaler = StandardScaler()\n",
        "    self.model = None\n",
        "    self.numerical_cols = None\n",
        "    self.fitted = False\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit the IsolationForest model on numerical features of the input\n",
        "    DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Input DataFrame containing numerical features.\n",
        "    - y (None): Ignored, included for scikit-learn compatibility.\n",
        "\n",
        "    Returns:\n",
        "    - self: Fitted IsolationForestWrapper instance.\n",
        "    \"\"\"\n",
        "    # Select numerical columns\n",
        "    self.numerical_cols = (\n",
        "        X\n",
        "        .select_dtypes(include=['int64', 'float64'])\n",
        "        .columns\n",
        "        .tolist()\n",
        "    )\n",
        "\n",
        "    # Extract numerical features\n",
        "    X_num = X[self.numerical_cols]\n",
        "\n",
        "    # Fit scaler\n",
        "    self.scaler.fit(X_num)\n",
        "\n",
        "    # Scale features\n",
        "    X_scaled = self.scaler.transform(X_num)\n",
        "\n",
        "    # Initialize and fit IsolationForest\n",
        "    self.model = IsolationForest(\n",
        "        contamination=self.contamination,\n",
        "        n_estimators=self.n_estimators,\n",
        "        max_samples=self.max_samples,\n",
        "        max_features=self.max_features,\n",
        "        random_state=self.random_state\n",
        "    )\n",
        "\n",
        "    self.model.fit(X_scaled)\n",
        "\n",
        "    self.fitted = True\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Remove outliers from the input DataFrame using the fitted\n",
        "    IsolationForest model.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Input DataFrame to filter outliers from.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Filtered DataFrame with outliers removed.\n",
        "\n",
        "    Raises:\n",
        "    - ValueError: If fit has not been called or if numerical columns mismatch.\n",
        "    \"\"\"\n",
        "    if not self.fitted:\n",
        "      raise ValueError(\"Model must be fitted before transform\")\n",
        "\n",
        "    # Extract numerical features\n",
        "    X_num = X[self.numerical_cols]\n",
        "\n",
        "    # Scale features\n",
        "    X_scaled = self.scaler.transform(X_num)\n",
        "\n",
        "    # Predict outliers (-1 for outliers, 1 for inliers)\n",
        "    mask = self.model.predict(X_scaled) == 1\n",
        "\n",
        "    # Return filtered DataFrame\n",
        "    return X[mask].copy()"
      ],
      "metadata": {
        "id": "yksSm0EYaxs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "zhpZW_P7CIGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from isolation_forest_transformer import IsolationForestTransformer\n",
        "\n",
        "isolation_forest_transformer = IsolationForestTransformer(\n",
        "    contamination = 0.07971598036312164,\n",
        "    n_estimators = 65,\n",
        "    max_samples = 0.9907725638374102,\n",
        "    max_features = 0.315372534187923,\n",
        "    random_state = RANDOM_STATE\n",
        ")\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=isolation_forest_transformer,\n",
        "    experiment_name=\"Isolation_Forest_Transformer\",\n",
        "    model_name=\"isolation_forest_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=None\n",
        ")"
      ],
      "metadata": {
        "id": "ijzAsMzDA_Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train"
      ],
      "metadata": {
        "id": "w2I3yhS4COkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We do not remove outliers from the validation/test data\n",
        "df_train = isolation_forest_transformer.transform(df_train)"
      ],
      "metadata": {
        "id": "1W_RtLwuCpYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Feature Distribution Mismatches"
      ],
      "metadata": {
        "id": "fBdJw7Ql_0vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_feature_distribution_mismatches(\n",
        "    df_train[skewed_cols],\n",
        "    df_val[skewed_cols],\n",
        "    test_type=\"Kolmogorov-Smirnov\"\n",
        ")"
      ],
      "metadata": {
        "id": "-RwTi2DM_1-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Feature Engineering with Leakage Prevention\n",
        "This custom transformer extracts time-based features (`day_of_week`, `month`, `is_weekend`) and computes historical averages for `publisher` and `market_id` (e.g., past average clicks, popularity) - all derived **only from prior dates** to prevent data leakage.\n",
        "\n",
        "Key design choices:\n",
        "\n",
        "- **Temporal safety:** Rolling stats use only rows where `date < current_date`.\n",
        "- **Production-ready:** Inherits from BaseEstimator/TransformerMixin for pipeline integration.\n",
        "- **Robust handling:** Maps unknown categories using global averages or zero fallbacks.\n",
        "- **Validation:** Checks input types, required columns, and datetime formatting.\n",
        "Ideal for time-ordered advertising data where realistic feature simulation is critical."
      ],
      "metadata": {
        "id": "PHc485PxPViG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporal Feature Engineer Transformer"
      ],
      "metadata": {
        "id": "aLoDDIgPCawG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile temporal_feature_engineer_transformer.py\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TemporalFeatureEngineerTransformer(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  A class to perform temporal feature engineering on advertising data.\n",
        "\n",
        "  Extracts day_of_week, month, is_weekend from date, and computes rolling\n",
        "  averages and popularity metrics for publisher and market_id based on clicks.\n",
        "\n",
        "  Attributes:\n",
        "  - required_cols (list): Required columns in input DataFrame.\n",
        "  - publisher_avg_clicks_map (pd.Series): Mapping of publisher to mean clicks.\n",
        "  - market_avg_clicks_map (pd.Series): Mapping of market_id to mean clicks.\n",
        "  - publisher_popularity_map (pd.Series): Mapping of publisher to mean\n",
        "  popularity.\n",
        "  - market_popularity_map (pd.Series): Mapping of market_id to mean popularity.\n",
        "  - fitted (bool): Whether the model has been fitted.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      required_cols=['date', 'publisher', 'market_id', 'clicks']\n",
        "  ):\n",
        "    \"\"\"Initialize the TemporalFeatureEngineer.\n",
        "\n",
        "    Args:\n",
        "    required_cols (list): List of required columns in input DataFrame.\n",
        "    \"\"\"\n",
        "    self.required_cols = required_cols\n",
        "    self.publisher_avg_clicks_map = None\n",
        "    self.market_avg_clicks_map = None\n",
        "    self.publisher_popularity_map = None\n",
        "    self.market_popularity_map = None\n",
        "    self.fitted = False\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit the feature engineer on training data, computing mappings.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Training DataFrame with required columns.\n",
        "    - y (None): Ignored, included for scikit-learn compatibility.\n",
        "\n",
        "    Returns:\n",
        "    - self: Fitted TemporalFeatureEngineer instance.\n",
        "\n",
        "    Raises:\n",
        "    - TypeError: If X is not a pandas DataFrame.\n",
        "    - ValueError: If required columns are missing or date is not datetime.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise TypeError(\"Input X must be a pandas DataFrame\")\n",
        "\n",
        "    if not all(col in X.columns for col in self.required_cols):\n",
        "      raise ValueError(\n",
        "          f\"Input DataFrame must contain all required columns:\\n\"\n",
        "          f\"{self.required_cols}\"\n",
        "      )\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(X['date']):\n",
        "      raise ValueError(\"Column 'date' must be of datetime type\")\n",
        "\n",
        "    # Compute global mean for fallback\n",
        "    global_mean_clicks = X['clicks'].mean()\n",
        "\n",
        "    # Initialize lists for rolling features\n",
        "    rolling_publisher_avg_clicks = []\n",
        "    rolling_market_avg_clicks = []\n",
        "    publisher_popularity = []\n",
        "    market_popularity = []\n",
        "\n",
        "    # Iterate over rows to respect temporal order\n",
        "    for idx, row in X.iterrows():\n",
        "      past_data = X[X['date'] < row['date']]\n",
        "\n",
        "      # Publisher-level rolling average\n",
        "      past_publisher_data = past_data[\n",
        "          past_data['publisher'] == row['publisher']\n",
        "      ]\n",
        "\n",
        "      pub_avg = (\n",
        "          past_publisher_data['clicks'].mean()\n",
        "          if not past_publisher_data.empty\n",
        "          else global_mean_clicks\n",
        "      )\n",
        "\n",
        "      rolling_publisher_avg_clicks.append(pub_avg)\n",
        "\n",
        "      # Market-level rolling average\n",
        "      past_market_data = past_data[past_data['market_id'] == row['market_id']]\n",
        "\n",
        "      market_avg = (\n",
        "          past_market_data['clicks'].mean()\n",
        "          if not past_market_data.empty\n",
        "          else global_mean_clicks\n",
        "      )\n",
        "\n",
        "      rolling_market_avg_clicks.append(market_avg)\n",
        "\n",
        "      # Publisher popularity (mean clicks per publisher in past)\n",
        "      pub_pop = (\n",
        "          past_publisher_data['clicks'].sum() / len(past_publisher_data)\n",
        "          if not past_publisher_data.empty\n",
        "          else 0\n",
        "      )\n",
        "\n",
        "      publisher_popularity.append(pub_pop)\n",
        "\n",
        "      # Market popularity (mean clicks per market in past)\n",
        "      market_pop = (\n",
        "          past_market_data['clicks'].sum() / len(past_market_data)\n",
        "          if not past_market_data.empty\n",
        "          else 0\n",
        "      )\n",
        "\n",
        "      market_popularity.append(market_pop)\n",
        "\n",
        "    # Create temporary DataFrame for mappings\n",
        "    temp_df = X.copy()\n",
        "    temp_df['publisher_avg_clicks'] = rolling_publisher_avg_clicks\n",
        "    temp_df['market_avg_clicks'] = rolling_market_avg_clicks\n",
        "    temp_df['publisher_popularity'] = publisher_popularity\n",
        "    temp_df['market_popularity'] = market_popularity\n",
        "\n",
        "    # Compute mappings\n",
        "    self.publisher_avg_clicks_map = (\n",
        "        temp_df.groupby('publisher')['clicks']\n",
        "        .mean()\n",
        "    )\n",
        "\n",
        "    self.market_avg_clicks_map = temp_df.groupby('market_id')['clicks'].mean()\n",
        "\n",
        "    self.publisher_popularity_map = (\n",
        "        temp_df.groupby('publisher')['publisher_popularity']\n",
        "        .mean()\n",
        "    )\n",
        "\n",
        "    self.market_popularity_map = (\n",
        "        temp_df.groupby('market_id')['market_popularity']\n",
        "        .mean()\n",
        "    )\n",
        "\n",
        "    self.fitted = True\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Transform the input DataFrame by adding temporal features and applying\n",
        "    mappings.\n",
        "\n",
        "    Args:\n",
        "    - X (pd.DataFrame): Input DataFrame to transform.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Transformed DataFrame with new features and date column\n",
        "    dropped.\n",
        "\n",
        "    Raises:\n",
        "    - TypeError: If X is not a pandas DataFrame.\n",
        "    - ValueError: If model is not fitted or required columns are missing.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise TypeError(\"Input X must be a pandas DataFrame\")\n",
        "    if not self.fitted:\n",
        "      raise ValueError(\"Model must be fitted before transform\")\n",
        "    if not all(col in X.columns for col in self.required_cols):\n",
        "      raise ValueError(f\"\"\"\n",
        "      Input DataFrame must contain all required columns: {self.required_cols}\n",
        "      \"\"\")\n",
        "\n",
        "    # Create a copy to avoid modifying the input\n",
        "    X_transformed = X.copy()\n",
        "\n",
        "    # Add temporal features\n",
        "    X_transformed['day_of_week'] = (\n",
        "        X_transformed['date']\n",
        "        .dt\n",
        "        .strftime('%A')\n",
        "        .astype('string')\n",
        "    )\n",
        "\n",
        "    X_transformed['month'] = (\n",
        "        X_transformed['date']\n",
        "        .dt\n",
        "        .strftime('%B')\n",
        "        .astype('string')\n",
        "    )\n",
        "\n",
        "    X_transformed['is_weekend'] = (\n",
        "        X_transformed['day_of_week']\n",
        "        .isin(['Saturday', 'Sunday'])\n",
        "    )\n",
        "\n",
        "    # Apply mappings for rolling averages and popularity\n",
        "    global_mean_clicks = (\n",
        "        self.publisher_avg_clicks_map.mean()\n",
        "        if self.publisher_avg_clicks_map is not None\n",
        "        else 0\n",
        "    )\n",
        "\n",
        "    X_transformed['publisher_avg_clicks'] = (\n",
        "        X_transformed['publisher']\n",
        "        .map(self.publisher_avg_clicks_map)\n",
        "        .fillna(global_mean_clicks)\n",
        "    )\n",
        "\n",
        "    X_transformed['market_avg_clicks'] = (\n",
        "        X_transformed['market_id']\n",
        "        .map(self.market_avg_clicks_map)\n",
        "        .fillna(global_mean_clicks)\n",
        "    )\n",
        "    X_transformed['publisher_popularity'] = (\n",
        "        X_transformed['publisher']\n",
        "        .map(self.publisher_popularity_map)\n",
        "        .fillna(0)\n",
        "    )\n",
        "\n",
        "    X_transformed['market_popularity'] = (\n",
        "        X_transformed['market_id']\n",
        "        .map(self.market_popularity_map)\n",
        "        .fillna(0)\n",
        "    )\n",
        "\n",
        "    # Drop date column\n",
        "    X_transformed.drop(columns=['date'], inplace=True)\n",
        "    return X_transformed"
      ],
      "metadata": {
        "id": "ZmKzj25uA-6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'temporal_feature_engineer_transformer.py',\n",
        "    destination_path = PATH / 'temporal_feature_engineer_transformer.py'\n",
        ")"
      ],
      "metadata": {
        "id": "h3NVYP7Wku0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "zdkh1id4Cf8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from temporal_feature_engineer_transformer import (\n",
        "    TemporalFeatureEngineerTransformer\n",
        ")\n",
        "\n",
        "temporal_feature_engineer_transformer = TemporalFeatureEngineerTransformer(\n",
        "    required_cols=['date', 'publisher', 'market_id', 'clicks']\n",
        ")\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=temporal_feature_engineer_transformer,\n",
        "    experiment_name=\"Temporal_Feature_Engineer_Transformer\",\n",
        "    model_name=\"temporal_feature_engineer_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "5v_qBkl3B5yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train and Validation"
      ],
      "metadata": {
        "id": "3_1A0t7hCh0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = apply_transformer(\n",
        "    df_train,\n",
        "    df_val,\n",
        "    temporal_feature_engineer_transformer,\n",
        "    TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "Kfo5ftGTFNED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Train and Validation Data for EDA"
      ],
      "metadata": {
        "id": "SJzrFVfqVqiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_object(df_train, 'df_train_eda.pkl')\n",
        "save_object(df_val, 'df_val_eda.pkl')"
      ],
      "metadata": {
        "id": "jLfBso01VvvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropping Irrelevant Columns  \n",
        "\n",
        "Before training the model, some columns should be removed because they are either not available before campaign launch or do not provide predictive value:  \n",
        "\n",
        "- **`cost`**, **`conversions`**, and **`clicks`**: These columns contain information that is only available **after** a campaign has been launched. Since we want to predict outcomes beforehand, they must be excluded.  \n",
        "- **`customer_id`**: This is simply a random identifier with no predictive power and should therefore be dropped.  \n",
        "\n",
        "The `DropColumnsTransformer` class below implements this logic as a scikit-learn compatible transformer.\n"
      ],
      "metadata": {
        "id": "Qo7m-jr9KBJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Columns Transformer"
      ],
      "metadata": {
        "id": "lD37qoWLClq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile drop_columns_transformer.py\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# - drop the columns cost, conversions and clicks as you will not have them\n",
        "# before the campaign is launched.\n",
        "# - customer_id does not have any predictive power as it is just a random id.\n",
        "class DropColumnsTransformer(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  A scikit-learn compatible transformer to drop specified columns from\n",
        "  a DataFrame.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      columns_to_drop=['cost', 'conversions', 'customer_id', 'clicks']\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Initialize the transformer with a list of columns to drop.\n",
        "\n",
        "    Parameters:\n",
        "    columns_to_drop : list,\n",
        "    default=['cost', 'conversions', 'customer_id', 'clicks']\n",
        "    List of column names to drop from the DataFrame.\n",
        "    \"\"\"\n",
        "    self.columns_to_drop = columns_to_drop\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit method (no-op, included for scikit-learn compatibility).\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame, Input data.\n",
        "\n",
        "    - y : None or array-like, default=None, Ignored. Included for compatibility.\n",
        "\n",
        "    Returns:\n",
        "    self : DropColumnsTransformer\n",
        "    Returns the instance itself.\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"Input X must be a pandas DataFrame\")\n",
        "\n",
        "    # Check which columns to drop exist in X\n",
        "    self.columns_to_drop_ = [\n",
        "        col for col in self.columns_to_drop if col in X.columns\n",
        "    ]\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Transform method to drop specified columns from the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    X : pandas DataFrame\n",
        "    Input data.\n",
        "\n",
        "    Returns:\n",
        "    X_transformed : pandas DataFrame\n",
        "    DataFrame with specified columns dropped.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"Input X must be a pandas DataFrame\")\n",
        "\n",
        "    # Create a copy to avoid modifying the original DataFrame\n",
        "    X_transformed = X.copy()\n",
        "\n",
        "    # Drop columns that exist in the DataFrame\n",
        "    X_transformed.drop(\n",
        "        columns=self.columns_to_drop_,\n",
        "        inplace=True,\n",
        "        errors='ignore'\n",
        "    )\n",
        "\n",
        "    return X_transformed"
      ],
      "metadata": {
        "id": "O4OHaKqOKHHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'drop_columns_transformer.py',\n",
        "    destination_path = PATH / 'drop_columns_transformer.py'\n",
        ")"
      ],
      "metadata": {
        "id": "dLhxN_ioA4V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "Abg3aVJsCpV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from drop_columns_transformer import DropColumnsTransformer\n",
        "\n",
        "drop_columns_transformer = DropColumnsTransformer(\n",
        "    columns_to_drop=['cost', 'conversions', 'customer_id', 'clicks']\n",
        ")\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=drop_columns_transformer,\n",
        "    experiment_name=\"Drop_Columns\",\n",
        "    model_name=\"drop_columns_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "KE2vnZie0ScG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train and Validation"
      ],
      "metadata": {
        "id": "jIIBDvRgCsfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = apply_transformer(\n",
        "    df_train,\n",
        "    df_val,\n",
        "    drop_columns_transformer,\n",
        "    TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "4MPYWWMa1rTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Detection After Feature Engineering  \n",
        "\n",
        "After feature engineering, **new columns are introduced** that were not present in the original dataset.  \n",
        "It is important to re-check for outliers at this stage because:  \n",
        "\n",
        "- Outliers should now be detected **only on the newly engineered columns**, not on the original ones.  \n",
        "- If ignored, these new features may introduce **skewness or distortions** into the data distribution, which can negatively affect model training and performance.  \n",
        "\n",
        "The following code performs outlier detection again, focusing specifically on the engineered features.\n"
      ],
      "metadata": {
        "id": "dppsRC5-L7lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define columns for outlier detection\n",
        "outlier_cols = [\n",
        "    'publisher_avg_clicks',\n",
        "    'market_avg_clicks',\n",
        "    'publisher_popularity',\n",
        "    'market_popularity'\n",
        "]\n",
        "\n",
        "# Select all numeric columns for model training, but only scale outlier_cols\n",
        "# for oulier detection\n",
        "numeric_cols = (\n",
        "    df_train\n",
        "    .select_dtypes(include=['int64', 'float64'])\n",
        "    .columns\n",
        "    .tolist()\n",
        ")\n",
        "\n",
        "# Ensure outlier_cols exist in numeric_cols\n",
        "outlier_cols = [col for col in outlier_cols if col in numeric_cols]\n",
        "\n",
        "X_train_outlier = df_train[outlier_cols]\n",
        "X_train_full = df_train[numeric_cols]\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_outlier)\n",
        "X_train_outlier_scaled = scaler.transform(X_train_outlier)\n",
        "X_val = df_val[numeric_cols]\n",
        "\n",
        "n_trials = 500\n",
        "n_splits = 5\n",
        "n_repeats = 3\n",
        "mlflow_experiment = \"Outlier_Detection_Again\"\n",
        "\n",
        "# SQLite storage configuration\n",
        "study_name = f\"{mlflow_experiment}_study\"\n",
        "storage = f\"sqlite:///{PATH}/{study_name}.db\"\n",
        "\n",
        "def objective(trial):\n",
        "  winsorize_upper = trial.suggest_float(\"winsorize_upper\", 0.90, 1)\n",
        "\n",
        "  # Apply winsorization to target\n",
        "  y_train_full = np.log1p(\n",
        "      mstats.winsorize(df_train[TARGET], limits=[0, 1-winsorize_upper])\n",
        "  )\n",
        "\n",
        "  outlier_method = trial.suggest_categorical(\n",
        "      \"outlier_method\",\n",
        "      [\n",
        "          \"IsolationForest\",\n",
        "          \"LocalOutlierFactor\",\n",
        "          \"OneClassSVM\",\n",
        "          \"DBSCAN\",\n",
        "          \"ZScore\"\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  if outlier_method == \"IsolationForest\":\n",
        "    contamination_option = trial.suggest_categorical(\n",
        "        \"if_contamination_option\",\n",
        "        [\"float\"]\n",
        "    )\n",
        "    contamination = trial.suggest_float(\"if_contamination\", 0.01, 0.2)\n",
        "\n",
        "    detector = IsolationForest(\n",
        "        n_estimators=trial.suggest_int(\"if_n_estimators\", 50, 300),\n",
        "        max_samples=trial.suggest_float(\"if_max_samples\", 0.1, 1.0),\n",
        "        contamination=contamination,\n",
        "        max_features=trial.suggest_float(\"if_max_features\", 0.1, 1.0),\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    mask = detector.fit_predict(X_train_outlier_scaled) == 1\n",
        "\n",
        "  elif outlier_method == \"LocalOutlierFactor\":\n",
        "    contamination_option = trial.suggest_categorical(\n",
        "        \"lof_contamination_option\",\n",
        "        [\"float\"]\n",
        "    )\n",
        "\n",
        "    contamination = trial.suggest_float(\"lof_contamination\", 0.01, 0.2)\n",
        "\n",
        "    detector = LocalOutlierFactor(\n",
        "        n_neighbors=trial.suggest_int(\"lof_n_neighbors\", 5, 150),\n",
        "        algorithm=trial.suggest_categorical(\n",
        "            \"lof_algorithm\",\n",
        "            [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "        ),\n",
        "\n",
        "        leaf_size=trial.suggest_int(\"lof_leaf_size\", 10, 100),\n",
        "        metric=trial.suggest_categorical(\n",
        "            \"lof_metric\",\n",
        "            [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\n",
        "        ),\n",
        "\n",
        "        p=trial.suggest_int(\"lof_p\", 1, 5),\n",
        "        contamination=contamination,\n",
        "        n_jobs=N_JOBS\n",
        "    )\n",
        "\n",
        "    mask = detector.fit_predict(X_train_outlier_scaled) == 1\n",
        "\n",
        "  elif outlier_method == \"OneClassSVM\":\n",
        "    detector = OneClassSVM(\n",
        "        kernel=trial.suggest_categorical(\n",
        "            \"ocsvm_kernel\",\n",
        "            [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "        ),\n",
        "\n",
        "        degree=trial.suggest_int(\"ocsvm_degree\", 2, 5),\n",
        "        gamma=trial.suggest_float(\"ocsvm_gamma\", 0.001, 1.0, log=True),\n",
        "        coef0=trial.suggest_float(\"ocsvm_coef0\", 0.0, 1.0),\n",
        "        tol=trial.suggest_float(\"ocsvm_tol\", 1e-4, 1e-2),\n",
        "        nu=trial.suggest_float(\"ocsvm_nu\", 0.01, 0.5),\n",
        "        shrinking=trial.suggest_categorical(\"ocsvm_shrinking\", [True, False]),\n",
        "        max_iter=trial.suggest_int(\"ocsvm_max_iter\", 100, 10000),\n",
        "        verbose=0\n",
        "    )\n",
        "    mask = detector.fit_predict(X_train_outlier_scaled) == 1\n",
        "\n",
        "  elif outlier_method == \"DBSCAN\":\n",
        "    detector = DBSCAN(\n",
        "        eps=trial.suggest_float(\"dbscan_eps\", 0.1, 5.0),\n",
        "        min_samples=trial.suggest_int(\"dbscan_min_samples\", 3, 20),\n",
        "        algorithm=trial.suggest_categorical(\n",
        "            \"dbscan_algorithm\",\n",
        "            [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "        ),\n",
        "\n",
        "        leaf_size=trial.suggest_int(\"dbscan_leaf_size\", 10, 100),\n",
        "        metric=trial.suggest_categorical(\n",
        "            \"dbscan_metric\",\n",
        "            [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\n",
        "        ),\n",
        "\n",
        "        p=trial.suggest_int(\"dbscan_p\", 1, 5),\n",
        "        n_jobs=N_JOBS\n",
        "    )\n",
        "\n",
        "    cluster_labels = detector.fit_predict(X_train_outlier_scaled)\n",
        "    mask = cluster_labels != -1\n",
        "\n",
        "  elif outlier_method == \"ZScore\":\n",
        "    threshold = trial.suggest_float(\"zscore_threshold\", 2.0, 4.0)\n",
        "    z_scores = np.abs(zscore(X_train_outlier, axis=0))\n",
        "    mask = (z_scores < threshold).all(axis=1)\n",
        "\n",
        "  # Apply mask to full dataset\n",
        "  X_clean = X_train_full[mask]\n",
        "  y_clean = pd.Series(y_train_full[mask])\n",
        "\n",
        "  # Repeated K-Fold Cross-Validation\n",
        "  rkf = RepeatedKFold(\n",
        "      n_splits=n_splits,\n",
        "      n_repeats=n_repeats,\n",
        "      random_state=RANDOM_STATE\n",
        "  )\n",
        "\n",
        "  rmse_list = []\n",
        "\n",
        "  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_clean)):\n",
        "    X_train, X_val_fold = X_clean.iloc[train_idx], X_clean.iloc[val_idx]\n",
        "    y_train, y_val_fold = y_clean.iloc[train_idx], y_clean.iloc[val_idx]\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective=trial.suggest_categorical('objective', ['reg:squarederror']),\n",
        "        base_score=trial.suggest_float('base_score', 0.1, 0.9),\n",
        "        booster=trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        colsample_bylevel=trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
        "        colsample_bynode=trial.suggest_float('colsample_bynode', 0.5, 1.0),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        gamma=trial.suggest_float('gamma', 1e-6, 5, log=True),\n",
        "        grow_policy=trial.suggest_categorical(\n",
        "            'grow_policy',\n",
        "             ['depthwise', 'lossguide']\n",
        "        ),\n",
        "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "        max_bin=trial.suggest_int('max_bin', 128, 512),\n",
        "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
        "        max_leaves=trial.suggest_int('max_leaves', 0, 64),\n",
        "        min_child_weight=trial.suggest_float('min_child_weight', 1, 10),\n",
        "        n_estimators=trial.suggest_int('n_estimators', 50, 500),\n",
        "        n_jobs=N_JOBS,\n",
        "        num_parallel_tree=trial.suggest_int('num_parallel_tree', 1, 3),\n",
        "        random_state=RANDOM_STATE,\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 1e-6, 1.0, log=True),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 1e-6, 1.0, log=True),\n",
        "        #GPU is needed to use 'gradient_based' as sampling_method\n",
        "        sampling_method='uniform',\n",
        "        scale_pos_weight=trial.suggest_float('scale_pos_weight', 0.5, 1.5),\n",
        "        subsample=trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        tree_method=trial.suggest_categorical(\n",
        "            'tree_method',\n",
        "             ['auto', 'approx', 'hist']\n",
        "        ),\n",
        "\n",
        "        validate_parameters=True,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val_fold)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n",
        "    gc.collect()\n",
        "\n",
        "    trial.report(np.mean(rmse_list), step=fold_idx)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "  return np.mean(rmse_list)\n",
        "\n",
        "# Start MLflow experiment\n",
        "mlflow.set_experiment(mlflow_experiment)\n",
        "\n",
        "with mlflow.start_run():\n",
        "  # - Ensure to use the same direction,\n",
        "  # pruner (min_resource, max_resource, reduction_factor, etc.) if you are\n",
        "  # loading an existing study\n",
        "  study = optuna.create_study(\n",
        "      study_name=study_name,\n",
        "      storage=storage,\n",
        "      direction=\"minimize\",\n",
        "      pruner=optuna.pruners.HyperbandPruner(\n",
        "          min_resource=max(1, n_splits),\n",
        "          max_resource=n_splits * n_repeats,\n",
        "          reduction_factor=2\n",
        "      ),\n",
        "      sampler=optuna.samplers.TPESampler(\n",
        "          n_startup_trials=max(1, int(n_trials * 0.2)),\n",
        "          seed=RANDOM_STATE,\n",
        "          multivariate=True\n",
        "      ),\n",
        "      load_if_exists=True\n",
        "  )\n",
        "\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  log_optuna_best_trial_search_space(study)\n",
        "\n",
        "  # Log Optuna storage details\n",
        "  mlflow.log_param(\"optuna_storage\", storage)\n",
        "  mlflow.log_param(\"optuna_study_name\", study_name)\n",
        "\n",
        "  # Log experiment parameters\n",
        "  mlflow.log_param(\"n_trials\", n_trials)\n",
        "  mlflow.log_param(\"n_splits\", n_splits)\n",
        "  mlflow.log_param(\"n_repeats\", n_repeats)\n",
        "\n",
        "  fanova_importances = get_param_importances(\n",
        "      study,\n",
        "      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n",
        "      target=(\n",
        "          lambda t: t.value\n",
        "          if t.state == optuna.trial.TrialState.COMPLETE\n",
        "          else None\n",
        "      ),\n",
        "      normalize=True\n",
        "  )\n",
        "\n",
        "  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n",
        "\n",
        "  for param, importance in fanova_importances.items():\n",
        "    mlflow.log_param(f\"fanova_{param}\", importance)\n",
        "\n",
        "  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n",
        "\n",
        "  best_params = study.best_params\n",
        "\n",
        "  for param, value in best_params.items():\n",
        "    mlflow.log_param(param, value)\n",
        "\n",
        "  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n",
        "  print(\"Best Params:\", best_params)\n",
        "  print(\"Best CV RMSE:\", study.best_value)\n",
        "\n",
        "  # Apply best winsorization and compute thresholds from training data\n",
        "  y_train_full = np.log1p(mstats.winsorize(\n",
        "      df_train[TARGET],\n",
        "      limits=[0, 1-best_params[\"winsorize_upper\"]]\n",
        "  ))\n",
        "\n",
        "  # Compute winsorization thresholds from training data\n",
        "  # Apply fixed thresholds to validation data\n",
        "  y_val = np.log1p(np.clip(\n",
        "      df_val[TARGET],\n",
        "      0,\n",
        "      np.percentile(df_train[TARGET], best_params[\"winsorize_upper\"] * 100)\n",
        "  ))\n",
        "\n",
        "  # Apply best outlier method\n",
        "  if best_params[\"outlier_method\"] == \"IsolationForest\":\n",
        "    contamination = (\n",
        "        best_params[\"if_contamination\"]\n",
        "        if best_params[\"if_contamination_option\"] == \"float\"\n",
        "        else 'auto'\n",
        "    )\n",
        "\n",
        "    detector = IsolationForest(\n",
        "        n_estimators=best_params[\"if_n_estimators\"],\n",
        "        max_samples=best_params[\"if_max_samples\"],\n",
        "        contamination=contamination,\n",
        "        max_features=best_params[\"if_max_features\"],\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    mask = detector.fit_predict(X_train_outlier_scaled) == 1\n",
        "    num_outliers = len(X_train_outlier_scaled) - np.sum(mask)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"LocalOutlierFactor\":\n",
        "    contamination = (\n",
        "        best_params[\"lof_contamination\"]\n",
        "        if best_params[\"lof_contamination_option\"] == \"float\"\n",
        "        else 'auto'\n",
        "    )\n",
        "\n",
        "    detector = LocalOutlierFactor(\n",
        "        n_neighbors=best_params[\"lof_n_neighbors\"],\n",
        "        algorithm=best_params[\"lof_algorithm\"],\n",
        "        leaf_size=best_params[\"lof_leaf_size\"],\n",
        "        metric=best_params[\"lof_metric\"],\n",
        "        p=best_params[\"lof_p\"],\n",
        "        contamination=contamination,\n",
        "        novelty=False\n",
        "    )\n",
        "\n",
        "    mask = detector.fit_predict(X_train_outlier_scaled) == 1\n",
        "    num_outliers = len(X_train_outlier_scaled) - np.sum(mask)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"OneClassSVM\":\n",
        "    detector = OneClassSVM(\n",
        "        kernel=best_params[\"ocsvm_kernel\"],\n",
        "        degree=best_params[\"ocsvm_degree\"],\n",
        "        gamma=best_params[\"ocsvm_gamma\"],\n",
        "        coef0=best_params[\"ocsvm_coef0\"],\n",
        "        tol=best_params[\"ocsvm_tol\"],\n",
        "        nu=best_params[\"ocsvm_nu\"],\n",
        "        shrinking=best_params[\"ocsvm_shrinking\"],\n",
        "        max_iter=best_params[\"ocsvm_max_iter\"],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    mask = detector.fit_predict(X_train_outlier_scaled) == 1\n",
        "    num_outliers = len(X_train_outlier_scaled) - np.sum(mask)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"DBSCAN\":\n",
        "    detector = DBSCAN(\n",
        "        eps=best_params[\"dbscan_eps\"],\n",
        "        min_samples=best_params[\"dbscan_min_samples\"],\n",
        "        algorithm=best_params[\"dbscan_algorithm\"],\n",
        "        leaf_size=best_params[\"dbscan_leaf_size\"],\n",
        "        metric=best_params[\"dbscan_metric\"],\n",
        "        p=best_params[\"dbscan_p\"]\n",
        "    )\n",
        "\n",
        "    cluster_labels = detector.fit_predict(X_train_outlier_scaled)\n",
        "    mask = cluster_labels != -1\n",
        "    num_outliers = np.sum(cluster_labels == -1)\n",
        "\n",
        "  elif best_params[\"outlier_method\"] == \"ZScore\":\n",
        "    zscore_threshold = best_params[\"zscore_threshold\"]\n",
        "    z_scores = np.abs(zscore(X_train_outlier, axis=0))\n",
        "    mask = (z_scores < zscore_threshold).all(axis=1)\n",
        "    num_outliers = len(X_train_outlier) - np.sum(mask)\n",
        "\n",
        "  mlflow.log_metric(\"num_outliers\", num_outliers)\n",
        "  print(f\"Number of outliers detected: {num_outliers}\")\n",
        "  outlier_percentage = (num_outliers / len(X_train_outlier_scaled)) * 100\n",
        "  mlflow.log_metric(\"outlier_percentage\", outlier_percentage)\n",
        "  print(f\"Percentage of outliers removed: {outlier_percentage:.2f}%\")\n",
        "\n",
        "  # Apply mask to full dataset\n",
        "  df_train = df_train[mask]\n",
        "  X_final = X_train_full[mask]\n",
        "  y_final = y_train_full[mask]\n",
        "\n",
        "  valid_params = {\n",
        "      k: v\n",
        "      for k, v in best_params.items()\n",
        "      if k in xgb.XGBRegressor().get_params()\n",
        "  }\n",
        "\n",
        "  # all the constant hyperparameters should be added here manually and are not\n",
        "  # part of study.best_params\n",
        "  final_model = xgb.XGBRegressor(\n",
        "      **valid_params,\n",
        "      n_jobs=N_JOBS,\n",
        "      random_state=RANDOM_STATE,\n",
        "      validate_parameters=True,\n",
        "      verbosity=0\n",
        "  )\n",
        "\n",
        "  X_final = X_final[sorted(X_final.columns)]\n",
        "  X_val = X_val[sorted(X_val.columns)]\n",
        "  final_model.fit(X_final, y_final)\n",
        "  y_pred = final_model.predict(X_val)\n",
        "\n",
        "  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")"
      ],
      "metadata": {
        "id": "kVuDPS7JL_XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ZScore Transformer"
      ],
      "metadata": {
        "id": "4Lxkz_f2gGmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile zscore_transformer.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ZScoreTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, columns, threshold):\n",
        "    \"\"\"\n",
        "    Initialize the ZScoreTransformer to remove outliers using Z-scores.\n",
        "\n",
        "    Parameters:\n",
        "    - columns : list\n",
        "    List of column names to apply Z-score outlier detection.\n",
        "    - threshold : float, optional (default=3.0)\n",
        "    Z-score threshold for identifying outliers. Rows with any Z-score\n",
        "    exceeding this threshold in the specified columns are removed.\n",
        "\n",
        "    Notes:\n",
        "    - This transformer removes rows identified as outliers, which may result\n",
        "    in different row counts between training and validation sets. Ensure this\n",
        "    behavior aligns with your pipeline, as it may affect index alignment\n",
        "    or downstream tasks expecting consistent row counts.\n",
        "    - Columns with zero variance (standard deviation = 0) are excluded from\n",
        "    outlier detection, as they contain no outliers by definition.\n",
        "    \"\"\"\n",
        "    self.columns = columns\n",
        "    self.threshold = threshold\n",
        "    self.means_ = None\n",
        "    self.stds_ = None\n",
        "    self.valid_columns_ = None\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit the transformer by computing mean and std for specified columns.\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame\n",
        "    Input DataFrame containing the columns to check for outliers.\n",
        "    - y : None\n",
        "    Ignored. For scikit-learn compatibility.\n",
        "\n",
        "    Returns:\n",
        "    self : ZScoreTransformer\n",
        "    Returns the instance itself.\n",
        "\n",
        "    Raises:\n",
        "    ValueError\n",
        "    If X is not a pandas DataFrame or if specified columns are missing.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"Input X must be a pandas DataFrame\")\n",
        "\n",
        "    # Ensure all specified columns exist in X\n",
        "    missing_cols = [col for col in self.columns if col not in X.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "      raise ValueError(f\"Columns {missing_cols} not found in input DataFrame\")\n",
        "\n",
        "    # Compute mean and std for specified columns\n",
        "    self.means_ = X[self.columns].mean()\n",
        "    self.stds_ = X[self.columns].std()\n",
        "\n",
        "    # Identify columns with non-zero standard deviation\n",
        "    self.valid_columns_ = [\n",
        "        col for col in self.columns if self.stds_[col] != 0\n",
        "    ]\n",
        "\n",
        "    if not self.valid_columns_:\n",
        "      print(\"\"\"\n",
        "      Warning: All specified columns have zero variance. No outlier detection\n",
        "      will be performed.\n",
        "      \"\"\")\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Transform the input DataFrame by removing outliers based on Z-scores.\n",
        "\n",
        "    Parameters:\n",
        "    X : pandas DataFrame\n",
        "    Input DataFrame to transform.\n",
        "\n",
        "    Returns:\n",
        "    X_transformed : pandas DataFrame\n",
        "    DataFrame with outlier rows removed based on Z-score threshold.\n",
        "\n",
        "    Raises:\n",
        "    ValueError\n",
        "    If X is not a pandas DataFrame, transformer is not fitted, or specified\n",
        "    columns are missing.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"Input X must be a pandas DataFrame\")\n",
        "\n",
        "    if (\n",
        "        self.means_ is None\n",
        "        or self.stds_ is None\n",
        "        or self.valid_columns_ is None\n",
        "    ):\n",
        "      raise ValueError(\"Transformer not fitted. Call fit() first.\")\n",
        "\n",
        "    # Ensure all specified columns exist in X\n",
        "    missing_cols = [col for col in self.columns if col not in X.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "      raise ValueError(f\"Columns {missing_cols} not found in input DataFrame\")\n",
        "\n",
        "    if not self.valid_columns_:\n",
        "      # If no valid columns, return copy of input DataFrame\n",
        "      return X.copy()\n",
        "\n",
        "    # Calculate Z-scores for valid columns\n",
        "    z_scores = np.abs(\n",
        "        (X[self.valid_columns_] - self.means_[self.valid_columns_])\n",
        "        / self.stds_[self.valid_columns_]\n",
        "    )\n",
        "\n",
        "    # Create mask where all Z-scores are below threshold\n",
        "    mask = (z_scores < self.threshold).all(axis=1)\n",
        "\n",
        "    # Return DataFrame with non-outlier rows\n",
        "    return X[mask].copy()"
      ],
      "metadata": {
        "id": "laN5JkHMgDrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "X8yCR4CggQTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zscore_transformer import ZScoreTransformer\n",
        "\n",
        "zscore_transformer = ZScoreTransformer(\n",
        "    columns=[\n",
        "        'publisher_avg_clicks',\n",
        "        'market_avg_clicks',\n",
        "        'publisher_popularity',\n",
        "        'market_popularity'\n",
        "    ],\n",
        "    threshold=3.3248732323726267\n",
        ")\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=zscore_transformer,\n",
        "    experiment_name=\"ZScore_Transformer\",\n",
        "    model_name=\"zscore_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=None\n",
        ")"
      ],
      "metadata": {
        "id": "9fGGbksRhhSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train"
      ],
      "metadata": {
        "id": "CIURISBQgSpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We do not remove outliers from the validation/test data\n",
        "df_train = zscore_transformer.transform(df_train)"
      ],
      "metadata": {
        "id": "QvhIpavfi1NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Feature Distribution Mismatches"
      ],
      "metadata": {
        "id": "slPVgH43_6Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_feature_distribution_mismatches(\n",
        "    df_train[skewed_cols],\n",
        "    df_val[skewed_cols],\n",
        "    test_type=\"Kolmogorov-Smirnov\"\n",
        ")"
      ],
      "metadata": {
        "id": "Vm8qJ8rP_4gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skewed Features\n",
        "\n",
        "Some numerical features in the dataset were **highly skewed** which can negatively impact model performance, especially for tree-based and linear models.\n",
        "\n",
        "To address this, several transformations were evaluated:\n",
        "\n",
        "- **Methods tried:** `Yeo-Johnson`, `Logarithmic (log)`, `Square Root (sqrt)`, `Box-Cox`, `Reciprocal`, `None`\n",
        "- **Purpose:** Reduce skewness in the data to improve model stability and predictive performance.\n",
        "\n",
        "Evaluation on the validation set:\n",
        "\n",
        "1. **Log transformation** produced the best results.\n",
        "2. **Followed by:** Yeo-Johnson, Box-Cox, Square Root (sqrt) and Reciprocal.  \n",
        "\n",
        "**Decision:** The final model used the **log transformation** on skewed features as it consistently reduced skewness and improved cross-validated RMSE.  \n",
        "\n",
        "**Takeaway:** This approach ensures that features are closer to a normal distribution which can improve model convergence, stability and overall predictive performance.\n"
      ],
      "metadata": {
        "id": "adbYGhKpVBCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of continuous features\n",
        "numeric_cols = (\n",
        "    df_train\n",
        "    .select_dtypes(include=['int64', 'float64'])\n",
        "    .columns\n",
        ")"
      ],
      "metadata": {
        "id": "Sb8ugbqlU-mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot distributions for continuous features\n",
        "for col in numeric_cols:\n",
        "  sns.histplot(df_train[col], kde=True)\n",
        "  plt.title(f\"Distribution of {col}\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "S_58qtE8VADL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = (\n",
        "    df_train\n",
        "    .select_dtypes(include=['int64', 'float64'])\n",
        "    .columns\n",
        "    .drop([TARGET])\n",
        "    .tolist()\n",
        ")\n",
        "\n",
        "print(f'Numerical columns: {numeric_cols}')\n",
        "skewed_cols = df_train[numeric_cols].skew().abs()\n",
        "cols_to_transform = skewed_cols[skewed_cols > 0.5].index.tolist()\n",
        "print(f'Skewed columns to transform: {cols_to_transform}')\n",
        "X_train_full = df_train[numeric_cols]\n",
        "X_val = df_val[numeric_cols]\n",
        "\n",
        "n_trials = 500\n",
        "n_splits = 5\n",
        "n_repeats = 3\n",
        "mlflow_experiment = \"skewness_detection\"\n",
        "\n",
        "# SQLite storage configuration\n",
        "study_name = f\"{mlflow_experiment}_study\"\n",
        "storage = f\"sqlite:///{PATH}/{study_name}.db\"\n",
        "\n",
        "def objective(trial):\n",
        "  # Tune transformation method\n",
        "  transform_method = trial.suggest_categorical(\n",
        "      'transform_method',\n",
        "      ['yeo-johnson', 'log', 'sqrt', 'box-cox', 'reciprocal', 'none']\n",
        "  )\n",
        "\n",
        "  yeo_johnson_standardize = (\n",
        "      trial.suggest_categorical('yeo_johnson_standardize', [True, False])\n",
        "      if transform_method == 'yeo-johnson'\n",
        "      else False\n",
        "  )\n",
        "\n",
        "  box_cox_standardize = (\n",
        "      trial.suggest_categorical('box_cox_standardize', [True, False])\n",
        "      if transform_method == 'box-cox'\n",
        "      else False\n",
        "  )\n",
        "\n",
        "  # Tune winsorization limits\n",
        "  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n",
        "\n",
        "  # Apply winsorization to target\n",
        "  y_train_full = np.log1p(\n",
        "      mstats.winsorize(df_train[TARGET], limits=(0, 1-winsorize_upper))\n",
        "  )\n",
        "\n",
        "  # Apply transformation to training data\n",
        "  X_train_transformed = X_train_full.copy()\n",
        "  transformed_columns = [f'{col}_transformed' for col in cols_to_transform]\n",
        "\n",
        "  if transform_method == 'yeo-johnson':\n",
        "    skewness_transformer = PowerTransformer(\n",
        "        method='yeo-johnson',\n",
        "        standardize=yeo_johnson_standardize\n",
        "    )\n",
        "\n",
        "    X_train_transformed[transformed_columns] = (\n",
        "        skewness_transformer\n",
        "        .fit_transform(X_train_full[cols_to_transform])\n",
        "    )\n",
        "\n",
        "  elif transform_method == 'log':\n",
        "    shift = (\n",
        "        abs(X_train_full[cols_to_transform].min().min()) + 1e-10\n",
        "        if X_train_full[cols_to_transform].min().min() <= 0\n",
        "        else 0\n",
        "    )\n",
        "\n",
        "    X_train_transformed[transformed_columns] = np.log1p(\n",
        "        X_train_full[cols_to_transform] + shift\n",
        "    )\n",
        "\n",
        "    trial.set_user_attr('shift', shift)\n",
        "\n",
        "  elif transform_method == 'sqrt':\n",
        "    shift = (\n",
        "        abs(X_train_full[cols_to_transform].min().min())\n",
        "        if X_train_full[cols_to_transform].min().min() < 0\n",
        "        else 0\n",
        "    )\n",
        "\n",
        "    X_train_transformed[transformed_columns] = np.sqrt(\n",
        "        X_train_full[cols_to_transform] + shift\n",
        "    )\n",
        "\n",
        "    trial.set_user_attr('shift', shift)\n",
        "\n",
        "  elif transform_method == 'box-cox':\n",
        "    shift = (\n",
        "        abs(X_train_full[cols_to_transform].min().min()) + 1e-10\n",
        "        if X_train_full[cols_to_transform].min().min() <= 0\n",
        "        else 0\n",
        "    )\n",
        "\n",
        "    skewness_transformer = PowerTransformer(\n",
        "        method='box-cox',\n",
        "        standardize=box_cox_standardize\n",
        "    )\n",
        "\n",
        "    X_train_transformed[transformed_columns] = (\n",
        "        skewness_transformer\n",
        "        .fit_transform(X_train_full[cols_to_transform] + shift)\n",
        "    )\n",
        "\n",
        "    trial.set_user_attr('shift', shift)\n",
        "\n",
        "  elif transform_method == 'reciprocal':\n",
        "    shift = (\n",
        "        1e-10\n",
        "        if X_train_full[cols_to_transform].eq(0).any().any()\n",
        "        else 0\n",
        "    )\n",
        "\n",
        "    X_train_transformed[transformed_columns] = (\n",
        "        1 / (X_train_full[cols_to_transform] + shift + 1e-10)\n",
        "    )\n",
        "\n",
        "    trial.set_user_attr('shift', shift)\n",
        "\n",
        "  if transform_method != 'none':\n",
        "      X_train_transformed.drop(columns=cols_to_transform, inplace=True)\n",
        "\n",
        "  # Repeated K-Fold Cross-Validation\n",
        "  rkf = RepeatedKFold(\n",
        "      n_splits=n_splits,\n",
        "      n_repeats=n_repeats,\n",
        "      random_state=RANDOM_STATE\n",
        "  )\n",
        "\n",
        "  rmse_list = []\n",
        "\n",
        "  for fold_idx, (train_idx, val_idx) in enumerate(\n",
        "      rkf.split(X_train_transformed)\n",
        "  ):\n",
        "    X_train, X_val_fold = (\n",
        "        X_train_transformed.iloc[train_idx],\n",
        "        X_train_transformed.iloc[val_idx]\n",
        "    )\n",
        "\n",
        "    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective=trial.suggest_categorical(\n",
        "            'objective',\n",
        "              ['reg:squarederror']\n",
        "        ),\n",
        "        booster=trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        colsample_bylevel=trial.suggest_float('colsample_bylevel', 0.2, 0.8),\n",
        "        colsample_bynode=trial.suggest_float('colsample_bynode', 0.2, 0.8),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.2, 0.8),\n",
        "        gamma=trial.suggest_float('gamma', 1e-3, 30, log=True),\n",
        "        grow_policy=trial.suggest_categorical(\n",
        "            'grow_policy',\n",
        "              ['depthwise', 'lossguide']\n",
        "        ),\n",
        "        learning_rate=trial.suggest_float(\n",
        "            'learning_rate',\n",
        "            0.001,\n",
        "            0.2,\n",
        "            log=True\n",
        "        ),\n",
        "        max_bin=trial.suggest_int('max_bin', 128, 512),\n",
        "        max_depth=trial.suggest_int('max_depth', 2, 6),\n",
        "        min_child_weight=trial.suggest_float('min_child_weight', 1, 20),\n",
        "        n_estimators=trial.suggest_int('n_estimators', 100, 3000),\n",
        "        n_jobs=N_JOBS,\n",
        "        random_state=RANDOM_STATE,\n",
        "        sampling_method=trial.suggest_categorical(\n",
        "            'sampling_method',\n",
        "              [\"uniform\"]\n",
        "        ),\n",
        "        subsample=trial.suggest_float('subsample', 0.2, 0.8),\n",
        "        tree_method=trial.suggest_categorical(\n",
        "            'tree_method',\n",
        "              [\"auto\", \"hist\"]\n",
        "        ),\n",
        "        base_score=trial.suggest_float('base_score', 0.1, 0.9),\n",
        "        max_leaves=trial.suggest_int('max_leaves', 0, 16),\n",
        "        num_parallel_tree=trial.suggest_int('num_parallel_tree', 1, 3),\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 1e-6, 20, log=True),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 1e-6, 20, log=True),\n",
        "        scale_pos_weight=trial.suggest_float('scale_pos_weight', 0.5, 1.5),\n",
        "        validate_parameters=True,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val_fold)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n",
        "    gc.collect()\n",
        "\n",
        "    trial.report(np.mean(rmse_list), step=fold_idx)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "  return np.mean(rmse_list)\n",
        "\n",
        "# Start MLflow experiment\n",
        "mlflow.set_experiment(mlflow_experiment)\n",
        "\n",
        "with mlflow.start_run():\n",
        "  # - Ensure to use the same direction,\n",
        "  # pruner (min_resource, max_resource, reduction_factor, etc.) if you are\n",
        "  # loading an existing study\n",
        "  study = optuna.create_study(\n",
        "      study_name=study_name,\n",
        "      storage=storage,\n",
        "      direction=\"minimize\",\n",
        "      pruner=optuna.pruners.HyperbandPruner(\n",
        "          min_resource=max(1, n_splits),\n",
        "          max_resource=n_splits * n_repeats,\n",
        "          reduction_factor=2\n",
        "      ),\n",
        "      sampler=optuna.samplers.TPESampler(\n",
        "          n_startup_trials=max(1, int(n_trials * 0.2)),\n",
        "          seed=RANDOM_STATE,\n",
        "          multivariate=True\n",
        "      ),\n",
        "      load_if_exists=True\n",
        "  )\n",
        "\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  log_optuna_best_trial_search_space(study)\n",
        "\n",
        "  # Log Optuna storage details\n",
        "  mlflow.log_param(\"optuna_storage\", storage)\n",
        "  mlflow.log_param(\"optuna_study_name\", study_name)\n",
        "\n",
        "  # Log experiment parameters\n",
        "  mlflow.log_param(\"n_trials\", n_trials)\n",
        "  mlflow.log_param(\"n_splits\", n_splits)\n",
        "  mlflow.log_param(\"n_repeats\", n_repeats)\n",
        "\n",
        "  # Log cols_to_transform\n",
        "  mlflow.log_param(\"cols_to_transform\", \",\".join(cols_to_transform))\n",
        "\n",
        "  fanova_importances = get_param_importances(\n",
        "      study,\n",
        "      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n",
        "      target=(\n",
        "          lambda t: t.value\n",
        "          if t.state == optuna.trial.TrialState.COMPLETE\n",
        "          else None\n",
        "      ),\n",
        "      normalize=True\n",
        "  )\n",
        "\n",
        "  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n",
        "\n",
        "  for param, importance in fanova_importances.items():\n",
        "    mlflow.log_param(f\"fanova_{param}\", importance)\n",
        "\n",
        "  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n",
        "\n",
        "  # Get best parameters and trial attributes\n",
        "  best_params = study.best_params\n",
        "  best_trial = study.best_trial\n",
        "\n",
        "  for param, value in best_params.items():\n",
        "    mlflow.log_param(param, value)\n",
        "\n",
        "  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n",
        "\n",
        "  print(\"Best Params:\", best_params)\n",
        "  print(\"Best CV RMSE:\", study.best_value)\n",
        "\n",
        "  # Apply best winsorization and transformation to final training and validation sets\n",
        "  y_train_full = np.log1p(mstats.winsorize(\n",
        "      df_train[TARGET],\n",
        "      limits=(0, 1-best_params['winsorize_upper'])\n",
        "  ))\n",
        "\n",
        "  # Compute winsorization thresholds from training data\n",
        "  # Apply fixed thresholds to validation data\n",
        "  y_val = np.log1p(np.clip(\n",
        "      df_val[TARGET],\n",
        "      0,\n",
        "      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n",
        "  ))\n",
        "\n",
        "  # Apply best transformation to training data\n",
        "  best_transform_method = best_params['transform_method']\n",
        "\n",
        "  standardize = (\n",
        "      best_params['yeo_johnson_standardize']\n",
        "      if best_transform_method == 'yeo-johnson'\n",
        "      else best_params.get('box_cox_standardize', False)\n",
        "      if best_transform_method == 'box-cox'\n",
        "      else False\n",
        "  )\n",
        "\n",
        "  X_final = X_train_full.copy()\n",
        "  transformed_columns = [f'{col}_transformed' for col in cols_to_transform]\n",
        "  shift = best_trial.user_attrs.get('shift', 0)\n",
        "\n",
        "  if best_transform_method == 'yeo-johnson':\n",
        "    skewness_transformer = PowerTransformer(\n",
        "        method='yeo-johnson',\n",
        "        standardize=standardize\n",
        "    )\n",
        "\n",
        "    X_final[transformed_columns] = (\n",
        "        skewness_transformer.fit_transform(X_train_full[cols_to_transform])\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'log':\n",
        "    X_final[transformed_columns] = (\n",
        "        np.log1p(X_train_full[cols_to_transform] + shift)\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'sqrt':\n",
        "    X_final[transformed_columns] = (\n",
        "        np.sqrt(X_train_full[cols_to_transform] + shift)\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'box-cox':\n",
        "    skewness_transformer = PowerTransformer(\n",
        "        method='box-cox',\n",
        "        standardize=standardize\n",
        "    )\n",
        "\n",
        "    X_final[transformed_columns] = (\n",
        "        skewness_transformer\n",
        "        .fit_transform(X_train_full[cols_to_transform] + shift)\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'reciprocal':\n",
        "    X_final[transformed_columns] = (\n",
        "        1 / (X_train_full[cols_to_transform] + shift + 1e-10)\n",
        "    )\n",
        "\n",
        "  if best_params['transform_method'] != 'none':\n",
        "    X_final.drop(columns=cols_to_transform, inplace=True)\n",
        "\n",
        "  # Apply transformation to validation data\n",
        "  X_val_transformed = X_val.copy()\n",
        "  if best_transform_method == 'yeo-johnson':\n",
        "    X_val_transformed[transformed_columns] = (\n",
        "        skewness_transformer.transform(X_val[cols_to_transform])\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'log':\n",
        "    X_val_transformed[transformed_columns] = (\n",
        "        np.log1p(X_val[cols_to_transform] + shift)\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'sqrt':\n",
        "    X_val_transformed[transformed_columns] = (\n",
        "        np.sqrt(X_val[cols_to_transform] + shift)\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'box-cox':\n",
        "    X_val_transformed[transformed_columns] = (\n",
        "        skewness_transformer.transform(X_val[cols_to_transform] + shift)\n",
        "    )\n",
        "\n",
        "  elif best_transform_method == 'reciprocal':\n",
        "    X_val_transformed[transformed_columns] = (\n",
        "        1 / (X_val[cols_to_transform] + shift + 1e-10)\n",
        "    )\n",
        "\n",
        "  X_val_transformed.drop(columns=cols_to_transform, inplace=True)\n",
        "\n",
        "  # Train final model with best XGBoost parameters\n",
        "  filtered_params = {\n",
        "      k: v\n",
        "      for k, v in best_params.items()\n",
        "      if k in xgb.XGBRegressor().get_params()\n",
        "  }\n",
        "\n",
        "  # all the constant hyperparameters should be added here manually and are not\n",
        "  # part of study.best_params\n",
        "  final_model = xgb.XGBRegressor(\n",
        "      **filtered_params,\n",
        "      n_jobs=N_JOBS,\n",
        "      random_state=RANDOM_STATE,\n",
        "      validate_parameters=True,\n",
        "      verbosity=0\n",
        "  )\n",
        "\n",
        "  X_final = X_final[sorted(X_final.columns)]\n",
        "  final_model.fit(X_final, y_train_full)\n",
        "  X_val_transformed = X_val_transformed[sorted(X_val_transformed.columns)]\n",
        "  y_pred = final_model.predict(X_val_transformed)\n",
        "\n",
        "  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")"
      ],
      "metadata": {
        "id": "jkfU_VOZPTrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skew Log Transformer"
      ],
      "metadata": {
        "id": "kEo202LMCw-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile log_transformer.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class LogTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, skew_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Initialize the transformer.\n",
        "\n",
        "    Parameters:\n",
        "    skew_threshold : float, default=0.5\n",
        "    Absolute skewness threshold to determine which columns to transform.\n",
        "    \"\"\"\n",
        "    self.skew_threshold = skew_threshold\n",
        "    self.numeric_cols_ = None\n",
        "    self.cols_to_transform_ = None\n",
        "    self.shift_ = None\n",
        "    self.transformed_columns_ = None\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit the transformer by identifying numerical columns, skewed columns and\n",
        "    computing shift.\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame\n",
        "    Input data to fit.\n",
        "    y : None\n",
        "    - Ignored. Included for scikit-learn compatibility.\n",
        "\n",
        "    Returns:\n",
        "    self : SkewLogTransformer\n",
        "    Returns the instance itself.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "    # Identify numerical columns\n",
        "    self.numeric_cols_ = (\n",
        "        X.select_dtypes(include=['int64', 'float64'])\n",
        "        .columns\n",
        "        .tolist()\n",
        "    )\n",
        "\n",
        "    if not self.numeric_cols_:\n",
        "      self.cols_to_transform_ = []\n",
        "      self.shift_ = 0\n",
        "      self.transformed_columns_ = []\n",
        "      return self\n",
        "\n",
        "    # Calculate skewness and identify columns to transform\n",
        "    skewed_cols = X[self.numeric_cols_].skew().abs()\n",
        "\n",
        "    self.cols_to_transform_ = (\n",
        "        skewed_cols[skewed_cols > self.skew_threshold]\n",
        "        .index\n",
        "        .tolist()\n",
        "    )\n",
        "\n",
        "    # Compute shift to ensure positive values for log1p\n",
        "    if self.cols_to_transform_:\n",
        "      self.shift_ = (\n",
        "          abs(X[self.cols_to_transform_].min().min()) + 1e-10\n",
        "          if X[self.cols_to_transform_].min().min() <= 0\n",
        "          else 0\n",
        "      )\n",
        "    else:\n",
        "      self.shift_ = 0\n",
        "\n",
        "    # Define transformed column names\n",
        "    self.transformed_columns_ = [\n",
        "        f'{col}_transformed' for col in self.cols_to_transform_\n",
        "    ]\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Transform the input data by applying log1p transformation to skewed columns.\n",
        "\n",
        "    Parameters:\n",
        "    X : pandas DataFrame\n",
        "    Input data to transform.\n",
        "\n",
        "    Returns:\n",
        "    X_transformed : pandas DataFrame\n",
        "    Transformed data with skewed columns replaced by their log1p\n",
        "    transformations.\n",
        "    \"\"\"\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"Input X must be a pandas DataFrame.\")\n",
        "\n",
        "    if self.cols_to_transform_ is None:\n",
        "      raise ValueError(\"Transformer has not been fitted. Call fit() first.\")\n",
        "\n",
        "    X_transformed = X.copy()\n",
        "\n",
        "    # Apply transformation only to columns present in X\n",
        "    cols_to_transform = [\n",
        "        col for col in self.cols_to_transform_ if col in X.columns\n",
        "    ]\n",
        "\n",
        "    if cols_to_transform:\n",
        "      transformed_columns = [f'{col}_transformed' for col in cols_to_transform]\n",
        "\n",
        "      X_transformed[transformed_columns] = np.log1p(\n",
        "          X[cols_to_transform] + self.shift_\n",
        "      )\n",
        "\n",
        "      # Drop original skewed columns\n",
        "      X_transformed.drop(columns=cols_to_transform, inplace=True)\n",
        "\n",
        "    return X_transformed"
      ],
      "metadata": {
        "id": "jpld8te2OIMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'log_transformer.py',\n",
        "    destination_path = PATH / 'log_transformer.py'\n",
        ")"
      ],
      "metadata": {
        "id": "DlmP9yxBBFvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "0j_YiEipC01-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from log_transformer import LogTransformer\n",
        "\n",
        "log_transformer = LogTransformer()\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=log_transformer,\n",
        "    experiment_name=\"Log_Transformer\",\n",
        "    model_name=\"log_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "LaONSIOhIOeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train and Test"
      ],
      "metadata": {
        "id": "9bI_0sh8C2Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = apply_transformer(\n",
        "    df_train,\n",
        "    df_val,\n",
        "    log_transformer,\n",
        "    TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "3AJ6h_9UIPNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Feature Distribution Mismatches"
      ],
      "metadata": {
        "id": "nAhU3FcDDnQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_feature_distribution_mismatches(\n",
        "    df_train[skewed_cols],\n",
        "    df_val[skewed_cols],\n",
        "    test_type=\"Kolmogorov-Smirnov\"\n",
        ")"
      ],
      "metadata": {
        "id": "fiLYtnxfDpFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boolean to String"
      ],
      "metadata": {
        "id": "T9pi3SVs1W53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boolean To String Transformer"
      ],
      "metadata": {
        "id": "rsyOyyeNC5H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile boolean_to_string_transformer.py\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class BooleanToStringTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, column_name):\n",
        "    self.column_name = column_name\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    X_copy = X.copy()\n",
        "    X_copy[self.column_name] = X_copy[self.column_name].astype('string')\n",
        "    return X_copy"
      ],
      "metadata": {
        "id": "rr_-Q2Xs2XZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'boolean_to_string_transformer.py',\n",
        "    destination_path = PATH / 'boolean_to_string_transformer.py'\n",
        ")"
      ],
      "metadata": {
        "id": "ub2qXraiBL_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "pEjZsP-_C8PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from boolean_to_string_transformer import BooleanToStringTransformer\n",
        "\n",
        "boolean_to_string_transformer = BooleanToStringTransformer(\n",
        "    column_name = 'is_weekend'\n",
        ")\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=boolean_to_string_transformer,\n",
        "    experiment_name=\"Boolean_To_String\",\n",
        "    model_name=\"boolean_to_string_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "L03W0ue449t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train and Test"
      ],
      "metadata": {
        "id": "ryqyYhgCC-Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = apply_transformer(\n",
        "    df_train,\n",
        "    df_val,\n",
        "    boolean_to_string_transformer,\n",
        "    TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "HRxU8OyY5jBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Categorical Features\n",
        "\n",
        "Several categorical encoding techniques were evaluated to handle the non-numerical features:\n",
        "\n",
        "- **Methods tried:** `Target Encoding`, `Leave-One-Out (LOO) Encoding`, `Binary Encoding`, `Frequency Encoding`, `Label Encoding`  \n",
        "- **Purpose:** Convert categorical variables into numerical representations while preserving predictive information and avoiding data leakage.\n",
        "\n",
        "Evaluation on the validation set:\n",
        "\n",
        "1. **Target Encoding** performed the best, providing the lowest cross-validated RMSE.  \n",
        "2. **Followed by:** Leave-One-Out (LOO) Encoding, Binary Encoding, Frequency Encoding and Label Encoding.\n",
        "\n",
        "**Decision:** The final model uses **Target Encoding** for categorical variables.\n",
        "\n",
        "**Takeaway:** This method captures the relationship between categorical features and the target variable more effectively than other encodings which improves model performance, especially in regression tasks.\n"
      ],
      "metadata": {
        "id": "4dsvEXE76R6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Weight of Evidence (WoE) Encoding is best for the binary classification\n",
        "# problems. Not using it here as this one is a regression problem.\n",
        "\n",
        "cols_to_encode = df_train.select_dtypes(include=['string']).columns.tolist()\n",
        "print(f'Categorical columns: {cols_to_encode}')\n",
        "X_train_full = df_train.drop(TARGET, axis=1)\n",
        "X_val = df_val.drop(TARGET, axis=1)\n",
        "\n",
        "n_trials = 500\n",
        "n_splits = 5\n",
        "n_repeats = 3\n",
        "mlflow_experiment = \"Encoding\"\n",
        "\n",
        "# Number of inner folds for k-fold target encoding\n",
        "inner_k_splits = 5\n",
        "\n",
        "# SQLite storage configuration\n",
        "study_name = f\"{mlflow_experiment}_study\"\n",
        "storage = f\"sqlite:///{PATH}/{study_name}.db\"\n",
        "\n",
        "def frequency_encode(df, columns, df_val=None, noise_level=0.0):\n",
        "  \"\"\"\n",
        "  Apply frequency encoding to specified columns with optional noise.\n",
        "  \"\"\"\n",
        "  df_encoded = df.copy()\n",
        "  df_val_encoded = df_val.copy() if df_val is not None else None\n",
        "\n",
        "  for col in columns:\n",
        "    # Calculate frequency counts from training data\n",
        "    freq = df[col].value_counts(normalize=True)\n",
        "\n",
        "    # Map frequencies to training data with optional noise\n",
        "    df_encoded[f'{col}_encoded'] = df[col].map(freq)\n",
        "\n",
        "    if noise_level > 0:\n",
        "      df_encoded[f'{col}_encoded'] += np.random.normal(\n",
        "          0,\n",
        "          noise_level,\n",
        "          len(df_encoded)\n",
        "      )\n",
        "\n",
        "      # Convert to NumPy array with float64 dtype for winsorize\n",
        "      df_encoded[f'{col}_encoded'] = mstats.winsorize(\n",
        "          df_encoded[f'{col}_encoded'].to_numpy(dtype=np.float64),\n",
        "          limits=(0.01, 0.01)\n",
        "      )\n",
        "\n",
        "    # Map frequencies to validation data if provided\n",
        "    if df_val is not None:\n",
        "      df_val_encoded[f'{col}_encoded'] = (\n",
        "          df_val[col]\n",
        "          .map(freq)\n",
        "          .fillna(freq.mean())\n",
        "      )\n",
        "\n",
        "    # Drop original categorical column\n",
        "    df_encoded = df_encoded.drop(columns=[col])\n",
        "\n",
        "    if df_val is not None:\n",
        "      df_val_encoded = df_val_encoded.drop(columns=[col])\n",
        "\n",
        "  return df_encoded, df_val_encoded\n",
        "\n",
        "def kfold_target_encode(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_val_fold,\n",
        "    columns,\n",
        "    smoothing,\n",
        "    df_val=None,\n",
        "    n_splits=5\n",
        "):\n",
        "  \"\"\"\n",
        "  Apply k-fold target encoding to specified columns to prevent data leakage.\n",
        "  \"\"\"\n",
        "  global_mean = y_train.mean()\n",
        "\n",
        "  # Validate input columns\n",
        "  for col in columns:\n",
        "    if col not in X_train.columns:\n",
        "      raise ValueError(f\"Column '{col}' not found in X_train\")\n",
        "    if col not in X_val_fold.columns:\n",
        "      raise ValueError(f\"Column '{col}' not found in X_val_fold\")\n",
        "    if df_val is not None and col not in df_val.columns:\n",
        "      raise ValueError(f\"Column '{col}' not found in df_val\")\n",
        "\n",
        "  X_train_encoded = X_train.copy()\n",
        "  X_val_encoded = X_val_fold.copy()\n",
        "  df_val_encoded = df_val.copy() if df_val is not None else None\n",
        "\n",
        "  # Check for empty validation set\n",
        "  if df_val is not None and len(df_val) == 0:\n",
        "    raise ValueError(\"Validation set (df_val) is empty\")\n",
        "\n",
        "  for col in columns:\n",
        "    # Initialize array to store encodings\n",
        "    train_encodings = np.zeros(len(X_train))\n",
        "    val_encodings = np.zeros(len(X_val_fold))\n",
        "    test_encodings = np.zeros(len(df_val)) if df_val is not None else None\n",
        "\n",
        "    # Inner k-fold for training data\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train):\n",
        "      X_inner_train, X_inner_val = (\n",
        "          X_train.iloc[train_idx],\n",
        "          X_train.iloc[val_idx]\n",
        "      )\n",
        "\n",
        "      y_inner_train = y_train.iloc[train_idx]\n",
        "\n",
        "      # Calculate target mean per category in inner training fold\n",
        "      target_means = y_inner_train.groupby(X_inner_train[col]).mean()\n",
        "      category_counts = X_inner_train[col].value_counts()\n",
        "\n",
        "      # Apply smoothing\n",
        "      weighted_mean = (\n",
        "          (category_counts * target_means + smoothing * global_mean)\n",
        "          / (category_counts + smoothing)\n",
        "      )\n",
        "\n",
        "      # Map to inner validation fold\n",
        "      train_encodings[val_idx] = (\n",
        "          X_inner_val[col]\n",
        "          .map(weighted_mean)\n",
        "          .fillna(global_mean)\n",
        "      )\n",
        "\n",
        "    target_means = y_train.groupby(X_train[col]).mean()\n",
        "    category_counts = X_train[col].value_counts()\n",
        "\n",
        "    weighted_mean = (\n",
        "        (category_counts * target_means + smoothing * global_mean)\n",
        "        / (category_counts + smoothing)\n",
        "    )\n",
        "\n",
        "    # Map to training and validation data\n",
        "    train_encodings = (\n",
        "        pd.Series(train_encodings, index=X_train.index)\n",
        "        .fillna(global_mean)\n",
        "    )\n",
        "\n",
        "    X_train_encoded[f'{col}_encoded'] = train_encodings\n",
        "\n",
        "    X_val_encoded[f'{col}_encoded'] = (\n",
        "        X_val_fold[col]\n",
        "        .map(weighted_mean)\n",
        "        .fillna(global_mean)\n",
        "    )\n",
        "\n",
        "    if df_val is not None:\n",
        "      df_val_encoded[f'{col}_encoded'] = (\n",
        "          df_val[col]\n",
        "          .map(weighted_mean)\n",
        "          .fillna(global_mean)\n",
        "      )\n",
        "\n",
        "    # Drop original categorical column\n",
        "    X_train_encoded = X_train_encoded.drop(columns=[col])\n",
        "    X_val_encoded = X_val_encoded.drop(columns=[col])\n",
        "\n",
        "    if df_val is not None:\n",
        "      df_val_encoded = df_val_encoded.drop(columns=[col])\n",
        "\n",
        "  return X_train_encoded, X_val_encoded, df_val_encoded\n",
        "\n",
        "def loo_encode(X_train, y_train, X_val_fold, columns, df_val=None, sigma=0.0):\n",
        "  \"\"\"\n",
        "  Apply Leave-One-Out Encoding to specified columns with optional noise.\n",
        "  \"\"\"\n",
        "  loo = LeaveOneOutEncoder(\n",
        "      cols=columns,\n",
        "      sigma=sigma,\n",
        "      random_state=RANDOM_STATE\n",
        "  )\n",
        "\n",
        "  X_train_encoded = loo.fit_transform(X_train, y_train)\n",
        "  X_val_encoded = loo.transform(X_val_fold)\n",
        "  df_val_encoded = loo.transform(df_val) if df_val is not None else None\n",
        "  return X_train_encoded, X_val_encoded, df_val_encoded\n",
        "\n",
        "def binary_encode(X_train, columns, df_val=None):\n",
        "  \"\"\"\n",
        "  Apply Binary Encoding to specified columns.\n",
        "  \"\"\"\n",
        "  binary = BinaryEncoder(cols=columns, return_df=True)\n",
        "  X_train_encoded = binary.fit_transform(X_train)\n",
        "  df_val_encoded = binary.transform(df_val) if df_val is not None else None\n",
        "  return X_train_encoded, df_val_encoded\n",
        "\n",
        "def label_encode(X_train, columns, df_val=None):\n",
        "  \"\"\"\n",
        "  Apply Label Encoding to specified columns.\n",
        "  \"\"\"\n",
        "  X_train_encoded = X_train.copy()\n",
        "  df_val_encoded = df_val.copy() if df_val is not None else None\n",
        "  label_encoders = {}\n",
        "\n",
        "  for col in columns:\n",
        "    le = LabelEncoder()\n",
        "    X_train_encoded[f'{col}_encoded'] = le.fit_transform(X_train[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "    if df_val is not None:\n",
        "      # Handle unseen categories in validation set\n",
        "      df_val_encoded[f'{col}_encoded'] = (\n",
        "          df_val[col]\n",
        "          .map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
        "      )\n",
        "\n",
        "    X_train_encoded = X_train_encoded.drop(columns=[col])\n",
        "\n",
        "    if df_val is not None:\n",
        "      df_val_encoded = df_val_encoded.drop(columns=[col])\n",
        "\n",
        "  return X_train_encoded, df_val_encoded, label_encoders\n",
        "\n",
        "def objective(trial):\n",
        "  # Tune winsorization limits\n",
        "  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n",
        "\n",
        "  y_train_full = np.log1p(mstats.winsorize(\n",
        "      df_train[TARGET].to_numpy(dtype=np.float64),\n",
        "      limits=(0, 1-winsorize_upper)\n",
        "  ))\n",
        "\n",
        "  # Choose encoding method\n",
        "  encoding_method = trial.suggest_categorical(\n",
        "      'encoding_method',\n",
        "      ['frequency', 'target', 'loo', 'binary', 'label']\n",
        "  )\n",
        "\n",
        "  # Tune encoding parameters\n",
        "  if encoding_method == 'target':\n",
        "    smoothing = trial.suggest_float('target_smoothing', 0.1, 10.0, log=True)\n",
        "  elif encoding_method == 'frequency':\n",
        "    noise_level = trial.suggest_float('freq_noise_level', 0.0, 0.1)\n",
        "  elif encoding_method == 'loo':\n",
        "    sigma = trial.suggest_float('loo_sigma', 0.0, 0.1)  # Noise for LOO\n",
        "  else:\n",
        "    # binary and label encoding have no tunable parameters\n",
        "    pass\n",
        "\n",
        "  # Repeated K-Fold Cross-Validation\n",
        "  rkf = RepeatedKFold(\n",
        "      n_splits=n_splits,\n",
        "      n_repeats=n_repeats,\n",
        "      random_state=RANDOM_STATE\n",
        "  )\n",
        "\n",
        "  rmse_list = []\n",
        "\n",
        "  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n",
        "    X_train, X_val_fold = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n",
        "    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "    # Apply encoding based on method\n",
        "    if encoding_method == 'frequency':\n",
        "      X_train_encoded, X_val_encoded = frequency_encode(\n",
        "          X_train,\n",
        "          cols_to_encode,\n",
        "          X_val_fold,\n",
        "          noise_level=noise_level\n",
        "      )\n",
        "    elif encoding_method == 'target':\n",
        "      y_train_series = pd.Series(y_train, index=X_train.index, name=TARGET)\n",
        "\n",
        "      X_train_encoded, X_val_encoded, _ = kfold_target_encode(\n",
        "          X_train,\n",
        "          y_train_series,\n",
        "          X_val_fold,\n",
        "          cols_to_encode,\n",
        "          smoothing=smoothing,\n",
        "          n_splits=inner_k_splits\n",
        "      )\n",
        "    elif encoding_method == 'loo':\n",
        "      y_train_series = pd.Series(y_train, index=X_train.index, name=TARGET)\n",
        "\n",
        "      X_train_encoded, X_val_encoded, _ = loo_encode(\n",
        "          X_train,\n",
        "          y_train_series,\n",
        "          X_val_fold,\n",
        "          cols_to_encode,\n",
        "          sigma=sigma\n",
        "      )\n",
        "    elif encoding_method == 'binary':\n",
        "      X_train_encoded, X_val_encoded = binary_encode(\n",
        "          X_train,\n",
        "          cols_to_encode,\n",
        "          X_val_fold\n",
        "      )\n",
        "    elif encoding_method == 'label':\n",
        "      X_train_encoded, X_val_encoded, _ = label_encode(\n",
        "          X_train,\n",
        "          cols_to_encode,\n",
        "          X_val_fold\n",
        "      )\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective=trial.suggest_categorical('objective', ['reg:squarederror']),\n",
        "        booster=trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        colsample_bylevel=trial.suggest_float('colsample_bylevel', 0.2, 0.8),\n",
        "        colsample_bynode=trial.suggest_float('colsample_bynode', 0.2, 0.8),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.2, 0.8),\n",
        "        gamma=trial.suggest_float('gamma', 1e-1, 30, log=True),\n",
        "        grow_policy=trial.suggest_categorical(\n",
        "            'grow_policy',\n",
        "            ['depthwise', 'lossguide']\n",
        "        ),\n",
        "        learning_rate=trial.suggest_float(\n",
        "            'learning_rate',\n",
        "            0.001,\n",
        "            0.2,\n",
        "            log=True\n",
        "        ),\n",
        "        max_bin=trial.suggest_int('max_bin', 128, 512),\n",
        "        max_depth=trial.suggest_int('max_depth', 2, 6),\n",
        "        min_child_weight=trial.suggest_float('min_child_weight', 1, 20),\n",
        "        n_estimators=trial.suggest_int('n_estimators', 100, 3000),\n",
        "        n_jobs=N_JOBS,\n",
        "        random_state=RANDOM_STATE,\n",
        "        sampling_method=trial.suggest_categorical(\n",
        "            'sampling_method',\n",
        "            [\"uniform\"]\n",
        "        ),\n",
        "        subsample=trial.suggest_float('subsample', 0.2, 0.8),\n",
        "        tree_method=trial.suggest_categorical('tree_method', [\"auto\", \"hist\"]),\n",
        "        base_score=trial.suggest_float('base_score', 0.1, 0.9),\n",
        "        max_leaves=trial.suggest_int('max_leaves', 0, 16),\n",
        "        num_parallel_tree=trial.suggest_int('num_parallel_tree', 1, 3),\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 1e-1, 20, log=True),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 1e-1, 20, log=True),\n",
        "        scale_pos_weight=trial.suggest_float('scale_pos_weight', 0.5, 1.5),\n",
        "        validate_parameters=True,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_encoded, y_train)\n",
        "    y_pred = model.predict(X_val_encoded)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n",
        "    gc.collect()\n",
        "\n",
        "    trial.report(np.mean(rmse_list), step=fold_idx)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "  return np.mean(rmse_list)\n",
        "\n",
        "# Start MLflow experiment\n",
        "mlflow.set_experiment(mlflow_experiment)\n",
        "\n",
        "with mlflow.start_run():\n",
        "  # - Ensure to use the same direction,\n",
        "  # pruner (min_resource, max_resource, reduction_factor, etc.) if you are\n",
        "  # loading an existing study\n",
        "  study = optuna.create_study(\n",
        "      study_name=study_name,\n",
        "      storage=storage,\n",
        "      direction=\"minimize\",\n",
        "      pruner=optuna.pruners.HyperbandPruner(\n",
        "          min_resource=max(1, n_splits),\n",
        "          max_resource=n_splits * n_repeats,\n",
        "          reduction_factor=2\n",
        "      ),\n",
        "      sampler=optuna.samplers.TPESampler(\n",
        "          n_startup_trials=max(1, int(n_trials * 0.2)),\n",
        "          seed=RANDOM_STATE,\n",
        "          multivariate=True\n",
        "      ),\n",
        "      load_if_exists=True\n",
        "  )\n",
        "\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  log_optuna_best_trial_search_space(study)\n",
        "\n",
        "  # Log Optuna storage details\n",
        "  mlflow.log_param(\"optuna_storage\", storage)\n",
        "  mlflow.log_param(\"optuna_study_name\", study_name)\n",
        "\n",
        "  # Log experiment parameters\n",
        "  mlflow.log_param(\"n_trials\", n_trials)\n",
        "  mlflow.log_param(\"n_splits\", n_splits)\n",
        "  mlflow.log_param(\"n_repeats\", n_repeats)\n",
        "\n",
        "  # Log cols_to_transform\n",
        "  mlflow.log_param(\"cols_to_encode\", \",\".join(cols_to_encode))\n",
        "\n",
        "  fanova_importances = get_param_importances(\n",
        "      study,\n",
        "      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n",
        "      target=(\n",
        "          lambda t: t.value\n",
        "          if t.state == optuna.trial.TrialState.COMPLETE\n",
        "          else None\n",
        "      ),\n",
        "      normalize=True\n",
        "  )\n",
        "\n",
        "  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n",
        "\n",
        "  for param, importance in fanova_importances.items():\n",
        "    mlflow.log_param(f\"fanova_{param}\", importance)\n",
        "\n",
        "  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n",
        "\n",
        "  # Get best parameters and trial attributes\n",
        "  best_params = study.best_params\n",
        "  best_trial = study.best_trial\n",
        "\n",
        "  for param, value in best_params.items():\n",
        "    mlflow.log_param(param, value)\n",
        "\n",
        "  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n",
        "\n",
        "  print(\"Best Params:\", best_params)\n",
        "  print(\"Best CV RMSE:\", study.best_value)\n",
        "\n",
        "  # Apply best winsorization and transformation to final training and validation\n",
        "  # sets\n",
        "  y_train_full = np.log1p(mstats.winsorize(\n",
        "      df_train[TARGET].to_numpy(dtype=np.float64),\n",
        "      limits=(0, 1-best_params['winsorize_upper'])\n",
        "  ))\n",
        "\n",
        "  # Compute winsorization thresholds from training data\n",
        "  # Apply fixed thresholds to validation data\n",
        "  y_val = np.log1p(np.clip(\n",
        "      df_val[TARGET],\n",
        "      0,\n",
        "      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n",
        "  ))\n",
        "\n",
        "  # Apply best encoding to final training and validation sets\n",
        "  if best_params['encoding_method'] == 'frequency':\n",
        "    X_final, X_val_encoded = frequency_encode(\n",
        "        X_train_full,\n",
        "        cols_to_encode,\n",
        "        X_val,\n",
        "        noise_level=best_params['freq_noise_level']\n",
        "    )\n",
        "\n",
        "  elif best_params['encoding_method'] == 'target':\n",
        "    y_train_series = pd.Series(\n",
        "        y_train_full,\n",
        "        index=X_train_full.index,\n",
        "        name=TARGET\n",
        "    )\n",
        "\n",
        "    X_final, _, X_val_encoded = kfold_target_encode(\n",
        "        X_train_full,\n",
        "        y_train_series,\n",
        "        X_train_full,\n",
        "        cols_to_encode,\n",
        "        smoothing=best_params['target_smoothing'],\n",
        "        df_val=X_val,\n",
        "        n_splits=inner_k_splits\n",
        "    )\n",
        "\n",
        "  elif best_params['encoding_method'] == 'loo':\n",
        "    y_train_series = pd.Series(\n",
        "        y_train_full,\n",
        "        index=X_train_full.index,\n",
        "        name=TARGET\n",
        "    )\n",
        "\n",
        "    X_final, _, X_val_encoded = loo_encode(\n",
        "        X_train_full,\n",
        "        y_train_series,\n",
        "        X_train_full,\n",
        "        cols_to_encode,\n",
        "        df_val=X_val,\n",
        "        sigma=best_params['loo_sigma']\n",
        "    )\n",
        "\n",
        "  elif best_params['encoding_method'] == 'binary':\n",
        "    X_final, X_val_encoded = binary_encode(\n",
        "        X_train_full,\n",
        "        cols_to_encode,\n",
        "        X_val\n",
        "    )\n",
        "\n",
        "  elif best_params['encoding_method'] == 'label':\n",
        "    X_final, X_val_encoded, _ = label_encode(\n",
        "        X_train_full,\n",
        "        cols_to_encode,\n",
        "        X_val\n",
        "    )\n",
        "\n",
        "  # Train final model with best XGBoost parameters\n",
        "  filtered_params = {\n",
        "      k: v for k, v in best_params.items()\n",
        "      if k in xgb.XGBRegressor().get_params()\n",
        "  }\n",
        "\n",
        "  # all the constant hyperparameters should be added here manually and are not\n",
        "  # part of study.best_params\n",
        "  final_model = xgb.XGBRegressor(\n",
        "      **filtered_params,\n",
        "      n_jobs=N_JOBS,\n",
        "      random_state=RANDOM_STATE,\n",
        "      validate_parameters=True,\n",
        "      verbosity=0\n",
        "  )\n",
        "\n",
        "  X_final = X_final[sorted(X_final.columns)]\n",
        "  final_model.fit(X_final, y_train_full)\n",
        "  X_val_encoded = X_val_encoded[sorted(X_val_encoded.columns)]\n",
        "  y_pred = final_model.predict(X_val_encoded)\n",
        "\n",
        "  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")"
      ],
      "metadata": {
        "id": "Pl8U5dOkNhen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target Encoder Transformer"
      ],
      "metadata": {
        "id": "KVbdWrkQDCOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile target_encoder_transformer.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "class TargetEncoderTransformer(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  Scikit-learn compatible class for k-fold target encoding.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      columns,\n",
        "      smoothing=1.0,\n",
        "      n_splits=5,\n",
        "      random_state=RANDOM_STATE\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Initialize the KFoldTargetEncoder.\n",
        "\n",
        "    Parameters:\n",
        "    - columns : list\n",
        "    List of column names to apply target encoding.\n",
        "    - smoothing : float, default=1.0\n",
        "    Smoothing parameter to balance category mean and global mean.\n",
        "    - n_splits : int, default=5\n",
        "    Number of folds for k-fold encoding.\n",
        "    - random_state : int, default=None\n",
        "    Random state for KFold reproducibility.\n",
        "    \"\"\"\n",
        "    self.columns = columns\n",
        "    self.smoothing = smoothing\n",
        "    self.n_splits = n_splits\n",
        "    self.random_state = random_state\n",
        "    self.global_mean_ = None\n",
        "    self.target_means_ = {}\n",
        "    self.category_counts_ = {}\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Fit the encoder using training data.\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame\n",
        "    Training data with categorical columns.\n",
        "    - y : pandas Series or array-like\n",
        "    Target variable.\n",
        "\n",
        "    Returns:\n",
        "    self : KFoldTargetEncoder\n",
        "    Fitted encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate inputs\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"X must be a pandas DataFrame\")\n",
        "    if not isinstance(y, (pd.Series, np.ndarray)):\n",
        "      raise ValueError(\"y must be a pandas Series or numpy array\")\n",
        "\n",
        "    y = pd.Series(y, index=X.index) if isinstance(y, np.ndarray) else y\n",
        "\n",
        "    for col in self.columns:\n",
        "      if col not in X.columns:\n",
        "        raise ValueError(f\"Column '{col}' not found in X\")\n",
        "\n",
        "    # Store global mean for test data encoding\n",
        "    self.global_mean_ = np.mean(y)\n",
        "\n",
        "    # Store target means and counts for each column (for test data)\n",
        "    for col in self.columns:\n",
        "      target_means = y.groupby(X[col]).mean()\n",
        "      category_counts = X[col].value_counts()\n",
        "      self.target_means_[col] = target_means\n",
        "      self.category_counts_[col] = category_counts\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Transform data using fitted encoder.\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame\n",
        "    Data to transform.\n",
        "    - y : pandas Series or array-like, default=None\n",
        "    Target variable (required for training data, ignored for test data).\n",
        "\n",
        "    Returns:\n",
        "    X_encoded : pandas DataFrame\n",
        "    Transformed data with encoded columns.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "      raise ValueError(\"X must be a pandas DataFrame\")\n",
        "\n",
        "    for col in self.columns:\n",
        "      if col not in X.columns:\n",
        "        raise ValueError(f\"Column '{col}' not found in X\")\n",
        "\n",
        "    X_encoded = X.copy()\n",
        "\n",
        "    # Training/validation data\n",
        "    if y is not None:\n",
        "      if not isinstance(y, (pd.Series, np.ndarray)):\n",
        "        raise ValueError(\"y must be a pandas Series or numpy array\")\n",
        "\n",
        "      y = pd.Series(y, index=X.index) if isinstance(y, np.ndarray) else y\n",
        "\n",
        "      # Initialize encoding arrays for each column\n",
        "      train_encodings = {col: np.zeros(len(X)) for col in self.columns}\n",
        "\n",
        "      kf = KFold(\n",
        "          n_splits=self.n_splits,\n",
        "          shuffle=True,\n",
        "          random_state=self.random_state\n",
        "      )\n",
        "\n",
        "      for train_idx, val_idx in kf.split(X):\n",
        "        X_inner_train, X_inner_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_inner_train = y.iloc[train_idx]\n",
        "\n",
        "        # Compute fold-specific global mean\n",
        "        fold_global_mean = np.mean(y_inner_train)\n",
        "\n",
        "        for col in self.columns:\n",
        "          target_means = y_inner_train.groupby(X_inner_train[col]).mean()\n",
        "          category_counts = X_inner_train[col].value_counts()\n",
        "\n",
        "          weighted_mean = (\n",
        "              (\n",
        "                  category_counts\n",
        "                  * target_means\n",
        "                  + self.smoothing\n",
        "                  * fold_global_mean\n",
        "              )\n",
        "              / (category_counts + self.smoothing)\n",
        "          )\n",
        "\n",
        "          encoded_values = (\n",
        "              X_inner_val[col]\n",
        "              .map(weighted_mean)\n",
        "              .fillna(fold_global_mean)\n",
        "          )\n",
        "\n",
        "          train_encodings[col][val_idx] = encoded_values\n",
        "\n",
        "          X_encoded.loc[X.index[val_idx], f'{col}_encoded'] = (\n",
        "              train_encodings[col][val_idx]\n",
        "          )\n",
        "    else:\n",
        "      # Test data\n",
        "      for col in self.columns:\n",
        "        weighted_mean = (\n",
        "            (\n",
        "                self.category_counts_[col]\n",
        "                * self.target_means_[col]\n",
        "                + self.smoothing\n",
        "                * self.global_mean_\n",
        "            )\n",
        "            / (self.category_counts_[col] + self.smoothing)\n",
        "        )\n",
        "\n",
        "        X_encoded[f'{col}_encoded'] = (\n",
        "            X[col]\n",
        "            .map(weighted_mean)\n",
        "            .fillna(self.global_mean_)\n",
        "        )\n",
        "\n",
        "    # Drop original categorical columns\n",
        "    X_encoded = X_encoded.drop(columns=self.columns)\n",
        "\n",
        "    return X_encoded\n",
        "\n",
        "  def fit_transform(self, X, y):\n",
        "    \"\"\"\n",
        "    Fit the encoder and transform the training data.\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame\n",
        "    Training data with categorical columns.\n",
        "    - y : pandas Series or array-like\n",
        "    Target variable.\n",
        "\n",
        "    Returns:\n",
        "    X_encoded : pandas DataFrame\n",
        "    Transformed training data.\n",
        "    \"\"\"\n",
        "\n",
        "    return self.fit(X, y).transform(X, y)"
      ],
      "metadata": {
        "id": "U-_LmuJfHot3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'target_encoder_transformer.py',\n",
        "    destination_path = PATH / 'target_encoder_transformer.py'\n",
        ")"
      ],
      "metadata": {
        "id": "7qqLDFhyBRKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "_MhG8wVfDFmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from target_encoder_transformer import TargetEncoderTransformer\n",
        "\n",
        "target_encoder_transformer = TargetEncoderTransformer(\n",
        "    columns = df_train.select_dtypes(include=['string']).columns.tolist(),\n",
        "    smoothing=0.2922108633966015\n",
        ")\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=target_encoder_transformer,\n",
        "    experiment_name=\"Target_Encoder_Transformer\",\n",
        "    model_name=\"target_encoder_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "_Z78Gbajw4hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train and Test"
      ],
      "metadata": {
        "id": "TbmR7mByDH53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = apply_transformer(\n",
        "    df_train,\n",
        "    df_val,\n",
        "    target_encoder_transformer,\n",
        "    TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "E8ridi1n2LyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Feature Distribution Mismatches"
      ],
      "metadata": {
        "id": "y7FfAJ9MDRfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_feature_distribution_mismatches(\n",
        "    df_train[cols_to_encode],\n",
        "    df_val[cols_to_encode],\n",
        "    test_type=\"Chi-squared\"\n",
        ")"
      ],
      "metadata": {
        "id": "xVDRuHkODTR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling Numerical Features\n",
        "\n",
        "Multiple scaling techniques were tested to transform the numerical features:\n",
        "\n",
        "- **Methods tried:** `StandardScaler`, `RobustScaler`, `MinMaxScaler`, `MaxAbsScaler`, `QuantileTransformer`, `Normalizer`, `log1p`  \n",
        "- **Purpose:** Ensure numerical features are on comparable scales, reduce the effect of outliers and improve model convergence.\n",
        "\n",
        "Evaluation on validation data:\n",
        "\n",
        "1. **StandardScaler** achieved the best performance in terms of cross-validated RMSE.  \n",
        "2. **Followed by:** RobustScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer, Normalizer, and log1p transformation.\n",
        "\n",
        "**Decision:** The final model uses **StandardScaler** for scaling numerical features.\n",
        "\n",
        "**Takeaway:** This approach standardizes features to have zero mean and unit variance which is generally effective for tree-based and gradient-boosting models in regression tasks."
      ],
      "metadata": {
        "id": "qwqWc7_t__Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to scale\n",
        "columns_to_scale = (\n",
        "    df_train\n",
        "    .select_dtypes(include=['int64', 'float64'])\n",
        "    .drop(columns=[TARGET])\n",
        "    .columns\n",
        "    .tolist()\n",
        ")\n",
        "\n",
        "print(f'Columns to scale: {columns_to_scale}')\n",
        "X_train_full = df_train.drop(TARGET, axis=1)\n",
        "X_val = df_val.drop(TARGET, axis=1)\n",
        "\n",
        "n_trials = 500\n",
        "n_splits = 5\n",
        "n_repeats = 3\n",
        "mlflow_experiment = \"Scaling\"\n",
        "\n",
        "# SQLite storage configuration\n",
        "study_name = f\"{mlflow_experiment}_study\"\n",
        "storage = f\"sqlite:///{PATH}/{study_name}.db\"\n",
        "\n",
        "def apply_log1p(X, columns, clip_negative=False):\n",
        "  \"\"\"\n",
        "  Apply log1p transformation to specified columns, handling negative values.\n",
        "  \"\"\"\n",
        "  X_transformed = X.copy()\n",
        "\n",
        "  for col in columns:\n",
        "    # Clip negative values to zero before log1p\n",
        "    data = np.maximum(X[col].to_numpy(), 0)\n",
        "    transformed = np.log1p(data)\n",
        "    X_transformed[f\"{col}_scaled\"] = transformed\n",
        "    X_transformed = X_transformed.drop(columns=[col])\n",
        "\n",
        "  return X_transformed\n",
        "\n",
        "def objective(trial):\n",
        "  # Tune winsorization limits\n",
        "  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n",
        "\n",
        "  y_train_full = np.log1p(mstats.winsorize(\n",
        "      df_train[TARGET].to_numpy(dtype=np.float64),\n",
        "      limits=(0, 1-winsorize_upper)\n",
        "  ))\n",
        "\n",
        "  # Choose scaling method\n",
        "  scaling_method = trial.suggest_categorical(\n",
        "      'scaling_method',\n",
        "      [\n",
        "          'standard',\n",
        "          'robust',\n",
        "          'minmax',\n",
        "          'maxabs',\n",
        "          'quantile',\n",
        "          'normalizer',\n",
        "          'log1p'\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  # Tune scaler-specific parameters\n",
        "  scaler_params = {}\n",
        "\n",
        "  if scaling_method == 'robust':\n",
        "    quantile_lower = trial.suggest_float('robust_quantile_lower', 10, 40)\n",
        "    scaler_params['quantile_range'] = (quantile_lower, 100 - quantile_lower)\n",
        "\n",
        "    scaler_params['unit_variance'] = trial.suggest_categorical(\n",
        "        'robust_unit_variance',\n",
        "         [True, False]\n",
        "    )\n",
        "\n",
        "  elif scaling_method == 'minmax':\n",
        "    feature_range_min = trial.suggest_float('minmax_feature_range_min', -1, 0)\n",
        "\n",
        "    scaler_params['feature_range'] = (\n",
        "        feature_range_min,\n",
        "        feature_range_min + 1\n",
        "    )\n",
        "\n",
        "    scaler_params['clip'] = trial.suggest_categorical(\n",
        "        'minmax_clip',\n",
        "          [True, False]\n",
        "    )\n",
        "\n",
        "  elif scaling_method == 'quantile':\n",
        "    scaler_params['n_quantiles'] = trial.suggest_int(\n",
        "        'quantile_n_quantiles', 100, 1000\n",
        "    )\n",
        "\n",
        "    scaler_params['output_distribution'] = trial.suggest_categorical(\n",
        "        'quantile_output_distribution', ['uniform', 'normal']\n",
        "    )\n",
        "\n",
        "    scaler_params['subsample'] = trial.suggest_int(\n",
        "        'quantile_subsample', int(1e4), int(1e6)\n",
        "    )\n",
        "\n",
        "  elif scaling_method == 'normalizer':\n",
        "    scaler_params['norm'] = trial.suggest_categorical(\n",
        "        'normalizer_norm', ['l1', 'l2', 'max']\n",
        "    )\n",
        "\n",
        "  elif scaling_method == 'log1p':\n",
        "    scaler_params['clip_negative'] = trial.suggest_categorical(\n",
        "        'log1p_clip_negative', [True, False]\n",
        "    )\n",
        "\n",
        "  # Repeated K-Fold Cross-Validation\n",
        "  rkf = RepeatedKFold(\n",
        "      n_splits=n_splits,\n",
        "      n_repeats=n_repeats,\n",
        "      random_state=RANDOM_STATE\n",
        "  )\n",
        "\n",
        "  rmse_list = []\n",
        "\n",
        "  # Update columns_to_scale with scaled names for model training\n",
        "  scaled_columns = [f\"{col}_scaled\" for col in columns_to_scale]\n",
        "\n",
        "  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n",
        "    X_train, X_val_fold = (\n",
        "        X_train_full.iloc[train_idx],\n",
        "        X_train_full.iloc[val_idx]\n",
        "    )\n",
        "    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n",
        "\n",
        "    # Apply selected scaler\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_val_fold_scaled = X_val_fold.copy()\n",
        "\n",
        "    if columns_to_scale and scaling_method != 'log1p':\n",
        "      # Initialize scaler for this fold\n",
        "      if scaling_method == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "      elif scaling_method == 'robust':\n",
        "        scaler = RobustScaler(**scaler_params)\n",
        "\n",
        "      elif scaling_method == 'minmax':\n",
        "        scaler = MinMaxScaler(**scaler_params)\n",
        "\n",
        "      elif scaling_method == 'maxabs':\n",
        "        scaler = MaxAbsScaler()\n",
        "\n",
        "      elif scaling_method == 'quantile':\n",
        "        scaler = QuantileTransformer(**scaler_params, random_state=RANDOM_STATE)\n",
        "\n",
        "      elif scaling_method == 'normalizer':\n",
        "        scaler = Normalizer(**scaler_params)\n",
        "\n",
        "      # Apply scaling and rename columns\n",
        "      scaled_data = scaler.fit_transform(X_train[columns_to_scale])\n",
        "      X_train_scaled = X_train_scaled.drop(columns=columns_to_scale)\n",
        "      X_train_scaled[scaled_columns] = scaled_data\n",
        "\n",
        "      scaled_data_val = scaler.transform(X_val_fold[columns_to_scale])\n",
        "      X_val_fold_scaled = X_val_fold_scaled.drop(columns=columns_to_scale)\n",
        "      X_val_fold_scaled[scaled_columns] = scaled_data_val\n",
        "\n",
        "    elif columns_to_scale and scaling_method == 'log1p':\n",
        "      X_train_scaled = apply_log1p(\n",
        "          X_train, columns_to_scale, scaler_params.get('clip_negative', False)\n",
        "      )\n",
        "      X_val_fold_scaled = apply_log1p(\n",
        "          X_val_fold, columns_to_scale, scaler_params.get('clip_negative', False)\n",
        "      )\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective=trial.suggest_categorical('objective', ['reg:squarederror']),\n",
        "        booster=trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        colsample_bylevel=trial.suggest_float('colsample_bylevel', 0.2, 0.8),\n",
        "        colsample_bynode=trial.suggest_float('colsample_bynode', 0.2, 1.0),\n",
        "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.2, 0.8),\n",
        "        gamma=trial.suggest_float('gamma', 1e-8, 30, log=True),\n",
        "        grow_policy=trial.suggest_categorical(\n",
        "            'grow_policy',\n",
        "            ['depthwise', 'lossguide']\n",
        "        ),\n",
        "        learning_rate=trial.suggest_float(\n",
        "            'learning_rate',\n",
        "            0.001,\n",
        "            0.2,\n",
        "            log=True\n",
        "        ),\n",
        "        max_bin=trial.suggest_int('max_bin', 128, 512),\n",
        "        max_depth=trial.suggest_int('max_depth', 2, 6),\n",
        "        min_child_weight=trial.suggest_float('min_child_weight', 1, 20),\n",
        "        n_estimators=trial.suggest_int('n_estimators', 100, 3000),\n",
        "        n_jobs=N_JOBS,\n",
        "        random_state=RANDOM_STATE,\n",
        "        sampling_method=trial.suggest_categorical(\n",
        "            'sampling_method',\n",
        "            [\"uniform\"]\n",
        "        ),\n",
        "        subsample=trial.suggest_float('subsample', 0.2, 0.8),\n",
        "        tree_method=trial.suggest_categorical('tree_method', [\"auto\", \"hist\"]),\n",
        "        base_score=trial.suggest_float('base_score', 0.1, 0.9),\n",
        "        max_leaves=trial.suggest_int('max_leaves', 0, 16),\n",
        "        num_parallel_tree=trial.suggest_int('num_parallel_tree', 1, 3),\n",
        "        reg_alpha=trial.suggest_float('reg_alpha', 1e-8, 20, log=True),\n",
        "        reg_lambda=trial.suggest_float('reg_lambda', 1e-8, 20, log=True),\n",
        "        scale_pos_weight=trial.suggest_float('scale_pos_weight', 0.5, 1.5),\n",
        "        validate_parameters=True,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_val_fold_scaled)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n",
        "    gc.collect()\n",
        "\n",
        "    trial.report(np.mean(rmse_list), step=fold_idx)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.TrialPruned()\n",
        "\n",
        "  return np.mean(rmse_list)\n",
        "\n",
        "# Start MLflow experiment\n",
        "mlflow.set_experiment(mlflow_experiment)\n",
        "\n",
        "with mlflow.start_run():\n",
        "  # - Ensure to use the same direction,\n",
        "  # pruner (min_resource, max_resource, reduction_factor, etc.) if you are\n",
        "  # loading an existing study\n",
        "  study = optuna.create_study(\n",
        "      study_name=study_name,\n",
        "      storage=storage,\n",
        "      direction=\"minimize\",\n",
        "      pruner=optuna.pruners.HyperbandPruner(\n",
        "          min_resource=max(1, n_splits),\n",
        "          max_resource=n_splits * n_repeats,\n",
        "          reduction_factor=2\n",
        "      ),\n",
        "      sampler=optuna.samplers.TPESampler(\n",
        "          n_startup_trials=max(1, int(n_trials * 0.2)),\n",
        "          seed=RANDOM_STATE,\n",
        "          multivariate=True\n",
        "      ),\n",
        "      load_if_exists=True\n",
        "  )\n",
        "\n",
        "  study.optimize(objective, n_trials=n_trials)\n",
        "  log_optuna_best_trial_search_space(study)\n",
        "\n",
        "  # Log Optuna storage details\n",
        "  mlflow.log_param(\"optuna_storage\", storage)\n",
        "  mlflow.log_param(\"optuna_study_name\", study_name)\n",
        "\n",
        "  # Log experiment parameters\n",
        "  mlflow.log_param(\"n_trials\", n_trials)\n",
        "  mlflow.log_param(\"n_splits\", n_splits)\n",
        "  mlflow.log_param(\"n_repeats\", n_repeats)\n",
        "\n",
        "  # Log cols_to_scale\n",
        "  mlflow.log_param(\"columns_to_scale\", \",\".join(columns_to_scale))\n",
        "\n",
        "  fanova_importances = get_param_importances(\n",
        "      study,\n",
        "      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n",
        "      target=(\n",
        "          lambda t: t.value\n",
        "          if t.state == optuna.trial.TrialState.COMPLETE\n",
        "          else None\n",
        "      ),\n",
        "      normalize=True\n",
        "  )\n",
        "\n",
        "  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n",
        "\n",
        "  for param, importance in fanova_importances.items():\n",
        "    mlflow.log_param(f\"fanova_{param}\", importance)\n",
        "\n",
        "  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n",
        "\n",
        "  # Get best parameters and trial attributes\n",
        "  best_params = study.best_params\n",
        "  best_trial = study.best_trial\n",
        "\n",
        "  for param, value in best_params.items():\n",
        "    mlflow.log_param(param, value)\n",
        "\n",
        "  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n",
        "\n",
        "  print(\"Best Params:\", best_params)\n",
        "  print(\"Best CV RMSE:\", study.best_value)\n",
        "\n",
        "  # Apply best winsorization and transformation to final training and\n",
        "  # validation sets\n",
        "  y_train_full = np.log1p(mstats.winsorize(\n",
        "      df_train[TARGET].to_numpy(dtype=np.float64),\n",
        "      limits=(0, 1-best_params['winsorize_upper'])\n",
        "  ))\n",
        "\n",
        "  # Compute winsorization thresholds from training data\n",
        "  # Apply fixed thresholds to validation data\n",
        "  y_val = np.log1p(np.clip(\n",
        "      df_val[TARGET],\n",
        "      0,\n",
        "      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n",
        "  ))\n",
        "\n",
        "  # Apply best scaling method to final training and validation sets\n",
        "  scaling_method = best_params['scaling_method']\n",
        "  scaler_params = {}\n",
        "\n",
        "  if scaling_method == 'robust':\n",
        "    quantile_lower = best_params['robust_quantile_lower']\n",
        "    scaler_params['quantile_range'] = (quantile_lower, 100 - quantile_lower)\n",
        "    scaler_params['unit_variance'] = best_params['robust_unit_variance']\n",
        "    scaler = RobustScaler(**scaler_params)\n",
        "\n",
        "  elif scaling_method == 'minmax':\n",
        "    feature_range_min = best_params['minmax_feature_range_min']\n",
        "    scaler_params['feature_range'] = (feature_range_min, feature_range_min + 1)\n",
        "    scaler_params['clip'] = best_params['minmax_clip']\n",
        "    scaler = MinMaxScaler(**scaler_params)\n",
        "\n",
        "  elif scaling_method == 'quantile':\n",
        "    scaler_params['n_quantiles'] = best_params['quantile_n_quantiles']\n",
        "\n",
        "    scaler_params['output_distribution'] = best_params[\n",
        "        'quantile_output_distribution'\n",
        "    ]\n",
        "\n",
        "    scaler_params['subsample'] = best_params['quantile_subsample']\n",
        "    scaler = QuantileTransformer(**scaler_params, random_state=RANDOM_STATE)\n",
        "\n",
        "  elif scaling_method == 'normalizer':\n",
        "    scaler_params['norm'] = best_params['normalizer_norm']\n",
        "    scaler = Normalizer(**scaler_params)\n",
        "\n",
        "  elif scaling_method == 'standard':\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "  elif scaling_method == 'maxabs':\n",
        "    scaler = MaxAbsScaler()\n",
        "\n",
        "  elif scaling_method == 'log1p':\n",
        "    scaler_params['clip_negative'] = best_params['log1p_clip_negative']\n",
        "\n",
        "  X_final = X_train_full.copy()\n",
        "  X_val_scaled = X_val.copy()\n",
        "\n",
        "  scaled_columns = [f\"{col}_scaled\" for col in columns_to_scale]\n",
        "\n",
        "  if columns_to_scale and scaling_method != 'log1p':\n",
        "    # Apply scaling and rename columns\n",
        "    scaled_data = scaler.fit_transform(X_train_full[columns_to_scale])\n",
        "    X_final = X_final.drop(columns=columns_to_scale)\n",
        "    X_final[scaled_columns] = scaled_data\n",
        "\n",
        "    scaled_data_validation = scaler.transform(X_val[columns_to_scale])\n",
        "    X_val_scaled = X_val_scaled.drop(columns=columns_to_scale)\n",
        "    X_val_scaled[scaled_columns] = scaled_data_validation\n",
        "\n",
        "  elif columns_to_scale and scaling_method == 'log1p':\n",
        "    X_final = apply_log1p(\n",
        "        X_train_full,\n",
        "        columns_to_scale,\n",
        "        scaler_params.get('clip_negative', False)\n",
        "    )\n",
        "\n",
        "    X_val_scaled = apply_log1p(\n",
        "        X_val, columns_to_scale, scaler_params.get('clip_negative', False)\n",
        "    )\n",
        "\n",
        "  # Train final model with best XGBoost parameters\n",
        "  filtered_params = {\n",
        "      k: v for k, v in best_params.items()\n",
        "      if k in xgb.XGBRegressor().get_params()\n",
        "  }\n",
        "\n",
        "  # all the constant hyperparameters should be added here manually and are not\n",
        "  # part of study.best_params\n",
        "  final_model = xgb.XGBRegressor(\n",
        "      **filtered_params,\n",
        "      n_jobs=N_JOBS,\n",
        "      random_state=RANDOM_STATE,\n",
        "      validate_parameters=True,\n",
        "      verbosity=0\n",
        "  )\n",
        "\n",
        "  X_final = X_final[sorted(X_final.columns)]\n",
        "  final_model.fit(X_final, y_train_full)\n",
        "  X_val_scaled = X_val_scaled[sorted(X_val_scaled.columns)]\n",
        "  y_pred = final_model.predict(X_val_scaled)\n",
        "\n",
        "  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")"
      ],
      "metadata": {
        "id": "0PPNNlUXVb6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaler Transformer"
      ],
      "metadata": {
        "id": "cv6ACT18YiUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile standard_scaler_transformer.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class StandardScalerTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    self.scaler = StandardScaler()\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit the scaler to the data.\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame or array-like, shape (n_samples, n_features)\n",
        "    Training data to fit the scaler\n",
        "    - y : None\n",
        "    Ignored. For compatibility with scikit-learn pipeline.\n",
        "\n",
        "    Returns:\n",
        "    self : object\n",
        "    Returns self.\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if not isinstance(X, (pd.DataFrame, np.ndarray)):\n",
        "      raise ValueError(\"Input must be a pandas DataFrame or numpy array\")\n",
        "\n",
        "    # Convert to numpy array if DataFrame\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "      self.feature_names_ = X.columns.tolist()\n",
        "      X = X.values\n",
        "    else:\n",
        "      self.feature_names_ = None\n",
        "      X = np.asarray(X)\n",
        "\n",
        "    # Store number of features for validation in transform\n",
        "    self.n_features_in_ = X.shape[1] if X.ndim > 1 else 1\n",
        "\n",
        "    # Reshape 1D array to 2D for StandardScaler\n",
        "    if X.ndim == 1:\n",
        "      X = X.reshape(-1, 1)\n",
        "\n",
        "    self.scaler.fit(X)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Transform the data using the fitted scaler and return as DataFrame\n",
        "    with '_scaled' appended to column names.\n",
        "\n",
        "    Parameters:\n",
        "    X : pandas DataFrame or array-like, shape (n_samples, n_features)\n",
        "    Data to transform\n",
        "\n",
        "    Returns:\n",
        "    X_transformed : pandas DataFrame\n",
        "    Scaled data with '_scaled' appended to column names\n",
        "    \"\"\"\n",
        "    # Check if fitted\n",
        "    if not hasattr(self.scaler, 'scale_'):\n",
        "      raise ValueError(\"\"\"\n",
        "      This StandardScalerTransformer instance is not fitted yet.\n",
        "      Call 'fit' first.\n",
        "      \"\"\")\n",
        "\n",
        "    # Input validation\n",
        "    if not isinstance(X, (pd.DataFrame, np.ndarray)):\n",
        "      raise ValueError(\"Input must be a pandas DataFrame or numpy array\")\n",
        "\n",
        "    # Convert to numpy array if DataFrame\n",
        "    is_dataframe = isinstance(X, pd.DataFrame)\n",
        "\n",
        "    if is_dataframe:\n",
        "      X_index = X.index\n",
        "      X_values = X.values\n",
        "    else:\n",
        "      X_index = None\n",
        "      X_values = np.asarray(X)\n",
        "\n",
        "    # Validate number of features\n",
        "    n_features = X_values.shape[1] if X_values.ndim > 1 else 1\n",
        "\n",
        "    if n_features != self.n_features_in_:\n",
        "      raise ValueError(f\"\"\"\n",
        "      X has {n_features} features,\n",
        "      but CustomScaler was fitted with {self.n_features_in_} features\n",
        "      \"\"\")\n",
        "\n",
        "    # Reshape 1D array to 2D for StandardScaler\n",
        "    if X_values.ndim == 1:\n",
        "      X_values = X_values.reshape(-1, 1)\n",
        "\n",
        "    # Transform the data\n",
        "    X_scaled = self.scaler.transform(X_values)\n",
        "\n",
        "    # Create new column names with '_scaled' suffix\n",
        "    if self.feature_names_ is not None:\n",
        "      scaled_columns = [f\"{col}_scaled\" for col in self.feature_names_]\n",
        "    else:\n",
        "      scaled_columns = [f\"feature_{i}_scaled\" for i in range(X_scaled.shape[1])]\n",
        "\n",
        "    # Return as DataFrame with preserved index\n",
        "    return pd.DataFrame(\n",
        "        X_scaled,\n",
        "        columns=scaled_columns,\n",
        "        index=X_index if is_dataframe else None\n",
        "    )"
      ],
      "metadata": {
        "id": "l0OitH8pYbn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'standard_scaler_transformer.py',\n",
        "    destination_path = PATH / 'standard_scaler_transformer.py'\n",
        ")"
      ],
      "metadata": {
        "id": "LJx-L3o2BYmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "4XreJMywZ0Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from standard_scaler_transformer import StandardScalerTransformer\n",
        "\n",
        "standard_scaler_transformer = StandardScalerTransformer()\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=standard_scaler_transformer,\n",
        "    experiment_name=\"Standard_Scaler_Transformer\",\n",
        "    model_name=\"standard_scaler_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "_bb_9yt4ZxLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Train and Test"
      ],
      "metadata": {
        "id": "aQnlZ7jcZPfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = apply_transformer(\n",
        "    df_train,\n",
        "    df_val,\n",
        "    standard_scaler_transformer,\n",
        "    TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "esWBOYE5Zdvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Feature Distribution Mismatches"
      ],
      "metadata": {
        "id": "sVvKPSWODwTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_feature_distribution_mismatches(\n",
        "    df_train[skewed_cols],\n",
        "    df_val[skewed_cols],\n",
        "    test_type=\"Kolmogorov-Smirnov\"\n",
        ")"
      ],
      "metadata": {
        "id": "L79lGu40DxQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kurtosis Analysis for Feature Distribution Evaluation\n",
        "\n",
        "We compute kurtosis (using Fisher's definition) to assess the \"tailedness\" of numerical features after scaling:\n",
        "\n",
        "- **Kurtosis ≈ 0:** Normal-like tails (ideal)\n",
        "- **Kurtosis > 0:** Heavy tails (outliers present)\n",
        "- **Kurtosis < 0:** Light tails (fewer extremes)\n",
        "\n",
        "While perfectly normal kurtosis is not required, high values (e.g. >3) may indicate outlier sensitivity that could hurt model performance - especially for algorithms like linear models or SVMs.\n",
        "\n",
        "This analysis helps **diagnose potential issues** and guides decisions on further transformation (e.g., Box-Cox, Yeo-Johnson) if needed to improve robustness.\n",
        "\n",
        "**Note:** We do not force kurtosis to zero because real-world data rarely follows perfect distributions. Instead, we use this as a diagnostic tool, not a strict rule."
      ],
      "metadata": {
        "id": "J-hudgR_iWfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kurtosis_values = (\n",
        "    df_train\n",
        "    .drop(TARGET, axis=1)\n",
        "    .select_dtypes(include=[np.number])\n",
        "    .apply(stats.kurtosis)\n",
        ")\n",
        "\n",
        "print(kurtosis_values.round(2))"
      ],
      "metadata": {
        "id": "h8K1FyvCibI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interaction Features Based on Correlation and Domain Logic\n",
        "\n",
        "We create multiplicative interaction terms between key transformed features to capture **potential synergies** between variables that may not be evident in isolation.\n",
        "\n",
        "Selection criteria:\n",
        "\n",
        "- **Moderate-to-high correlation:** Suggests meaningful relationships worth exploring.\n",
        "- **Domain relevance:** Interactions reflect real-world dynamics - e.g. a popular publisher with high average clicks may have a compounding effect on performance.\n",
        "- **Post-scaling consistency:** All features are already scaled/transformed, ensuring interaction terms are numerically stable and suitable for models sensitive to scale.\n",
        "\n",
        "These interactions help models learn **nonlinear patterns** without requiring complex algorithms - especially valuable if using linear models or boosting with shallow trees.\n",
        "\n",
        "**Note:** We avoid exhaustive pairwise combinations to prevent overfitting and maintain interpretability. Only theoretically and statistically justified pairs are included."
      ],
      "metadata": {
        "id": "cRR0Qs8NwAMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature pairs are selected for creating the interaction terms considering\n",
        "# the correlation value between them.\n",
        "\n",
        "# Create interaction terms for the selected pairs\n",
        "df_train['pub_popularity_x_pub_clicks'] = (\n",
        "    df_train['publisher_popularity_transformed_scaled'] *\n",
        "    df_train['publisher_avg_clicks_transformed_scaled']\n",
        ")\n",
        "\n",
        "df_train['market_clicks_x_market_popularity'] = (\n",
        "    df_train['market_avg_clicks_transformed_scaled'] *\n",
        "    df_train['market_popularity_transformed_scaled']\n",
        ")\n",
        "\n",
        "df_train['pub_clicks_x_pub_encoded'] = (\n",
        "    df_train['publisher_avg_clicks_transformed_scaled'] *\n",
        "    df_train['publisher_encoded_scaled']\n",
        ")\n",
        "\n",
        "df_train['market_popularity_x_market_id'] = (\n",
        "    df_train['market_popularity_transformed_scaled'] *\n",
        "    df_train['market_id_encoded_scaled']\n",
        ")\n",
        "\n",
        "df_train['market_clicks_x_market_id'] = (\n",
        "    df_train['market_avg_clicks_transformed_scaled'] *\n",
        "    df_train['market_id_encoded_scaled']\n",
        ")\n",
        "\n",
        "# Verify the updated DataFrame\n",
        "print(\"Shape of updated df_train:\", df_train.shape)\n",
        "print(\"Columns in updated df_train:\", df_train.columns.tolist())"
      ],
      "metadata": {
        "id": "PpC9HmBwqPKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Features"
      ],
      "metadata": {
        "id": "XzogMxe9q0Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diagnosing Non-Linearity with **Ramsey's RESET Test**\n",
        "\n",
        "To assess whether **non-linear relationships** exist between features and the target, we apply Ramsey's RESET test on a baseline linear model.\n",
        "\n",
        "Why this matters:\n",
        "- Linear models assume a straight-line relationship but real-world data often violates this.\n",
        "- RESET test checks if adding powers of predicted values improves fit - a significant result (low p-value) indicates **missed non-linearity**.\n",
        "\n",
        "This is not about adding polynomial features directly - it is a **diagnostic tool** to validate the need for more complex modeling strategies."
      ],
      "metadata": {
        "id": "c0FI_yVPwPO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_train.drop(TARGET, axis=1)\n",
        "y = df_train[TARGET]\n",
        "\n",
        "# Add intercept\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(y, X).fit()"
      ],
      "metadata": {
        "id": "8xiIGN-Pq5-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RESET test\n",
        "# Tests for quadratic terms\n",
        "reset_result = linear_reset(model, power=2)\n",
        "\n",
        "# Significant p-value suggests non-linearity\n",
        "print(reset_result.summary())"
      ],
      "metadata": {
        "id": "YqZjJZH1tGQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RESET test\n",
        "# Tests for quadratic terms\n",
        "reset_result = linear_reset(model, power=3)\n",
        "\n",
        "# Significant p-value suggests non-linearity\n",
        "print(reset_result.summary())"
      ],
      "metadata": {
        "id": "XRwU4sKwtHTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run RESET test\n",
        "# Tests for quadratic terms\n",
        "reset_result = linear_reset(model, power=4)\n",
        "\n",
        "# Significant p-value suggests non-linearity\n",
        "print(reset_result.summary())"
      ],
      "metadata": {
        "id": "VnNloBkGtH1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Multicollinearity with VIF and Correlation Analysis\n",
        "\n",
        "To ensure model stability, interpretability, and numerical reliability, we diagnose and address **multicollinearity** using two complementary methods:\n",
        "\n",
        "1. **Variance Inflation Factor (VIF)**: Quantifies how much feature variance is inflated due to correlation with other features.\n",
        "   - VIF > 5: Investigate\n",
        "   - VIF > 10: Strong multicollinearity - action needed\n",
        "\n",
        "2. **Correlation Matrix Heatmap**: Visualizes pairwise correlations to identify redundant features.\n",
        "\n",
        "- **Strategy for Feature Removal:**\n",
        "\n",
        "  When two (or more) features are highly correlated:\n",
        "  - We compare their **individual correlation with the target**.\n",
        "  - Retain the feature more strongly associated with the target.\n",
        "  - Drop the weaker one to reduce redundancy without losing predictive signal.\n",
        "\n",
        "- **Actions Taken:**\n",
        "  - Iteratively removed `pub_clicks_x_pub_encoded`, `market_popularity_x_market_id` and `market_clicks_x_market_id` based on high VIF/correlation and lower relevance to target.\n",
        "  - Re-evaluated VIF and correlation after each removal to ensure progressive improvement.\n",
        "\n",
        "This **target-aware, iterative approach** balances multicollinearity reduction with preservation of predictive power - critical for robust modeling, especially in regression and inference settings."
      ],
      "metadata": {
        "id": "catFxb8ZwRFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_vif(df_train, TARGET)"
      ],
      "metadata": {
        "id": "Eclv0g_Rukhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_and_save_correlation_matrix(df_train, PATH)"
      ],
      "metadata": {
        "id": "D2kmSwrptfdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - market_clicks_x_market_id and highly correlated with\n",
        "# market_clicks_x_market_popularity and pub_clicks_x_pub_encoded. dropped the\n",
        "# column pub_clicks_x_pub_encoded as it has the lowest correlation with\n",
        "# the target column.\n",
        "\n",
        "# - market_clicks_x_market_popularity is highly correlated with\n",
        "# market_popularity_x_market_id and market_clicks_x_market_id. dropped the\n",
        "# column market_popularity_x_market_id as it has the lowest correlation with\n",
        "# the target column.\n",
        "\n",
        "df_train.drop(\n",
        "    columns=['pub_clicks_x_pub_encoded', 'market_popularity_x_market_id'],\n",
        "    inplace = True\n",
        ")"
      ],
      "metadata": {
        "id": "DcKnFf5XIe3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_vif(df_train, TARGET)"
      ],
      "metadata": {
        "id": "rjo-6dK7l7Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_and_save_correlation_matrix(df_train, PATH)"
      ],
      "metadata": {
        "id": "8DzVa1cSuzou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - market_clicks_x_market_id is highly correlated with\n",
        "# market_clicks_x_market_popularity. dropped the\n",
        "# column market_clicks_x_market_id as it has the lowest correlation with\n",
        "# the target column.\n",
        "\n",
        "# - market_clicks_x_market_popularity is highly correlated with\n",
        "# market_popularity_x_market_id and market_clicks_x_market_id. dropped the\n",
        "# column market_clicks_x_market_popularity as it has the lowest correlation with\n",
        "# the target column.\n",
        "\n",
        "df_train.drop(\n",
        "    columns=['market_clicks_x_market_id'],\n",
        "    inplace = True\n",
        ")"
      ],
      "metadata": {
        "id": "VHN0gPcdqTJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_vif(df_train, TARGET)"
      ],
      "metadata": {
        "id": "0mLy6cs0w6Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection: Balancing Predictive Power, Stability and Simplicity\n",
        "\n",
        "We apply a multi-stage, diagnostic-driven strategy to refine the feature set and improve model generalization:\n",
        "\n",
        "- **Correlation with Target:**  \n",
        "We first assess each feature's absolute Pearson correlation with the target (`cpa`) to prioritize those with stronger linear signal.\n",
        "\n",
        "- **High Pairwise Correlation (Redundancy Removal):**  \n",
        "Features with correlation > 0.8 are considered redundant. When a pair is highly correlated, we retain the one more strongly linked to the target, minimizing information loss while reducing multicollinearity.\n",
        "\n",
        "> Removed:  \n",
        "> - `publisher_popularity_transformed_scaled` (vs. `publisher_avg_clicks`)  \n",
        "> - `market_avg_clicks_transformed_scaled` (vs. `market_popularity`)\n",
        "\n",
        "- **Low Variance Check:**  \n",
        "Features with variance < 0.01 contribute little to model discrimination and can destabilize some algorithms.  \n",
        "We remove such features to simplify the model without sacrificing predictive power.\n",
        "\n",
        "- **Final Diagnostics:**  \n",
        "After filtering, we re-check:\n",
        "  - **VIF** - Ensure multicollinearity is under control\n",
        "  - **Correlation matrix** - Confirm no high-correlation pairs remain\n",
        "\n",
        "This iterative, evidence-based approach ensures our final feature set is:\n",
        "- Predictive  \n",
        "- Clean (no redundancy)  \n",
        "- Robust to overfitting and numerical instability\n",
        "\n",
        "It reflects a balance between statistical rigor and practical modeling - not just throwing all features at a model, but curating them intentionally."
      ],
      "metadata": {
        "id": "PjR_CIYtLIfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pearson Correlation with Target"
      ],
      "metadata": {
        "id": "Ewm_YUHKLPLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overview\n",
        "(\n",
        "    df_train.drop(TARGET, axis=1)\n",
        "    .corrwith(df_train[TARGET])\n",
        "    .abs()\n",
        "    .sort_values(ascending=False)\n",
        ")"
      ],
      "metadata": {
        "id": "hhiN1V_Gj30L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Highly Correlated Features"
      ],
      "metadata": {
        "id": "s-d_X0Fmx3ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there is any pair of features whose correlation values is more\n",
        "# than 0.8\n",
        "plot_and_save_correlation_matrix(df_train, PATH)"
      ],
      "metadata": {
        "id": "g77Yiz6Gx9Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - publisher_popularity_transformed_scaled and\n",
        "# publisher_avg_clicks_transformed_scaled have correlation of 0.843127458401863.\n",
        "# Removing the column publisher_popularity_transformed_scaled as it has lower\n",
        "# correlation with the target column.\n",
        "\n",
        "# - market_popularity_transformed_scaled and\n",
        "# market_avg_clicks_transformed_scaled have correlation of 0.804201534180658.\n",
        "# Removing the column market_avg_clicks_transformed_scaled as it has lower\n",
        "# correlation with the target column.\n",
        "\n",
        "# - publisher_avg_clicks_transformed_scaled and\n",
        "# publisher_popularity_transformed_scaled have correlation of 0.843127458401863.\n",
        "# Column publisher_popularity_transformed_scaled already have been decided to\n",
        "# drop.\n",
        "\n",
        "# - market_avg_clicks_transformed_scaled and\n",
        "# market_popularity_transformed_scaled have correlation of 0.804201534180658.\n",
        "# Column market_avg_clicks_transformed_scaled already have been decided to\n",
        "# drop.\n",
        "\n",
        "df_train.drop(\n",
        "    columns=[\n",
        "        'publisher_popularity_transformed_scaled',\n",
        "        'market_avg_clicks_transformed_scaled'\n",
        "    ],\n",
        "    inplace = True\n",
        ")"
      ],
      "metadata": {
        "id": "HdQxvkhV0ZS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there is any pair of features whose correlation values is more than 0.8\n",
        "plot_and_save_correlation_matrix(df_train, PATH)"
      ],
      "metadata": {
        "id": "0oXRE9a72Dtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Variance"
      ],
      "metadata": {
        "id": "QvAnTMhx4U4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate variance for all numeric features, excluding TARGET\n",
        "variances = df_train.drop(TARGET, axis=1).var(numeric_only=True)\n",
        "\n",
        "# Sort variances in descending order\n",
        "sorted_variances = variances.sort_values(ascending=False)\n",
        "\n",
        "# Print variance for each feature\n",
        "print(\"Variance of each feature (in descending order):\")\n",
        "\n",
        "for feature, variance in sorted_variances.items():\n",
        "  print(f\"{feature}: {variance:.4f}\")"
      ],
      "metadata": {
        "id": "x3LNrTB34vVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - Remove features with variance < 0.01\n",
        "# - In general, features with variance < 0.05 can be dropped if that\n",
        "# improves the model performance.\n",
        "\n",
        "threshold = 0.01\n",
        "\n",
        "X = df_train.drop(TARGET, axis=1)\n",
        "selector = VarianceThreshold(threshold=threshold)\n",
        "X_var = selector.fit_transform(X)\n",
        "dropped_features_var = X.columns[~selector.get_support()].tolist()\n",
        "\n",
        "if dropped_features_var:\n",
        "  df_train.drop(columns=dropped_features_var, inplace=True)\n",
        "  print(\"Features dropped due to low variance: \", dropped_features_var)\n",
        "else:\n",
        "  print(\"No features dropped due to low variance.\")"
      ],
      "metadata": {
        "id": "uAl5vwdG4Y3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multicollinearity and Correlation (Final Check)"
      ],
      "metadata": {
        "id": "JD4Ha6rvZnbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check multicollinearity and correlation before finalising the features\n",
        "# one last time\n",
        "calculate_vif(df_train, TARGET)"
      ],
      "metadata": {
        "id": "FcfY_2cCZWbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_and_save_correlation_matrix(df_train, PATH)"
      ],
      "metadata": {
        "id": "HPDcEyGQZtvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Align Train and Validation Data Features"
      ],
      "metadata": {
        "id": "t7wOmqcLXCwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_only = compare_features_train_validation(df_train, df_val)"
      ],
      "metadata": {
        "id": "C5V_rLlMbE7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interaction Transformer"
      ],
      "metadata": {
        "id": "27Jut5zgpQhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile interaction_transformer.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class InteractionTransformer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, interaction_groups=None, columns_to_drop=None):\n",
        "    \"\"\"\n",
        "    Initialize the transformer with interaction groups and columns to drop.\n",
        "\n",
        "    Parameters:\n",
        "    - interaction_groups : list of lists\n",
        "      List of lists, each containing two input column names and the desired\n",
        "      interaction term name\n",
        "      e.g., [['col1', 'col2', 'col1_x_col2'], ['col3', 'col4', 'col3_x_col4']]\n",
        "    - columns_to_drop : list\n",
        "      List of column names to drop from the DataFrame\n",
        "    \"\"\"\n",
        "    self.interaction_groups = (\n",
        "        interaction_groups if interaction_groups is not None else []\n",
        "    )\n",
        "\n",
        "    self.columns_to_drop = (\n",
        "        columns_to_drop if columns_to_drop is not None else []\n",
        "    )\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Fit the transformer (no-op, included for scikit-learn compatibility).\n",
        "    \"\"\"\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\"\n",
        "    Create interaction terms and drop specified columns.\n",
        "\n",
        "    Parameters:\n",
        "    - X : pandas DataFrame\n",
        "      Input DataFrame\n",
        "\n",
        "    Returns:\n",
        "    pandas DataFrame with interaction terms added and specified columns dropped\n",
        "    \"\"\"\n",
        "    X_transformed = X.copy()\n",
        "\n",
        "    # Create interaction terms\n",
        "    for col1, col2, interaction_name in self.interaction_groups:\n",
        "      # Multiply the columns to create interaction term\n",
        "      X_transformed[interaction_name] = (\n",
        "          X_transformed[col1] * X_transformed[col2]\n",
        "      )\n",
        "\n",
        "    # Drop specified columns\n",
        "    columns_to_drop = [\n",
        "        col for col in self.columns_to_drop if col in X_transformed.columns\n",
        "    ]\n",
        "\n",
        "    if columns_to_drop:\n",
        "      X_transformed = X_transformed.drop(columns=columns_to_drop)\n",
        "\n",
        "    return X_transformed"
      ],
      "metadata": {
        "id": "LuG8sG0IlALa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'interaction_transformer.py',\n",
        "    destination_path = PATH / 'interaction_transformer.py'\n",
        ")"
      ],
      "metadata": {
        "id": "9RJ8SMgkBhna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fitted Model"
      ],
      "metadata": {
        "id": "_momnNRzpYzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from interaction_transformer import InteractionTransformer\n",
        "\n",
        "interaction_groups = [\n",
        "    [\n",
        "        'publisher_popularity_transformed_scaled',\n",
        "        'publisher_avg_clicks_transformed_scaled',\n",
        "        'pub_popularity_x_pub_clicks'\n",
        "    ],\n",
        "    [\n",
        "        'market_avg_clicks_transformed_scaled',\n",
        "        'market_popularity_transformed_scaled',\n",
        "        'market_clicks_x_market_popularity'\n",
        "    ]\n",
        "]\n",
        "\n",
        "columns_to_drop = [\n",
        "    'publisher_popularity_transformed_scaled',\n",
        "    'market_avg_clicks_transformed_scaled'\n",
        "]\n",
        "\n",
        "interaction_transformer = InteractionTransformer(\n",
        "    interaction_groups = interaction_groups, columns_to_drop = columns_to_drop\n",
        ")\n",
        "\n",
        "log_transformer_to_mlflow(\n",
        "    transformer=interaction_transformer,\n",
        "    experiment_name=\"Interaction_Transformer\",\n",
        "    model_name=\"interaction_transformer\",\n",
        "    df=df_train,\n",
        "    target_col=TARGET\n",
        ")"
      ],
      "metadata": {
        "id": "TEF5qmx9nIjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Validation"
      ],
      "metadata": {
        "id": "hWE81TP4peRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = pd.concat(\n",
        "      [\n",
        "          interaction_transformer.transform(df_val.drop(TARGET, axis=1)),\n",
        "          df_val[TARGET]\n",
        "      ],\n",
        "      axis=1\n",
        "  )"
      ],
      "metadata": {
        "id": "miN5IKfho2Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = compare_features_train_validation(df_train, df_val)"
      ],
      "metadata": {
        "id": "C_Q2gTIaZtiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Selector for Test Data during Inference"
      ],
      "metadata": {
        "id": "RPszC6UKKjCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile column_selector.py\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "\n",
        "# Custom transformer to select specific columns\n",
        "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, columns):\n",
        "    self.columns = columns\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    return X[self.columns]"
      ],
      "metadata": {
        "id": "677mCn6_KpFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_file_to_destination(\n",
        "    source_path = 'column_selector.py',\n",
        "    destination_path = PATH / 'column_selector.py'\n",
        ")"
      ],
      "metadata": {
        "id": "lGJBGSj4Ku5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Train and Validation Data"
      ],
      "metadata": {
        "id": "yjZW_0OT-YvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_object(obj=df_train, filename='df_train.pkl')\n",
        "save_object(obj=df_val, filename='df_val.pkl')"
      ],
      "metadata": {
        "id": "H7CeafayFCyV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}