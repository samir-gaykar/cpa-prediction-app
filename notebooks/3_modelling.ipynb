{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"18O8euHpJQHB--nkUWV8a_9TtfiEzAfwo","authorship_tag":"ABX9TyM2DTWZbO63D9FVKp2qmlKk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Installations"],"metadata":{"id":"lnVNyqfz0MvW"}},{"cell_type":"code","source":["%%writefile requirements.txt\n","optuna\n","xgboost\n","lightgbm\n","scikit-learn\n","optuna_fast_fanova\n","mlflow"],"metadata":{"id":"CRMKwB8SBFeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -qU -r requirements.txt"],"metadata":{"id":"8IYO4cA1BLM6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"tBBGG7E6FUky"}},{"cell_type":"code","source":["# Standard Library Imports\n","import gc\n","import json\n","import os\n","import sys\n","from datetime import datetime\n","from pathlib import Path\n","\n","# Third-Party Imports\n","import joblib\n","import lightgbm as lgb\n","import matplotlib.pyplot as plt\n","import mlflow\n","import mlflow.lightgbm\n","import mlflow.sklearn\n","import numpy as np\n","import optuna\n","import pandas as pd\n","import seaborn as sns\n","import shap\n","import statsmodels.api as sm\n","from lightgbm import LGBMRegressor\n","from optuna.importance import get_param_importances\n","from optuna.pruners import HyperbandPruner\n","from optuna.visualization import (\n","    plot_optimization_history,\n","    plot_param_importances\n",")\n","from optuna_fast_fanova import FanovaImportanceEvaluator\n","from scipy.stats import mstats\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.ensemble import (\n","    BaggingRegressor,\n","    GradientBoostingRegressor,\n","    HistGradientBoostingRegressor,\n","    RandomForestRegressor,\n","    StackingRegressor,\n","    VotingRegressor\n",")\n","from sklearn.inspection import PartialDependenceDisplay\n","from sklearn.linear_model import (\n","    ElasticNet,\n","    HuberRegressor,\n","    Lasso,\n","    LassoLarsCV,\n","    LinearRegression,\n","    Ridge\n",")\n","from sklearn.metrics import (\n","    mean_absolute_error,\n","    mean_squared_error,\n","    r2_score\n",")\n","from sklearn.model_selection import (\n","    RepeatedKFold,\n","    learning_curve,\n","    train_test_split\n",")\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import LinearSVR, NuSVR, SVR\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.utils import all_estimators\n","from xgboost import XGBRegressor\n","import yaml\n","\n","# Local Imports\n","from mlflow.tracking import MlflowClient"],"metadata":{"id":"7YXi862t0TcK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Constants"],"metadata":{"id":"W2rE0-TFHJmT"}},{"cell_type":"code","source":["config_yaml_path = os.path.join(\n","    '/content',\n","    'drive',\n","    'MyDrive',\n","    'Colab',\n","    'Machine Learning',\n","    'Regression',\n","    'Radancy',\n","    'config.yaml'\n",")\n","\n","# Load config\n","with open(config_yaml_path, \"r\") as file:\n","  config = yaml.safe_load(file)\n","\n","# Cross-platform path assembly\n","PATH = Path().joinpath(*config[\"PATH_PARTS\"])\n","\n","# Other parameters\n","RANDOM_STATE = config[\"RANDOM_STATE\"]\n","COMPRESS = config[\"COMPRESS\"]\n","N_JOBS = config[\"N_JOBS\"]\n","TARGET = config[\"TARGET\"]"],"metadata":{"id":"cG-itCntG6_a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Add Project Directory to sys.path"],"metadata":{"id":"Ib81oV-vG8aR"}},{"cell_type":"code","source":["# Add the project directory to sys.path so we can import local modules\n","sys.path.append(str(PATH))"],"metadata":{"id":"En7I0vDA2NBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ml_utils import *"],"metadata":{"id":"sLXCzpzQ2EEK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MLflow Tracking URI"],"metadata":{"id":"StZE0CZ7s0QC"}},{"cell_type":"code","source":["mlflow.set_tracking_uri(f\"file://{PATH / 'ml_experiments'}\")"],"metadata":{"id":"kvdP8xGBsyly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"7NwrmgYoILXM"}},{"cell_type":"code","source":["df_train = joblib.load(PATH / 'df_train.pkl')\n","df_val = joblib.load(PATH / 'df_val.pkl')"],"metadata":{"id":"0-bCzmoKO6vJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Models"],"metadata":{"id":"TkFIwUuKN6Sl"}},{"cell_type":"code","source":["# Set MLflow experiment\n","mlflow_experiment = \"baseline_models_2\"\n","mlflow.set_experiment(mlflow_experiment)\n","\n","# Function to get all regression models\n","def get_regression_models():\n","  regressors = []\n","  # Add scikit-learn regressors\n","  for name, RegressorClass in all_estimators(type_filter='regressor'):\n","    try:\n","      # Exclude models that require additional setup or specific parameters\n","      if name in [\n","          'MultiOutputRegressor',\n","          'StackingRegressor',\n","          'VotingRegressor',\n","          'MultiTaskElasticNet',\n","          'MultiTaskElasticNetCV',\n","          'MultiTaskLasso',\n","          'MultiTaskLassoCV'\n","      ]:\n","        continue\n","\n","      regressor = RegressorClass()\n","      regressors.append((name, regressor))\n","    except Exception as e:\n","      print(f\"Could not instantiate {name}: {e}\")\n","\n","  # Add XGBoost and LightGBM regressors\n","  regressors.append(('XGBRegressor', XGBRegressor()))\n","  regressors.append(('LGBMRegressor', LGBMRegressor()))\n","\n","  return regressors\n","\n","# Prepare features and target\n","X_train = df_train.drop(columns=[TARGET])\n","y_train = df_train[TARGET]\n","X_val = df_val.drop(columns=[TARGET])\n","y_val = df_val[TARGET]\n","\n","# Log-transform the target variable\n","# (using log1p to handle zero/negative values safely)\n","y_train_log = np.log1p(y_train)\n","y_val_log = np.log1p(y_val)\n","\n","# Get all regression models\n","models = get_regression_models()\n","\n","# Dictionary to store results\n","results_log = {}\n","\n","# Train and evaluate each model on log-transformed target\n","for name, model in models:\n","  try:\n","    with mlflow.start_run():\n","      # Train the model on log-transformed target\n","      model.fit(X_train, y_train_log)\n","\n","      # Make predictions\n","      y_pred_log = model.predict(X_val)\n","\n","      # Transform predictions back to original scale\n","      y_pred = np.expm1(y_pred_log)\n","\n","      # Calculate metrics on log scale\n","      mse_log = mean_squared_error(y_val_log, y_pred_log)\n","      rmse_log = np.sqrt(mse_log)\n","      mae_log = mean_absolute_error(y_val_log, y_pred_log)\n","      r2_log = r2_score(y_val_log, y_pred_log)\n","\n","      # Calculate metrics on original scale\n","      mse_orig = mean_squared_error(y_val, y_pred)\n","      rmse_orig = np.sqrt(mse_orig)\n","      mae_orig = mean_absolute_error(y_val, y_pred)\n","      r2_orig = r2_score(y_val, y_pred)\n","\n","      # Log model and metrics to MLflow\n","      mlflow.log_param(\"model_name\", name)\n","      mlflow.log_metric(\"RMSE_log\", rmse_log)\n","      mlflow.log_metric(\"MAE_log\", mae_log)\n","      mlflow.log_metric(\"R2_log\", r2_log)\n","      mlflow.log_metric(\"RMSE_orig\", rmse_orig)\n","      mlflow.log_metric(\"MAE_orig\", mae_orig)\n","      mlflow.log_metric(\"R2_orig\", r2_orig)\n","\n","  except Exception as e:\n","    print(f\"Error with {name} (log-transformed): {e}\")"],"metadata":{"id":"rabuTZnNvfES"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelling"],"metadata":{"id":"fcbs2F0Y3mp0"}},{"cell_type":"markdown","source":["## LinearSVR"],"metadata":{"id":"NrthHv4Z3oiC"}},{"cell_type":"code","source":["selected_features = [\n","    'market_id_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'category_id_encoded_scaled',\n","    'market_popularity_transformed_scaled',\n","    'publisher_avg_clicks_transformed_scaled'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"LinearSVR\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.7, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune LinearSVR parameters\n","  C = trial.suggest_float('C', 1e-12, 1e-3, log=True)\n","  epsilon = trial.suggest_float('epsilon', 0.0, 1.0)\n","\n","  loss = trial.suggest_categorical(\n","      'loss',\n","       ['epsilon_insensitive', 'squared_epsilon_insensitive']\n","  )\n","\n","  max_iter = trial.suggest_int('max_iter', 500, 5000)\n","  tol = trial.suggest_float('tol', 1e-5, 1e-2, log=True)\n","  fit_intercept = True  # Fix to True\n","  intercept_scaling = trial.suggest_float('intercept_scaling', 1.0, 10.0)\n","  dual = trial.suggest_categorical('dual', ['auto', True, False])\n","\n","  # Enforce valid dual setting for loss='epsilon_insensitive'\n","  if loss == 'epsilon_insensitive' and dual is False:\n","    dual = True  # Override to a valid value\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx],\n","        X_train_full.iloc[val_idx]\n","    )\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Initialize LinearSVR model\n","    model = LinearSVR(\n","        C=C,\n","        epsilon=epsilon,\n","        loss=loss,\n","        max_iter=max_iter,\n","        tol=tol,\n","        dual=dual,\n","        fit_intercept=fit_intercept,\n","        intercept_scaling=intercept_scaling,\n","        random_state=RANDOM_STATE\n","    )\n","\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      ),\n","      load_if_exists=True\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters and trial attributes\n","  best_params = study.best_params\n","  best_trial = study.best_trial\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Train final model with best LinearSVR parameters\n","  filtered_params = {\n","      k: v\n","      for k, v in best_params.items()\n","      if k in LinearSVR().get_params().keys()\n","  }\n","\n","  # Ensure dual is valid for the best loss parameter\n","  if (\n","      filtered_params['loss'] == 'epsilon_insensitive'\n","      and filtered_params['dual'] is False\n","  ):\n","    filtered_params['dual'] = True\n","\n","  # Added fit_intercept and random_state here as these are constant values\n","  final_model = LinearSVR(\n","      **filtered_params,\n","      fit_intercept = True,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","  mlflow.sklearn.log_model(sk_model=final_model, name=\"LinearSVR\")\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis using LinearExplainer\n","  shap_analysis(final_model, X_final, model_type=\"linear\")"],"metadata":{"id":"Gm2e3No438h-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## HuberRegressor"],"metadata":{"id":"HY7XsnzYwQvW"}},{"cell_type":"code","source":["selected_features = [\n","    'publisher_avg_clicks_transformed_scaled',\n","    'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled' #,\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks',\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"HuberRegressor\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune HuberRegressor parameters\n","  epsilon = trial.suggest_float('epsilon', 1.01, 5.0)\n","  alpha = trial.suggest_float('alpha', 0.1, 10.0, log=True)\n","  max_iter = trial.suggest_int('max_iter', 100, 1000)\n","  tol = trial.suggest_float('tol', 1e-6, 1e-3, log=True)\n","  fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx],\n","        X_train_full.iloc[val_idx]\n","    )\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Initialize HuberRegressor model\n","    model = HuberRegressor(\n","        epsilon=epsilon,\n","        alpha=alpha,\n","        max_iter=max_iter,\n","        tol=tol,\n","        fit_intercept=fit_intercept\n","    )\n","\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      ),\n","      load_if_exists=True\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters and trial attributes\n","  best_params = study.best_params\n","  best_trial = study.best_trial\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Train final model with best HuberRegressor parameters\n","  filtered_params = {\n","      k: v\n","      for k, v in best_params.items()\n","      if k in HuberRegressor().get_params().keys()\n","  }\n","\n","  # Added fit_intercept as a constant value\n","  final_model = HuberRegressor(\n","      **filtered_params,\n","      fit_intercept = True\n","  )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","  mlflow.sklearn.log_model(sk_model=final_model, name=\"HuberRegressor\")\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis using LinearExplainer\n","  shap_analysis(final_model, X_final, model_type=\"linear\")"],"metadata":{"id":"vqsMru9GxHj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BaggingRegressor"],"metadata":{"id":"RaKxf3Jj0O57"}},{"cell_type":"code","source":["selected_features = [\n","    'publisher_avg_clicks_transformed_scaled',\n","    # 'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled',\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks',\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"BaggingRegressor\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune BaggingRegressor parameters\n","  n_estimators = trial.suggest_int('n_estimators', 10, 200)\n","  max_samples = trial.suggest_float('max_samples', 0.1, 1.0)\n","  max_features = trial.suggest_float('max_features', 0.1, 1.0)\n","  bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n","\n","  bootstrap_features = trial.suggest_categorical(\n","      'bootstrap_features', [True, False]\n","  )\n","\n","  oob_score = (\n","      trial.suggest_categorical('oob_score', [True, False])\n","      if bootstrap else False\n","  )\n","\n","  # Select base estimator\n","  estimator_name = trial.suggest_categorical('estimator', [\n","      'DecisionTreeRegressor',\n","      'LinearRegression',\n","      'Ridge',\n","      'KNeighborsRegressor',\n","      'SVR'\n","  ])\n","\n","  # Initialize base estimator with specific hyperparameters\n","  if estimator_name == 'DecisionTreeRegressor':\n","    max_depth = trial.suggest_int('max_depth', 3, 15, log=True)\n","    min_samples_split = trial.suggest_int('min_samples_split', 5, 20)\n","    min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n","    criterion = trial.suggest_categorical(\n","        'criterion',\n","          ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n","    )\n","    splitter = trial.suggest_categorical('splitter', ['best', 'random'])\n","    min_weight_fraction_leaf = trial.suggest_float(\n","        'min_weight_fraction_leaf',\n","        0.01,\n","        0.3\n","    )\n","    max_features_dt = trial.suggest_float('max_features_dt', 0.1, 1.0)\n","    max_leaf_nodes = trial.suggest_categorical(\n","        'max_leaf_nodes',\n","         [None, 10, 50, 100, 500]\n","    )\n","    min_impurity_decrease = trial.suggest_float(\n","        'min_impurity_decrease',\n","        0.0,\n","        0.05\n","    )\n","    ccp_alpha = trial.suggest_float('ccp_alpha', 0.0, 0.05)\n","    base_estimator = DecisionTreeRegressor(\n","        max_depth=max_depth,\n","        min_samples_split=min_samples_split,\n","        min_samples_leaf=min_samples_leaf,\n","        criterion=criterion,\n","        splitter=splitter,\n","        min_weight_fraction_leaf=min_weight_fraction_leaf,\n","        max_features=max_features_dt,\n","        max_leaf_nodes=max_leaf_nodes,\n","        min_impurity_decrease=min_impurity_decrease,\n","        ccp_alpha=ccp_alpha,\n","        # Set to None as no specific constraints provided\n","        monotonic_cst=None,\n","        random_state=RANDOM_STATE\n","    )\n","  elif estimator_name == 'LinearRegression':\n","    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n","    tol = trial.suggest_float('tol', 1e-6, 1e-3, log=True)\n","    base_estimator = LinearRegression(\n","        fit_intercept=fit_intercept,\n","        tol=tol,\n","        n_jobs=N_JOBS\n","    )\n","  elif estimator_name == 'Ridge':\n","    alpha = trial.suggest_float('alpha', 1e-4, 100.0, log=True)\n","    fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n","    tol = trial.suggest_float('tol', 1e-6, 1e-3, log=True)\n","    solver = trial.suggest_categorical(\n","        'solver',\n","         ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n","    )\n","    base_estimator = Ridge(\n","        alpha=alpha,\n","        fit_intercept=fit_intercept,\n","        tol=tol,\n","        solver=solver,\n","        random_state=RANDOM_STATE\n","    )\n","  elif estimator_name == 'KNeighborsRegressor':\n","    n_neighbors = trial.suggest_int('n_neighbors', 5, 30)\n","    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n","    algorithm = trial.suggest_categorical(\n","        'algorithm',\n","         ['auto', 'ball_tree', 'kd_tree', 'brute']\n","    )\n","    leaf_size = trial.suggest_int('leaf_size', 10, 50)\n","    p = trial.suggest_int('p', 1, 2)\n","    metric = trial.suggest_categorical(\n","        'metric',\n","         ['minkowski', 'euclidean', 'manhattan']\n","    )\n","    base_estimator = KNeighborsRegressor(\n","        n_neighbors=n_neighbors,\n","        weights=weights,\n","        algorithm=algorithm,\n","        leaf_size=leaf_size,\n","        p=p,\n","        metric=metric,\n","        metric_params=None,\n","        n_jobs=N_JOBS\n","    )\n","  elif estimator_name == 'SVR':\n","    C = trial.suggest_float('C', 1e-2, 5.0, log=True)\n","    epsilon = trial.suggest_float('epsilon', 1e-2, 0.5, log=True)\n","    kernel = trial.suggest_categorical(\n","        'kernel',\n","         ['rbf', 'linear', 'poly', 'sigmoid']\n","    )\n","    degree = trial.suggest_int('degree', 1, 5)\n","    gamma = trial.suggest_categorical(\n","        'gamma',\n","         ['scale', 'auto'] + [1e-3, 1e-2, 1e-1]\n","    )\n","    coef0 = trial.suggest_float('coef0', 0.0, 1.0)\n","    tol = trial.suggest_float('tol', 1e-5, 1e-2, log=True)\n","    shrinking = trial.suggest_categorical('shrinking', [True, False])\n","    base_estimator = SVR(\n","        C=C,\n","        epsilon=epsilon,\n","        kernel=kernel,\n","        degree=degree,\n","        gamma=gamma,\n","        coef0=coef0,\n","        tol=tol,\n","        shrinking=shrinking\n","    )\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx],\n","        X_train_full.iloc[val_idx]\n","    )\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Initialize BaggingRegressor model\n","    model = BaggingRegressor(\n","        estimator=base_estimator,\n","        n_estimators=n_estimators,\n","        max_samples=max_samples,\n","        max_features=max_features,\n","        bootstrap=bootstrap,\n","        bootstrap_features=bootstrap_features,\n","        oob_score=oob_score,\n","        random_state=RANDOM_STATE,\n","        n_jobs=N_JOBS\n","    )\n","\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      ),\n","      load_if_exists=True\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters and trial attributes\n","  best_params = study.best_params\n","  best_trial = study.best_trial\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Create best base estimator\n","  if best_params['estimator'] == 'DecisionTreeRegressor':\n","    base_estimator = DecisionTreeRegressor(\n","        max_depth=best_params['max_depth'],\n","        min_samples_split=best_params['min_samples_split'],\n","        min_samples_leaf=best_params['min_samples_leaf'],\n","        criterion=best_params['criterion'],\n","        splitter=best_params['splitter'],\n","        min_weight_fraction_leaf=best_params['min_weight_fraction_leaf'],\n","        max_features=best_params['max_features_dt'],\n","        max_leaf_nodes=best_params['max_leaf_nodes'],\n","        min_impurity_decrease=best_params['min_impurity_decrease'],\n","        ccp_alpha=best_params['ccp_alpha'],\n","        monotonic_cst=None,\n","        random_state=RANDOM_STATE\n","    )\n","  elif best_params['estimator'] == 'LinearRegression':\n","    base_estimator = LinearRegression(\n","        fit_intercept=best_params['fit_intercept'],\n","        tol=best_params['tol'],\n","        n_jobs=N_JOBS\n","    )\n","  elif best_params['estimator'] == 'Ridge':\n","    base_estimator = Ridge(\n","        alpha=best_params['alpha'],\n","        fit_intercept=best_params['fit_intercept'],\n","        tol=best_params['tol'],\n","        solver=best_params['solver'],\n","        random_state=RANDOM_STATE\n","    )\n","  elif best_params['estimator'] == 'KNeighborsRegressor':\n","    base_estimator = KNeighborsRegressor(\n","        n_neighbors=best_params['n_neighbors'],\n","        weights=best_params['weights'],\n","        algorithm=best_params['algorithm'],\n","        leaf_size=best_params['leaf_size'],\n","        p=best_params['p'],\n","        metric=best_params['metric'],\n","        metric_params=None,\n","        n_jobs=N_JOBS\n","    )\n","  elif best_params['estimator'] == 'SVR':\n","    base_estimator = SVR(\n","        C=best_params['C'],\n","        epsilon=best_params['epsilon'],\n","        kernel=best_params['kernel'],\n","        degree=best_params['degree'],\n","        gamma=best_params['gamma'],\n","        coef0=best_params['coef0'],\n","        tol=best_params['tol'],\n","        shrinking=best_params['shrinking']\n","    )\n","\n","  # Train final model with best BaggingRegressor parameters\n","  filtered_params = {\n","      k: v\n","      for k, v in best_params.items()\n","      if k in BaggingRegressor().get_params().keys() and k != 'estimator'\n","  }\n","\n","  final_model = BaggingRegressor(\n","      estimator=base_estimator,\n","      **filtered_params,\n","      random_state=RANDOM_STATE,\n","      n_jobs=N_JOBS\n","  )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","  mlflow.sklearn.log_model(sk_model=final_model, name=\"BaggingRegressor\")\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis using KernelExplainer\n","  shap_analysis(final_model, X_final, model_type=\"kernel\")"],"metadata":{"id":"Jg5pktLFXnuD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## HistGradientBoostingRegressor"],"metadata":{"id":"j3aaVDQaR_RI"}},{"cell_type":"code","source":["selected_features = [\n","    # 'publisher_avg_clicks_transformed_scaled',\n","    # 'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    # 'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled' #,\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks',\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"HistGradientBoostingRegressor\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.85, 0.95)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune HistGradientBoostingRegressor parameters\n","  learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)\n","  max_iter = trial.suggest_int('max_iter', 100, 1000)\n","  max_depth = trial.suggest_int('max_depth', 3, 6)\n","  min_samples_leaf = trial.suggest_int('min_samples_leaf', 100, 500)\n","  l2_regularization = trial.suggest_float('l2_regularization', 1.0, 20.0)\n","  max_bins = trial.suggest_int('max_bins', 50, 200)\n","  max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 15, 50)\n","  tol = trial.suggest_float('tol', 1e-7, 1e-4, log=True)\n","  validation_fraction = trial.suggest_float('validation_fraction', 0.2, 0.4)\n","  n_iter_no_change = trial.suggest_int('n_iter_no_change', 5, 100)\n","  loss = trial.suggest_categorical(\n","      'loss',\n","       ['squared_error', 'absolute_error', 'gamma', 'poisson', 'quantile']\n","  )\n","  early_stopping = True\n","  random_state=RANDOM_STATE\n","\n","  # Tune quantile if loss is quantile\n","  quantile = None\n","  if loss == 'quantile':\n","    quantile = trial.suggest_float('quantile', 0.1, 0.9)\n","\n","  # Tune interaction constraints\n","  interaction_cst = trial.suggest_categorical(\n","      'interaction_cst', ['pairwise', 'no_interactions']\n","  )\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Noise Level: Add slight Gaussian noise to training data for\n","    # robustness. The standard deviation of 0.01 is chosen to be small\n","    # enough to avoid significantly altering the data distribution while\n","    # introducing enough variation to improve robustness. You can\n","    # experiment with values like 0.005 or 0.02 if needed, but 0.01 is a\n","    # common starting point for scaled features.\n","    # X_train_noisy = X_train + np.random.normal(0, 0.02, X_train.shape)\n","\n","    # Initialize HistGradientBoostingRegressor model\n","    model = HistGradientBoostingRegressor(\n","        learning_rate=learning_rate,\n","        max_iter=max_iter,\n","        max_depth=max_depth,\n","        min_samples_leaf=min_samples_leaf,\n","        l2_regularization=l2_regularization,\n","        max_bins=max_bins,\n","        max_leaf_nodes=max_leaf_nodes,\n","        tol=tol,\n","        validation_fraction=validation_fraction,\n","        loss=loss,\n","        random_state=random_state,\n","        interaction_cst=interaction_cst,\n","        quantile=quantile if loss == 'quantile' else None,\n","        n_iter_no_change=n_iter_no_change,\n","        early_stopping=early_stopping\n","    )\n","\n","    model.fit(X_train, y_train)\n","    # model.fit(X_train_noisy, y_train)\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n","    # del X_train_noisy, X_val_fold, y_train, y_val_fold, y_pred, model\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      ),\n","      load_if_exists=True\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters and trial attributes\n","  best_params = study.best_params\n","  best_trial = study.best_trial\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Train final model with best HistGradientBoostingRegressor parameters\n","  filtered_params = {\n","      k: v\n","      for k, v in best_params.items()\n","      if k in HistGradientBoostingRegressor().get_params().keys()\n","  }\n","\n","  final_model = HistGradientBoostingRegressor(\n","      **filtered_params,\n","      early_stopping=True,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","\n","  mlflow.sklearn.log_model(\n","      sk_model=final_model, name=\"HistGradientBoostingRegressor\"\n","  )\n","\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis using TreeExplainer\n","  shap_analysis(final_model, X_final, model_type=\"tree\")"],"metadata":{"id":"rm24aiJlR8u_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## NuSVR"],"metadata":{"id":"yH3RG3bP_9sn"}},{"cell_type":"code","source":["selected_features = [\n","    'publisher_avg_clicks_transformed_scaled',\n","    'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled' #,\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks' #,\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"NuSVR\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune NuSVR parameters\n","  C = trial.suggest_float('C', 0.001, 5, log=True)  # Expanded range\n","\n","  kernel = trial.suggest_categorical(\n","      'kernel', ['linear', 'poly', 'rbf', 'sigmoid']\n","  )\n","\n","  nu = trial.suggest_float('nu', 0.05, 1.0)  # Lowered minimum\n","  shrinking = trial.suggest_categorical('shrinking', [True, False])\n","  tol = trial.suggest_float('tol', 1e-6, 1e-1, log=True)\n","\n","  # Parameters specific to certain kernels\n","  degree = trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3\n","\n","  coef0 = (\n","      trial.suggest_float('coef0', 0.0, 1.0)\n","      if kernel in ['poly', 'sigmoid'] else 0.0\n","  )\n","\n","  if kernel in ['rbf', 'poly', 'sigmoid']:\n","    gamma_choice = trial.suggest_categorical(\n","        'gamma_choice',\n","         ['scale', 'auto', 'float']\n","    )\n","    if gamma_choice == 'float':\n","      gamma = trial.suggest_float('gamma_float', 1e-4, 1e-1, log=True)\n","    else:\n","      gamma = gamma_choice\n","  else:\n","    gamma = 'scale'\n","\n","  # Tune Gaussian noise standard deviation\n","  noise_std = trial.suggest_float('noise_std', 0.01, 0.02, log=True)\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx],\n","        X_train_full.iloc[val_idx]\n","    )\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Add Gaussian noise to training data for robustness\n","    X_train_noisy = X_train + np.random.normal(0, noise_std, X_train.shape)\n","\n","    # Initialize NuSVR model\n","    model = NuSVR(\n","        C=C,\n","        kernel=kernel,\n","        nu=nu,\n","        # shrinking=shrinking,\n","        tol=tol,\n","        degree=degree,\n","        coef0=coef0,\n","        gamma=gamma\n","    )\n","\n","    # model.fit(X_train, y_train)\n","    model.fit(X_train_noisy, y_train)\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    # del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model, X_train_noisy\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      ),\n","      load_if_exists=True\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters\n","  best_params = study.best_params\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Train final model with best NuSVR parameters\n","  filtered_params = {\n","      k: v\n","      for k, v in best_params.items()\n","      if k in NuSVR().get_params().keys()\n","  }\n","\n","  final_model = NuSVR(**filtered_params)\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","\n","  mlflow.sklearn.log_model(sk_model=final_model, name=\"NuSVR\")\n","\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis using KernelExplainer\n","  shap_analysis(final_model, X_final, model_type=\"kernel\")"],"metadata":{"id":"dgA6tFgA__FX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## XGBoostRegressor"],"metadata":{"id":"1XCVXH9bMMvb"}},{"cell_type":"code","source":["selected_features = [\n","    'publisher_avg_clicks_transformed_scaled',\n","    # 'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled' #,\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks',\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"XGBoostRegressor\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune XGBoost parameters\n","  tree_method = trial.suggest_categorical(\n","      'tree_method', ['auto', 'exact', 'approx', 'hist']\n","  )\n","\n","  params = {\n","      'base_score': trial.suggest_float('base_score', 0.3, 0.7),\n","      'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n","      'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 0.9),\n","      'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n","      'early_stopping_rounds': trial.suggest_int(\n","          'early_stopping_rounds', 20, 60\n","      ),\n","      'gamma': trial.suggest_float('gamma', 0.1, 3),\n","      'grow_policy': trial.suggest_categorical(\n","          'grow_policy', ['depthwise', 'lossguide']\n","      ),\n","      'importance_type': trial.suggest_categorical(\n","          'importance_type', ['gain', 'weight', 'cover']\n","      ),\n","      'learning_rate': trial.suggest_float(\n","          'learning_rate', 0.01, 0.2, log=True\n","      ),\n","      'max_bin': trial.suggest_int('max_bin', 256, 768, step=128),\n","      'max_delta_step': trial.suggest_float('max_delta_step', 0, 5),\n","      'max_depth': trial.suggest_int('max_depth', 3, 8),\n","      'max_leaves': trial.suggest_int('max_leaves', 0, 32),\n","      'min_child_weight': trial.suggest_float('min_child_weight', 1, 8),\n","      'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n","      'num_parallel_tree': trial.suggest_int('num_parallel_tree', 1, 5),\n","      'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5),\n","      'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5),\n","      'sampling_method': trial.suggest_categorical(\n","          'sampling_method', ['uniform', 'gradient_based']\n","      ),\n","      'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.5, 2.0),\n","      'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n","      'tree_method': tree_method,\n","      'objective': 'reg:squarederror',\n","      'eval_metric': 'rmse',\n","      'n_jobs': N_JOBS,\n","      'random_state': RANDOM_STATE,\n","      'validate_parameters': True\n","  }\n","\n","  # Only include colsample_bynode if tree_method is not 'exact'\n","  if tree_method != 'exact':\n","    params['colsample_bynode'] = trial.suggest_float(\n","        'colsample_bynode', 0.5, 1.0\n","    )\n","\n","  # Tune Gaussian noise standard deviation\n","  noise_std = trial.suggest_float('noise_std', 0.001, 0.015, log=True)\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx],\n","        X_train_full.iloc[val_idx]\n","    )\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Add Gaussian noise to training data\n","    X_train_noisy = X_train + np.random.normal(0, noise_std, X_train.shape)\n","\n","    # Initialize XGBRegressor model\n","    model = XGBRegressor(**params)\n","\n","    # Fit with early stopping\n","    model.fit(\n","        X_train_noisy, y_train,\n","        # X_train, y_train,\n","        eval_set=[(X_val_fold, y_val_fold)],\n","        verbose=False\n","    )\n","\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model, X_train_noisy\n","    # del X_train, X_val_fold, y_train, y_val_fold, y_pred, model\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      ),\n","      load_if_exists=True\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters\n","  best_params = study.best_params\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Train final model with best XGBoost parameters\n","  filtered_params = {\n","      k: v\n","      for k, v in best_params.items()\n","      if k in XGBRegressor().get_params().keys()\n","  }\n","\n","  final_model = XGBRegressor(\n","      **filtered_params,\n","      objective='reg:squarederror',\n","      eval_metric='rmse',\n","      n_jobs=N_JOBS,\n","      random_state=RANDOM_STATE,\n","      sampling_method='uniform',\n","      validate_parameters=True\n","  )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","\n","  final_model.fit(\n","      X_final, y_train_full,\n","      eval_set=[(X_final, y_train_full)],\n","      verbose=False\n","  )\n","\n","  mlflow.xgboost.log_model(xgb_model=final_model, name=\"XGBoostRegressor\")\n","\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis using TreeExplainer (more efficient for tree-based models)\n","  shap_analysis(final_model, X_final, model_type=\"tree\")"],"metadata":{"id":"QLu7W__qMXiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LGBMRegressor"],"metadata":{"id":"Lq6N6ebCMP67"}},{"cell_type":"code","source":["selected_features = [\n","    'publisher_avg_clicks_transformed_scaled',\n","    'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled',\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks',\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"LightGBMRegressor\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.95, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune LightGBM parameters\n","  boosting_type = trial.suggest_categorical(\n","      'boosting_type',\n","       ['gbdt', 'dart', 'goss']\n","  )\n","\n","  params = {\n","      'boosting_type': boosting_type,\n","      'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n","      'importance_type': trial.suggest_categorical(\n","          'importance_type',\n","           ['split', 'gain']\n","      ),\n","      'learning_rate': trial.suggest_float(\n","          'learning_rate',\n","          0.005,\n","          0.1,\n","          log=True\n","      ),\n","      'max_depth': trial.suggest_int('max_depth', 3, 6),\n","      'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n","      'min_child_weight': trial.suggest_float(\n","          'min_child_weight',\n","          1e-3,\n","          5,\n","          log=True\n","      ),\n","      'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.2),\n","      'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n","      'n_jobs': N_JOBS,\n","      'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n","      'objective': 'regression',\n","      'random_state': RANDOM_STATE,\n","      'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0),\n","      'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),\n","      'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n","      'subsample_for_bin': trial.suggest_int(\n","          'subsample_for_bin',\n","          50000,\n","          150000,\n","          step=50000\n","      ),\n","      'subsample_freq': trial.suggest_int('subsample_freq', 1, 5),\n","      'verbose': -1  # Suppress log\n","  }\n","\n","  # Tune Gaussian noise standard deviation\n","  noise_std = trial.suggest_float('noise_std', 0.001, 0.02, log=True)\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n","    )\n","\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Add Gaussian noise to training data\n","    X_train_noisy = X_train + np.random.normal(0, noise_std, X_train.shape)\n","\n","    # Initialize LightGBM model\n","    model = LGBMRegressor(**params)\n","\n","    # Fit model\n","    model.fit(\n","        X_train_noisy, y_train,\n","        eval_set=[(X_val_fold, y_val_fold)],\n","        eval_metric='rmse',\n","        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n","    )\n","\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model, X_train_noisy\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      ),\n","      load_if_exists=True\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances: {fanova_importances}')\n","\n","  # Get best parameters\n","  best_params = study.best_params\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Train final model with best LightGBM parameters\n","  filtered_params = {\n","      k: v for k, v in best_params.items()\n","      if k in LGBMRegressor().get_params().keys()\n","  }\n","\n","  final_model = LGBMRegressor(\n","      **filtered_params,\n","      objective='regression',\n","      metric='rmse',\n","      n_jobs=N_JOBS,\n","      random_state=RANDOM_STATE,\n","      verbose=-1\n","  )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","\n","  final_model.fit(\n","      X_final, y_train_full,\n","      eval_set=[(X_final, y_train_full)],\n","      eval_metric='rmse',\n","      callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n","  )\n","\n","  mlflow.lightgbm.log_model(lgb_model=final_model, name=\"LightGBMRegressor\")\n","\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis using TreeExplainer\n","  shap_analysis(final_model, X_final, model_type=\"tree\")"],"metadata":{"id":"BClKWYxEMX0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## VotingRegressor"],"metadata":{"id":"abGaCl1JlxH0"}},{"cell_type":"code","source":["# Initialize estimators with type-converted parameters\n","best_params_linear_svr = get_best_params(\"LinearSVR\")\n","\n","# Ensure valid parameter combination for LinearSVR\n","loss = best_params_linear_svr['loss']\n","dual = convert_param(best_params_linear_svr['dual'], 'bool', 'dual')\n","\n","if loss == 'epsilon_insensitive' and not dual:\n","  dual = True  # Force dual=True for epsilon_insensitive to avoid ValueError\n","\n","linear_svr = LinearSVR(\n","    C=convert_param(best_params_linear_svr['C'], 'float', 'C'),\n","    epsilon=convert_param(\n","        best_params_linear_svr['epsilon'], 'float', 'epsilon'\n","    ),\n","    loss=loss,\n","    max_iter=convert_param(\n","        best_params_linear_svr['max_iter'], 'int', 'max_iter'\n","    ),\n","    tol=convert_param(best_params_linear_svr['tol'], 'float', 'tol'),\n","    dual=dual,\n","    intercept_scaling=convert_param(\n","        best_params_linear_svr['intercept_scaling'],\n","        'float',\n","        'intercept_scaling'\n","    ),\n","    random_state=RANDOM_STATE\n",")\n","\n","best_params_huber_regressor = get_best_params(\"HuberRegressor\")\n","\n","huber_regressor = HuberRegressor(\n","    epsilon=convert_param(\n","        best_params_huber_regressor['epsilon'],\n","        'float',\n","        'epsilon'\n","    ),\n","    alpha=convert_param(best_params_huber_regressor['alpha'], 'float', 'alpha'),\n","    max_iter=convert_param(\n","        best_params_huber_regressor['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    tol=convert_param(best_params_huber_regressor['tol'], 'float', 'tol')\n",")\n","\n","best_params_bagging_regressor = get_best_params(\"BaggingRegressor\")\n","\n","bagging_regressor = BaggingRegressor(\n","    estimator=LinearRegression(),\n","    n_estimators=convert_param(\n","        best_params_bagging_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    max_samples=convert_param(\n","        best_params_bagging_regressor['max_samples'],\n","        'float',\n","        'max_samples'\n","    ),\n","    max_features=convert_param(\n","        best_params_bagging_regressor['max_features'],\n","        'float',\n","        'max_features'\n","    ),\n","    bootstrap=convert_param(\n","        best_params_bagging_regressor['bootstrap'],\n","        'bool',\n","        'bootstrap'\n","    ),\n","    bootstrap_features=convert_param(\n","        best_params_bagging_regressor['bootstrap_features'],\n","        'bool',\n","        'bootstrap_features'\n","    ),\n","    random_state=RANDOM_STATE,\n","    n_jobs=N_JOBS\n",")\n","\n","best_params_hist_gradient_boosting_regressor = (\n","    get_best_params(\"HistGradientBoostingRegressor\")\n",")\n","\n","hist_gradient_boosting_regressor = HistGradientBoostingRegressor(\n","    learning_rate=convert_param(\n","        best_params_hist_gradient_boosting_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_iter=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    max_depth=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    min_samples_leaf=convert_param(\n","        best_params_hist_gradient_boosting_regressor['min_samples_leaf'],\n","        'int',\n","        'min_samples_leaf'\n","    ),\n","    l2_regularization=convert_param(\n","        best_params_hist_gradient_boosting_regressor['l2_regularization'],\n","        'float',\n","        'l2_regularization'\n","    ),\n","    max_bins=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_bins'],\n","        'int',\n","        'max_bins'\n","    ),\n","    max_leaf_nodes=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_leaf_nodes'],\n","        'int',\n","        'max_leaf_nodes'\n","    ),\n","    tol=convert_param(\n","        best_params_hist_gradient_boosting_regressor['tol'], 'float', 'tol'\n","    ),\n","    validation_fraction=convert_param(\n","        best_params_hist_gradient_boosting_regressor['validation_fraction'],\n","        'float',\n","        'validation_fraction'\n","    ),\n","    loss=best_params_hist_gradient_boosting_regressor['loss'],  # Categorical\n","    random_state=RANDOM_STATE,\n","    interaction_cst=(\n","        best_params_hist_gradient_boosting_regressor['interaction_cst']\n","    ),\n","    n_iter_no_change=convert_param(\n","        best_params_hist_gradient_boosting_regressor['n_iter_no_change'],\n","        'int',\n","        'n_iter_no_change'\n","    )\n",")\n","\n","best_params_nu_svr = get_best_params(\"NuSVR\")\n","\n","nu_svr = NuSVR(\n","    C=convert_param(best_params_nu_svr['C'], 'float', 'C'),\n","    kernel=best_params_nu_svr['kernel'],\n","    nu=convert_param(best_params_nu_svr['nu'], 'float', 'nu'),\n","    shrinking=convert_param(\n","        best_params_nu_svr['shrinking'],'bool',\n","        'shrinking'\n","    ),\n","    tol=convert_param(best_params_nu_svr['tol'], 'float', 'tol')\n",")\n","\n","best_params_xgb_regressor = get_best_params(\"XGBoostRegressor\")\n","\n","# Remove early_stopping_rounds from best_params_xgb_regressor to avoid conflicts\n","best_params_xgb_regressor.pop('early_stopping_rounds', None)\n","\n","xgb_regressor = XGBRegressor(\n","    base_score=convert_param(\n","        best_params_xgb_regressor['base_score'],\n","        'float',\n","        'base_score'\n","    ),\n","    booster=best_params_xgb_regressor['booster'],\n","    colsample_bylevel=convert_param(\n","        best_params_xgb_regressor['colsample_bylevel'],\n","        'float',\n","        'colsample_bylevel'\n","    ),\n","    colsample_bytree=convert_param(\n","        best_params_xgb_regressor['colsample_bytree'],\n","        'float',\n","        'colsample_bytree'\n","    ),\n","    gamma=convert_param(best_params_xgb_regressor['gamma'], 'float', 'gamma'),\n","    grow_policy=best_params_xgb_regressor['grow_policy'],\n","    importance_type=best_params_xgb_regressor['importance_type'],\n","    learning_rate=convert_param(\n","        best_params_xgb_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_bin=convert_param(\n","        best_params_xgb_regressor['max_bin'],\n","        'int',\n","        'max_bin'\n","    ),\n","    max_delta_step=convert_param(\n","        best_params_xgb_regressor['max_delta_step'],\n","        'float',\n","        'max_delta_step'\n","    ),\n","    max_depth=convert_param(\n","        best_params_xgb_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    max_leaves=convert_param(\n","        best_params_xgb_regressor['max_leaves'],\n","        'int',\n","        'max_leaves'\n","    ),\n","    min_child_weight=convert_param(\n","        best_params_xgb_regressor['min_child_weight'],\n","        'float',\n","        'min_child_weight'\n","    ),\n","    n_estimators=convert_param(\n","        best_params_xgb_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    num_parallel_tree=convert_param(\n","        best_params_xgb_regressor['num_parallel_tree'],\n","        'int',\n","        'num_parallel_tree'\n","    ),\n","    reg_alpha=convert_param(\n","        best_params_xgb_regressor['reg_alpha'],\n","        'float',\n","        'reg_alpha'\n","    ),\n","    reg_lambda=convert_param(\n","        best_params_xgb_regressor['reg_lambda'],\n","        'float',\n","        'reg_lambda'\n","    ),\n","    subsample=convert_param(\n","        best_params_xgb_regressor['subsample'],\n","        'float',\n","        'subsample'\n","    ),\n","    tree_method=best_params_xgb_regressor['tree_method'],\n","    objective='reg:squarederror',\n","    eval_metric='rmse',\n","    n_jobs=N_JOBS,\n","    random_state=RANDOM_STATE,\n","    sampling_method='uniform'\n",")\n","\n","best_params_lgbm_regressor = get_best_params(\"LightGBMRegressor\")\n","\n","lgbm_regressor = LGBMRegressor(\n","    boosting_type=best_params_lgbm_regressor['boosting_type'],\n","    colsample_bytree=convert_param(\n","        best_params_lgbm_regressor['colsample_bytree'],\n","        'float',\n","        'colsample_bytree'\n","    ),\n","    importance_type=best_params_lgbm_regressor['importance_type'],\n","    learning_rate=convert_param(\n","        best_params_lgbm_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_depth=convert_param(\n","        best_params_lgbm_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    min_child_samples=convert_param(\n","        best_params_lgbm_regressor['min_child_samples'],\n","        'int',\n","        'min_child_samples'\n","    ),\n","    min_child_weight=convert_param(\n","        best_params_lgbm_regressor['min_child_weight'],\n","        'float',\n","        'min_child_weight'\n","    ),\n","    min_split_gain=convert_param(\n","        best_params_lgbm_regressor['min_split_gain'],\n","        'float',\n","        'min_split_gain'\n","    ),\n","    n_estimators=convert_param(\n","        best_params_lgbm_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    num_leaves=convert_param(\n","        best_params_lgbm_regressor['num_leaves'],\n","        'int',\n","        'num_leaves'\n","    ),\n","    reg_alpha=convert_param(\n","        best_params_lgbm_regressor['reg_alpha'],\n","        'float',\n","        'reg_alpha'\n","    ),\n","    reg_lambda=convert_param(\n","        best_params_lgbm_regressor['reg_lambda'],\n","        'float',\n","        'reg_lambda'\n","    ),\n","    subsample=convert_param(\n","        best_params_lgbm_regressor['subsample'],\n","        'float',\n","        'subsample'\n","    ),\n","    subsample_for_bin=convert_param(\n","        best_params_lgbm_regressor['subsample_for_bin'],\n","        'int',\n","        'subsample_for_bin'\n","    ),\n","    subsample_freq=convert_param(\n","        best_params_lgbm_regressor['subsample_freq'],\n","        'int',\n","        'subsample_freq'\n","    ),\n","    n_jobs=N_JOBS,\n","    random_state=RANDOM_STATE,\n","    verbose=-1,\n","    objective='regression',\n","    metric='rmse'\n",")\n","\n","selected_features = [\n","    'publisher_avg_clicks_transformed_scaled',\n","    'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled',\n","    'is_weekend_encoded_scaled',\n","    'pub_popularity_x_pub_clicks',\n","    'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"VotingRegressor\"\n","study_name = f\"{mlflow_experiment}_study\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","# Define base estimators\n","estimators = [\n","    ('linear_svr', linear_svr),\n","    ('huber_regressor', huber_regressor),\n","    ('bagging_regressor', bagging_regressor),\n","    ('hist_gradient_boosting', hist_gradient_boosting_regressor),\n","    ('nu_svr', nu_svr),\n","    ('xgb_regressor', xgb_regressor),\n","    ('lgbm_regressor', lgbm_regressor)\n","]\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune weights for VotingRegressor using estimator names\n","  weights = []\n","\n","  for name, _ in estimators:\n","    weights.append(trial.suggest_float(f'weight_{name}', 0.0, 1.0))\n","\n","  weights = np.array(weights)\n","  weights = weights / weights.sum()  # Normalize weights to sum to 1\n","\n","  # Tune Gaussian noise standard deviation\n","  noise_std = trial.suggest_float('noise_std', 0.001, 0.02, log=True)\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n","    )\n","\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Add Gaussian noise to training data and preserve feature names\n","    noise = np.random.normal(0, noise_std, X_train.shape)\n","\n","    X_train_noisy = pd.DataFrame(\n","        X_train + noise, columns=X_train.columns, index=X_train.index\n","    )\n","\n","    # Initialize VotingRegressor with estimators\n","    model = VotingRegressor(\n","        estimators=estimators, weights=weights, n_jobs=N_JOBS\n","    )\n","\n","    # Fit VotingRegressor\n","    model.fit(X_train_noisy, y_train)\n","\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del (\n","        X_train,\n","        X_val_fold,\n","        y_train,\n","        y_val_fold,\n","        y_pred, model,\n","        X_train_noisy,\n","        noise\n","    )\n","\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      load_if_exists=True,\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      )\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters\n","  best_params = study.best_params\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Prepare weights for final model using estimator names\n","  weights = np.array(\n","      [best_params.get(f'weight_{name}', 0.0) for name, _ in estimators]\n","  )\n","  weights = weights / weights.sum()  # Normalize weights\n","\n","  # Train final VotingRegressor\n","  final_model = VotingRegressor(\n","      estimators=estimators, weights=weights, n_jobs=N_JOBS\n","  )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","\n","  mlflow.sklearn.log_model(final_model, name=\"VotingRegressor\")\n","\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis\n","  shap_analysis(final_model, X_final, model_type=\"kernel\")"],"metadata":{"id":"vx8w9f-Uxdoj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## StackingRegressor - Baseline"],"metadata":{"id":"64cFgOQNcHww"}},{"cell_type":"code","source":["# Define excluded estimators globally\n","excluded_estimators = [\n","    'MultiOutputRegressor',\n","    'StackingRegressor',\n","    'VotingRegressor',\n","    'MultiTaskElasticNet',\n","    'MultiTaskElasticNetCV',\n","    'MultiTaskLasso',\n","    'MultiTaskLassoCV',\n","    'GammaRegressor',\n","    'PoissonRegressor',\n","    'TweedieRegressor',\n","    'PLSRegression',\n","    'RegressorChain'\n","]\n","\n","# Function to get all regression models\n","def get_regression_models():\n","  regressors = []\n","\n","  # Add scikit-learn regressors\n","  for name, RegressorClass in all_estimators(type_filter='regressor'):\n","    try:\n","      # Exclude models that require additional setup or specific parameters\n","      if name in excluded_estimators:\n","        continue\n","\n","      regressor = RegressorClass()\n","      regressors.append((name, regressor))\n","    except Exception as e:\n","      print(f\"Could not instantiate {name}: {e}\")\n","\n","  # Add XGBoost and LightGBM regressors\n","  regressors.append(('XGBRegressor', XGBRegressor()))\n","  regressors.append(('LGBMRegressor', LGBMRegressor()))\n","  return regressors\n","\n","# Initialize estimators with type-converted parameters\n","best_params_linear_svr = get_best_params(\"LinearSVR\")\n","loss = best_params_linear_svr['loss']\n","dual = convert_param(best_params_linear_svr['dual'], 'bool', 'dual')\n","\n","if loss == 'epsilon_insensitive' and not dual:\n","  dual = True  # Force dual=True for epsilon_insensitive to avoid ValueError\n","\n","linear_svr = LinearSVR(\n","    C=convert_param(best_params_linear_svr['C'], 'float', 'C'),\n","    epsilon=convert_param(\n","        best_params_linear_svr['epsilon'],\n","        'float',\n","        'epsilon'\n","    ),\n","    loss=loss,\n","    max_iter=convert_param(\n","        best_params_linear_svr['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    tol=convert_param(best_params_linear_svr['tol'], 'float', 'tol'),\n","    dual=dual,\n","    intercept_scaling=convert_param(\n","        best_params_linear_svr['intercept_scaling'],\n","        'float',\n","        'intercept_scaling'\n","    ),\n","    random_state=RANDOM_STATE\n",")\n","\n","best_params_huber_regressor = get_best_params(\"HuberRegressor\")\n","\n","huber_regressor = HuberRegressor(\n","    epsilon=convert_param(\n","        best_params_huber_regressor['epsilon'],\n","        'float',\n","        'epsilon'\n","    ),\n","    alpha=convert_param(best_params_huber_regressor['alpha'], 'float', 'alpha'),\n","    max_iter=convert_param(\n","        best_params_huber_regressor['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    tol=convert_param(best_params_huber_regressor['tol'], 'float', 'tol')\n",")\n","\n","best_params_bagging_regressor = get_best_params(\"BaggingRegressor\")\n","\n","bagging_regressor = BaggingRegressor(\n","    estimator=HuberRegressor(),\n","    n_estimators=convert_param(\n","        best_params_bagging_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    max_samples=convert_param(\n","        best_params_bagging_regressor['max_samples'],\n","        'float',\n","        'max_samples'\n","    ),\n","    max_features=convert_param(\n","        best_params_bagging_regressor['max_features'],\n","        'float',\n","        'max_features'\n","    ),\n","    bootstrap=convert_param(\n","        best_params_bagging_regressor['bootstrap'],\n","        'bool',\n","        'bootstrap'\n","    ),\n","    bootstrap_features=convert_param(\n","        best_params_bagging_regressor['bootstrap_features'],\n","        'bool',\n","        'bootstrap_features'\n","    ),\n","    random_state=RANDOM_STATE,\n","    n_jobs=N_JOBS\n",")\n","\n","best_params_hist_gradient_boosting_regressor = get_best_params(\n","    \"HistGradientBoostingRegressor\"\n",")\n","\n","hist_gradient_boosting_regressor = HistGradientBoostingRegressor(\n","    learning_rate=convert_param(\n","        best_params_hist_gradient_boosting_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_iter=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    max_depth=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    min_samples_leaf=convert_param(\n","        best_params_hist_gradient_boosting_regressor['min_samples_leaf'],\n","        'int',\n","        'min_samples_leaf'\n","    ),\n","    l2_regularization=convert_param(\n","        best_params_hist_gradient_boosting_regressor['l2_regularization'],\n","        'float',\n","        'l2_regularization'\n","    ),\n","    max_bins=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_bins'],\n","        'int',\n","        'max_bins'\n","    ),\n","    max_leaf_nodes=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_leaf_nodes'],\n","        'int',\n","        'max_leaf_nodes'\n","    ),\n","    tol=convert_param(\n","        best_params_hist_gradient_boosting_regressor['tol'], 'float', 'tol'\n","    ),\n","    validation_fraction=convert_param(\n","        best_params_hist_gradient_boosting_regressor['validation_fraction'],\n","        'float',\n","        'validation_fraction'\n","    ),\n","    loss=best_params_hist_gradient_boosting_regressor['loss'],\n","    random_state=RANDOM_STATE,\n","    interaction_cst=best_params_hist_gradient_boosting_regressor[\n","        'interaction_cst'\n","    ],\n","    n_iter_no_change=convert_param(\n","        best_params_hist_gradient_boosting_regressor['n_iter_no_change'],\n","        'int',\n","        'n_iter_no_change'\n","    )\n",")\n","\n","best_params_nu_svr = get_best_params(\"NuSVR\")\n","\n","nu_svr = NuSVR(\n","    C=convert_param(best_params_nu_svr['C'], 'float', 'C'),\n","    kernel=best_params_nu_svr['kernel'],\n","    nu=convert_param(best_params_nu_svr['nu'], 'float', 'nu'),\n","    shrinking=convert_param(\n","        best_params_nu_svr['shrinking'], 'bool', 'shrinking'\n","    ),\n","    tol=convert_param(best_params_nu_svr['tol'], 'float', 'tol')\n",")\n","\n","best_params_xgb_regressor = get_best_params(\"XGBoostRegressor\")\n","best_params_xgb_regressor.pop('early_stopping_rounds', None)\n","\n","xgb_regressor = XGBRegressor(\n","    base_score=convert_param(\n","        best_params_xgb_regressor['base_score'],\n","        'float',\n","        'base_score'\n","    ),\n","    booster=best_params_xgb_regressor['booster'],\n","    colsample_bylevel=convert_param(\n","        best_params_xgb_regressor['colsample_bylevel'],\n","        'float',\n","        'colsample_bylevel'\n","    ),\n","    colsample_bytree=convert_param(\n","        best_params_xgb_regressor['colsample_bytree'],\n","        'float',\n","        'colsample_bytree'\n","    ),\n","    gamma=convert_param(best_params_xgb_regressor['gamma'], 'float', 'gamma'),\n","    grow_policy=best_params_xgb_regressor['grow_policy'],\n","    importance_type=best_params_xgb_regressor['importance_type'],\n","    learning_rate=convert_param(\n","        best_params_xgb_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_bin=convert_param(\n","        best_params_xgb_regressor['max_bin'],\n","        'int',\n","        'max_bin'\n","    ),\n","    max_delta_step=convert_param(\n","        best_params_xgb_regressor['max_delta_step'],\n","        'float',\n","        'max_delta_step'\n","    ),\n","    max_depth=convert_param(\n","        best_params_xgb_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    max_leaves=convert_param(\n","        best_params_xgb_regressor['max_leaves'],\n","        'int',\n","        'max_leaves'\n","    ),\n","    min_child_weight=convert_param(\n","        best_params_xgb_regressor['min_child_weight'],\n","        'float',\n","        'min_child_weight'\n","    ),\n","    n_estimators=convert_param(\n","        best_params_xgb_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    num_parallel_tree=convert_param(\n","        best_params_xgb_regressor['num_parallel_tree'],\n","        'int',\n","        'num_parallel_tree'\n","    ),\n","    reg_alpha=convert_param(\n","        best_params_xgb_regressor['reg_alpha'],\n","        'float',\n","        'reg_alpha'\n","    ),\n","    reg_lambda=convert_param(\n","        best_params_xgb_regressor['reg_lambda'],\n","        'float',\n","        'reg_lambda'\n","    ),\n","    subsample=convert_param(\n","        best_params_xgb_regressor['subsample'],\n","        'float',\n","        'subsample'\n","    ),\n","    tree_method=best_params_xgb_regressor['tree_method'],\n","    objective='reg:squarederror',\n","    eval_metric='rmse',\n","    n_jobs=N_JOBS,\n","    random_state=RANDOM_STATE,\n","    sampling_method='uniform'\n",")\n","\n","best_params_lgbm_regressor = get_best_params(\"LightGBMRegressor\")\n","\n","lgbm_regressor = LGBMRegressor(\n","    boosting_type=best_params_lgbm_regressor['boosting_type'],\n","    colsample_bytree=convert_param(\n","        best_params_lgbm_regressor['colsample_bytree'],\n","        'float',\n","        'colsample_bytree'\n","    ),\n","    importance_type=best_params_lgbm_regressor['importance_type'],\n","    learning_rate=convert_param(\n","        best_params_lgbm_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_depth=convert_param(\n","        best_params_lgbm_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    min_child_samples=convert_param(\n","        best_params_lgbm_regressor['min_child_samples'],\n","        'int',\n","        'min_child_samples'\n","    ),\n","    min_child_weight=convert_param(\n","        best_params_lgbm_regressor['min_child_weight'],\n","        'float',\n","        'min_child_weight'\n","    ),\n","    min_split_gain=convert_param(\n","        best_params_lgbm_regressor['min_split_gain'],\n","        'float',\n","        'min_split_gain'\n","    ),\n","    n_estimators=convert_param(\n","        best_params_lgbm_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    num_leaves=convert_param(\n","        best_params_lgbm_regressor['num_leaves'],\n","        'int',\n","        'num_leaves'\n","    ),\n","    reg_alpha=convert_param(\n","        best_params_lgbm_regressor['reg_alpha'],\n","        'float',\n","        'reg_alpha'\n","    ),\n","    reg_lambda=convert_param(\n","        best_params_lgbm_regressor['reg_lambda'],\n","        'float',\n","        'reg_lambda'\n","    ),\n","    subsample=convert_param(\n","        best_params_lgbm_regressor['subsample'],\n","        'float',\n","        'subsample'\n","    ),\n","    subsample_for_bin=convert_param(\n","        best_params_lgbm_regressor['subsample_for_bin'],\n","        'int',\n","        'subsample_for_bin'\n","    ),\n","    subsample_freq=convert_param(\n","        best_params_lgbm_regressor['subsample_freq'],\n","        'int',\n","        'subsample_freq'\n","    ),\n","    n_jobs=N_JOBS,\n","    random_state=RANDOM_STATE,\n","    verbose=-1,\n","    objective='regression',\n","    metric='rmse'\n",")\n","\n","selected_features = [\n","    'publisher_avg_clicks_transformed_scaled',\n","    'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    # 'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled' #,\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks',\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"StackingRegressor\"\n","current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","study_name = f\"{mlflow_experiment}_{current_time}\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","# Define base estimators\n","estimators = [\n","    ('linear_svr', linear_svr),\n","    ('huber_regressor', huber_regressor),\n","    ('bagging_regressor', bagging_regressor),\n","    ('hist_gradient_boosting', hist_gradient_boosting_regressor),\n","    ('nu_svr', nu_svr),\n","    ('xgb_regressor', xgb_regressor),\n","    ('lgbm_regressor', lgbm_regressor)\n","]\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.9, 1)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune whether to add noise\n","  add_noise = trial.suggest_categorical('add_noise', [True, False])\n","  noise_std = 0.0\n","\n","  if add_noise:\n","    noise_std = trial.suggest_float('noise_std', 0.001, 0.02, log=True)\n","\n","  # Select final estimator from regression models\n","  regressor_names = [name for name, _ in get_regression_models()]\n","\n","  final_estimator_name = trial.suggest_categorical(\n","      'final_estimator',\n","      regressor_names\n","  )\n","\n","  # Explicitly exclude problematic estimators\n","  if final_estimator_name in excluded_estimators:\n","    raise optuna.TrialPruned(\n","        f\"Final estimator {final_estimator_name} is excluded.\"\n","    )\n","\n","  # Initialize final estimator with default parameters\n","  if final_estimator_name == 'LinearRegression':\n","    final_estimator = LinearRegression()\n","  elif final_estimator_name == 'Ridge':\n","    final_estimator = Ridge(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'Lasso':\n","    final_estimator = Lasso(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'ElasticNet':\n","    final_estimator = ElasticNet(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'SVR':\n","    final_estimator = SVR()\n","  elif final_estimator_name == 'LinearSVR':\n","    final_estimator = LinearSVR(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'HuberRegressor':\n","    final_estimator = HuberRegressor()\n","  elif final_estimator_name == 'RandomForestRegressor':\n","    final_estimator = RandomForestRegressor(\n","        random_state=RANDOM_STATE, n_jobs=N_JOBS\n","    )\n","  elif final_estimator_name == 'GradientBoostingRegressor':\n","    final_estimator = GradientBoostingRegressor(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'XGBRegressor':\n","    final_estimator = XGBRegressor(\n","        objective='reg:squarederror', n_jobs=N_JOBS, random_state=RANDOM_STATE\n","    )\n","  elif final_estimator_name == 'LGBMRegressor':\n","    final_estimator = LGBMRegressor(\n","        objective='regression',\n","        n_jobs=N_JOBS,\n","        random_state=RANDOM_STATE,\n","        verbose=-1\n","    )\n","  else:\n","    # For other regressors, use default parameters\n","    for name, regressor in get_regression_models():\n","      if name == final_estimator_name:\n","        final_estimator = regressor\n","        break\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx],\n","        X_train_full.iloc[val_idx]\n","    )\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Add Gaussian noise to training data if selected\n","    if add_noise:\n","      noise = np.random.normal(0, noise_std, X_train.shape)\n","\n","      X_train_noisy = pd.DataFrame(\n","          X_train + noise, columns=X_train.columns, index=X_train.index\n","      )\n","    else:\n","      X_train_noisy = X_train\n","\n","    # Initialize StackingRegressor\n","    model = StackingRegressor(\n","        estimators=estimators,\n","        final_estimator=final_estimator,\n","        n_jobs=N_JOBS,\n","        cv=5,\n","        passthrough=False\n","    )\n","\n","    # Fit StackingRegressor\n","    try:\n","      model.fit(X_train_noisy, y_train)\n","    except ValueError as e:\n","      print(f\"Error fitting model with {final_estimator_name}: {e}\")\n","      raise optuna.TrialPruned(f\"Failed to fit {final_estimator_name}: {e}\")\n","\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model, X_train_noisy\n","\n","    if add_noise:\n","      del noise\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      load_if_exists=True,\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      )\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target=(\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters\n","  best_params = study.best_params\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Prepare final estimator for final model with default parameters\n","  final_estimator_name = best_params['final_estimator']\n","\n","  if final_estimator_name in excluded_estimators:\n","    raise ValueError(\n","        f\"Best final estimator {final_estimator_name} is excluded.\"\n","    )\n","  if final_estimator_name == 'LinearRegression':\n","    final_estimator = LinearRegression()\n","  elif final_estimator_name == 'Ridge':\n","    final_estimator = Ridge(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'Lasso':\n","    final_estimator = Lasso(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'ElasticNet':\n","    final_estimator = ElasticNet(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'SVR':\n","    final_estimator = SVR()\n","  elif final_estimator_name == 'LinearSVR':\n","    final_estimator = LinearSVR(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'HuberRegressor':\n","    final_estimator = HuberRegressor()\n","  elif final_estimator_name == 'RandomForestRegressor':\n","    final_estimator = RandomForestRegressor(\n","        random_state=RANDOM_STATE, n_jobs=N_JOBS\n","    )\n","  elif final_estimator_name == 'GradientBoostingRegressor':\n","    final_estimator = GradientBoostingRegressor(random_state=RANDOM_STATE)\n","  elif final_estimator_name == 'XGBRegressor':\n","    final_estimator = XGBRegressor(\n","        objective='reg:squarederror', n_jobs=N_JOBS, random_state=RANDOM_STATE\n","    )\n","  elif final_estimator_name == 'LGBMRegressor':\n","    final_estimator = LGBMRegressor(\n","        objective='regression',\n","        n_jobs=N_JOBS,\n","        random_state=RANDOM_STATE,\n","        verbose=-1\n","    )\n","  else:\n","    # For other regressors, use default parameters\n","    for name, regressor in get_regression_models():\n","      if name == final_estimator_name:\n","        final_estimator = regressor\n","        break\n","\n","  # Train final StackingRegressor\n","  final_model = StackingRegressor(\n","      estimators=estimators,\n","      final_estimator=final_estimator,\n","      n_jobs=N_JOBS,\n","      cv=5,\n","      passthrough=False\n","  )\n","\n","  # Add noise to final training data if selected\n","  if best_params['add_noise']:\n","    noise = np.random.normal(0, best_params['noise_std'], X_final.shape)\n","\n","    X_final = pd.DataFrame(\n","        X_final + noise, columns=X_final.columns, index=X_final.index\n","    )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","\n","  mlflow.sklearn.log_model(final_model, name=\"StackingRegressor\")\n","\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  evaluate_and_log_metrics(y_val, y_pred, prefix=\"val\")\n","\n","  # SHAP analysis\n","  shap_analysis(final_model, X_final, model_type=\"kernel\")"],"metadata":{"id":"-rYHdENicLeq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## StackingRegressor - LassoLarsCV"],"metadata":{"id":"ejGpTHyC70xy"}},{"cell_type":"code","source":["# Initialize base estimators with type-converted parameters\n","best_params_linear_svr = get_best_params(\"LinearSVR\")\n","loss = best_params_linear_svr['loss']\n","dual = convert_param(best_params_linear_svr['dual'], 'bool', 'dual')\n","\n","if loss == 'epsilon_insensitive' and not dual:\n","  dual = True  # Force dual=True for epsilon_insensitive to avoid ValueError\n","\n","linear_svr = LinearSVR(\n","    C=convert_param(best_params_linear_svr['C'], 'float', 'C'),\n","    epsilon=convert_param(\n","        best_params_linear_svr['epsilon'],\n","        'float',\n","        'epsilon'\n","    ),\n","    loss=loss,\n","    max_iter=convert_param(\n","        best_params_linear_svr['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    tol=convert_param(best_params_linear_svr['tol'], 'float', 'tol'),\n","    dual=dual,\n","    intercept_scaling=convert_param(\n","        best_params_linear_svr['intercept_scaling'],\n","        'float',\n","        'intercept_scaling'\n","    ),\n","    random_state=RANDOM_STATE\n",")\n","\n","best_params_huber_regressor = get_best_params(\"HuberRegressor\")\n","\n","huber_regressor = HuberRegressor(\n","    epsilon=convert_param(\n","        best_params_huber_regressor['epsilon'],\n","        'float',\n","        'epsilon'\n","    ),\n","    alpha=convert_param(best_params_huber_regressor['alpha'], 'float', 'alpha'),\n","\n","    max_iter=convert_param(\n","        best_params_huber_regressor['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    tol=convert_param(best_params_huber_regressor['tol'], 'float', 'tol')\n",")\n","\n","best_params_bagging_regressor = get_best_params(\"BaggingRegressor\")\n","\n","bagging_regressor = BaggingRegressor(\n","    estimator=HuberRegressor(),\n","    n_estimators=convert_param(\n","        best_params_bagging_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    max_samples=convert_param(\n","        best_params_bagging_regressor['max_samples'],\n","        'float',\n","        'max_samples'\n","    ),\n","    max_features=convert_param(\n","        best_params_bagging_regressor['max_features'],\n","        'float',\n","        'max_features'\n","    ),\n","    bootstrap=convert_param(\n","        best_params_bagging_regressor['bootstrap'],\n","        'bool',\n","        'bootstrap'\n","    ),\n","    bootstrap_features=convert_param(\n","        best_params_bagging_regressor['bootstrap_features'],\n","        'bool',\n","        'bootstrap_features'\n","    ),\n","    random_state=RANDOM_STATE,\n","    n_jobs=N_JOBS\n",")\n","\n","best_params_hist_gradient_boosting_regressor = get_best_params(\n","    \"HistGradientBoostingRegressor\"\n",")\n","\n","hist_gradient_boosting_regressor = HistGradientBoostingRegressor(\n","    learning_rate=convert_param(\n","        best_params_hist_gradient_boosting_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_iter=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_iter'],\n","        'int',\n","        'max_iter'\n","    ),\n","    max_depth=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    min_samples_leaf=convert_param(\n","        best_params_hist_gradient_boosting_regressor['min_samples_leaf'],\n","        'int',\n","        'min_samples_leaf'\n","    ),\n","    l2_regularization=convert_param(\n","        best_params_hist_gradient_boosting_regressor['l2_regularization'],\n","        'float',\n","        'l2_regularization'\n","    ),\n","    max_bins=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_bins'],\n","        'int',\n","        'max_bins'\n","    ),\n","    max_leaf_nodes=convert_param(\n","        best_params_hist_gradient_boosting_regressor['max_leaf_nodes'],\n","        'int',\n","        'max_leaf_nodes'\n","    ),\n","    tol=convert_param(\n","        best_params_hist_gradient_boosting_regressor['tol'],\n","        'float',\n","        'tol'\n","    ),\n","    validation_fraction=convert_param(\n","        best_params_hist_gradient_boosting_regressor['validation_fraction'],\n","        'float',\n","        'validation_fraction'\n","    ),\n","    loss=best_params_hist_gradient_boosting_regressor['loss'],\n","    random_state=RANDOM_STATE,\n","    interaction_cst=best_params_hist_gradient_boosting_regressor[\n","        'interaction_cst'\n","    ],\n","    n_iter_no_change=convert_param(\n","        best_params_hist_gradient_boosting_regressor['n_iter_no_change'],\n","        'int',\n","        'n_iter_no_change'\n","    )\n",")\n","\n","best_params_nu_svr = get_best_params(\"NuSVR\")\n","\n","nu_svr = NuSVR(\n","    C=convert_param(best_params_nu_svr['C'], 'float', 'C'),\n","    kernel=best_params_nu_svr['kernel'],\n","    nu=convert_param(best_params_nu_svr['nu'], 'float', 'nu'),\n","    shrinking=convert_param(\n","        best_params_nu_svr['shrinking'],\n","        'bool',\n","        'shrinking'\n","    ),\n","    tol=convert_param(best_params_nu_svr['tol'], 'float', 'tol')\n",")\n","\n","best_params_xgb_regressor = get_best_params(\"XGBoostRegressor\")\n","best_params_xgb_regressor.pop('early_stopping_rounds', None)\n","\n","xgb_regressor = XGBRegressor(\n","    base_score=convert_param(\n","        best_params_xgb_regressor['base_score'],\n","        'float',\n","        'base_score'\n","    ),\n","    booster=best_params_xgb_regressor['booster'],\n","    colsample_bylevel=convert_param(\n","        best_params_xgb_regressor['colsample_bylevel'],\n","        'float',\n","        'colsample_bylevel'\n","    ),\n","    colsample_bytree=convert_param(\n","        best_params_xgb_regressor['colsample_bytree'],\n","        'float',\n","        'colsample_bytree'\n","    ),\n","    gamma=convert_param(best_params_xgb_regressor['gamma'], 'float', 'gamma'),\n","    grow_policy=best_params_xgb_regressor['grow_policy'],\n","    importance_type=best_params_xgb_regressor['importance_type'],\n","    learning_rate=convert_param(\n","        best_params_xgb_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_bin=convert_param(\n","        best_params_xgb_regressor['max_bin'],\n","        'int',\n","        'max_bin'\n","    ),\n","    max_delta_step=convert_param(\n","        best_params_xgb_regressor['max_delta_step'],\n","        'float',\n","        'max_delta_step'\n","    ),\n","    max_depth=convert_param(\n","        best_params_xgb_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    max_leaves=convert_param(\n","        best_params_xgb_regressor['max_leaves'],\n","        'int',\n","        'max_leaves'\n","    ),\n","    min_child_weight=convert_param(\n","        best_params_xgb_regressor['min_child_weight'],\n","        'float',\n","        'min_child_weight'\n","    ),\n","    n_estimators=convert_param(\n","        best_params_xgb_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    num_parallel_tree=convert_param(\n","        best_params_xgb_regressor['num_parallel_tree'],\n","        'int',\n","        'num_parallel_tree'\n","    ),\n","    reg_alpha=convert_param(\n","        best_params_xgb_regressor['reg_alpha'],\n","        'float',\n","        'reg_alpha'\n","    ),\n","    reg_lambda=convert_param(\n","        best_params_xgb_regressor['reg_lambda'],\n","        'float',\n","        'reg_lambda'\n","    ),\n","    subsample=convert_param(\n","        best_params_xgb_regressor['subsample'],\n","        'float',\n","        'subsample'\n","    ),\n","    tree_method=best_params_xgb_regressor['tree_method'],\n","    objective='reg:squarederror',\n","    eval_metric='rmse',\n","    n_jobs=N_JOBS,\n","    random_state=RANDOM_STATE,\n","    sampling_method='uniform'\n",")\n","\n","best_params_lgbm_regressor = get_best_params(\"LightGBMRegressor\")\n","\n","lgbm_regressor = LGBMRegressor(\n","    boosting_type=best_params_lgbm_regressor['boosting_type'],\n","    colsample_bytree=convert_param(\n","        best_params_lgbm_regressor['colsample_bytree'],\n","        'float',\n","        'colsample_bytree'\n","    ),\n","    importance_type=best_params_lgbm_regressor['importance_type'],\n","    learning_rate=convert_param(\n","        best_params_lgbm_regressor['learning_rate'],\n","        'float',\n","        'learning_rate'\n","    ),\n","    max_depth=convert_param(\n","        best_params_lgbm_regressor['max_depth'],\n","        'int',\n","        'max_depth'\n","    ),\n","    min_child_samples=convert_param(\n","        best_params_lgbm_regressor['min_child_samples'],\n","        'int',\n","        'min_child_samples'\n","    ),\n","    min_child_weight=convert_param(\n","        best_params_lgbm_regressor['min_child_weight'],\n","        'float',\n","        'min_child_weight'\n","    ),\n","    min_split_gain=convert_param(\n","        best_params_lgbm_regressor['min_split_gain'],\n","        'float',\n","        'min_split_gain'\n","    ),\n","    n_estimators=convert_param(\n","        best_params_lgbm_regressor['n_estimators'],\n","        'int',\n","        'n_estimators'\n","    ),\n","    num_leaves=convert_param(\n","        best_params_lgbm_regressor['num_leaves'],\n","        'int',\n","        'num_leaves'\n","    ),\n","    reg_alpha=convert_param(\n","        best_params_lgbm_regressor['reg_alpha'],\n","        'float',\n","        'reg_alpha'\n","    ),\n","    reg_lambda=convert_param(\n","        best_params_lgbm_regressor['reg_lambda'],\n","        'float',\n","        'reg_lambda'\n","    ),\n","    subsample=convert_param(\n","        best_params_lgbm_regressor['subsample'],\n","        'float',\n","        'subsample'\n","    ),\n","    subsample_for_bin=convert_param(\n","        best_params_lgbm_regressor['subsample_for_bin'],\n","        'int',\n","        'subsample_for_bin'\n","    ),\n","    subsample_freq=convert_param(\n","        best_params_lgbm_regressor['subsample_freq'],\n","        'int',\n","        'subsample_freq'\n","    ),\n","    n_jobs=N_JOBS,\n","    random_state=RANDOM_STATE,\n","    verbose=-1,\n","    objective='regression',\n","    metric='rmse'\n",")\n","\n","selected_features = [\n","    # 'publisher_avg_clicks_transformed_scaled',\n","    'market_popularity_transformed_scaled',\n","    'category_id_encoded_scaled',\n","    # 'industry_encoded_scaled',\n","    'publisher_encoded_scaled',\n","    'market_id_encoded_scaled',\n","    'day_of_week_encoded_scaled',\n","    'month_encoded_scaled' #,\n","    # 'is_weekend_encoded_scaled',\n","    # 'pub_popularity_x_pub_clicks',\n","    # 'market_clicks_x_market_popularity'\n","]\n","\n","X_train_full = df_train[selected_features]\n","X_val = df_val[selected_features]\n","\n","n_trials = 500\n","n_splits = 5\n","n_repeats = 3\n","mlflow_experiment = \"StackingRegressor_LassoLarsCV\"\n","current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","study_name = f\"{mlflow_experiment}_{current_time}\"\n","storage = f\"sqlite:///{PATH}/{study_name}.db\"\n","\n","# Define base estimators\n","estimators = [\n","    ('linear_svr', linear_svr),\n","    ('huber_regressor', huber_regressor),\n","    ('bagging_regressor', bagging_regressor),\n","    ('hist_gradient_boosting', hist_gradient_boosting_regressor),\n","    ('nu_svr', nu_svr),\n","    ('xgb_regressor', xgb_regressor),\n","    ('lgbm_regressor', lgbm_regressor)\n","]\n","\n","def objective(trial):\n","  # Tune winsorization limits\n","  winsorize_upper = trial.suggest_float('winsorize_upper', 0.75, 0.95)\n","\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-winsorize_upper)\n","  ))\n","\n","  # Tune whether to add noise\n","  add_noise = trial.suggest_categorical('add_noise', [True, False])\n","  noise_std = 0.0\n","\n","  if add_noise:\n","    noise_std = trial.suggest_float('noise_std', 0.001, 0.02, log=True)\n","\n","  # Tune LassoLarsCV parameters\n","  fit_intercept = trial.suggest_categorical(\n","      'lasso_fit_intercept', [True, False]\n","  )\n","\n","  max_iter = trial.suggest_int('lasso_max_iter', 500, 5000)\n","  max_n_alphas = trial.suggest_int('lasso_max_n_alphas', 100, 500)\n","  eps = trial.suggest_float('lasso_eps', 1e-6, 1e-5, log=True)\n","  positive = trial.suggest_categorical('lasso_positive', [True, False])\n","\n","  # Initialize LassoLarsCV as final estimator\n","  final_estimator = LassoLarsCV(\n","      fit_intercept=fit_intercept,\n","      max_iter=max_iter,\n","      max_n_alphas=max_n_alphas,\n","      n_jobs=N_JOBS,\n","      eps=eps,\n","      positive=positive\n","  )\n","\n","  # Repeated K-Fold Cross-Validation\n","  rkf = RepeatedKFold(\n","      n_splits=n_splits,\n","      n_repeats=n_repeats,\n","      random_state=RANDOM_STATE\n","  )\n","\n","  rmse_list = []\n","\n","  for fold_idx, (train_idx, val_idx) in enumerate(rkf.split(X_train_full)):\n","    X_train, X_val_fold = (\n","        X_train_full.iloc[train_idx],\n","        X_train_full.iloc[val_idx]\n","    )\n","    y_train, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    # Add Gaussian noise to training data if selected\n","    if add_noise:\n","      noise = np.random.normal(0, noise_std, X_train.shape)\n","\n","      X_train_noisy = pd.DataFrame(\n","          X_train + noise, columns=X_train.columns, index=X_train.index\n","      )\n","    else:\n","      X_train_noisy = X_train\n","\n","    # Initialize StackingRegressor\n","    model = StackingRegressor(\n","        estimators=estimators,\n","        final_estimator=final_estimator,\n","        n_jobs=N_JOBS,\n","        cv=5,\n","        passthrough=False\n","    )\n","\n","    # Fit StackingRegressor\n","    try:\n","      model.fit(X_train_noisy, y_train)\n","    except ValueError as e:\n","      print(f\"Error fitting model with LassoLarsCV: {e}\")\n","      raise optuna.TrialPruned(f\"Failed to fit LassoLarsCV: {e}\")\n","\n","    y_pred = model.predict(X_val_fold)\n","    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n","    rmse_list.append(rmse)\n","\n","    del X_train, X_val_fold, y_train, y_val_fold, y_pred, model, X_train_noisy\n","\n","    if add_noise:\n","      del noise\n","\n","    gc.collect()\n","\n","    trial.report(np.mean(rmse_list), step=fold_idx)\n","\n","    if trial.should_prune():\n","      raise optuna.TrialPruned()\n","\n","  return np.mean(rmse_list)\n","\n","# Start MLflow experiment\n","mlflow.set_experiment(mlflow_experiment)\n","\n","with mlflow.start_run():\n","  study = optuna.create_study(\n","      study_name=study_name,\n","      storage=storage,\n","      direction=\"minimize\",\n","      load_if_exists=True,\n","      pruner=optuna.pruners.HyperbandPruner(\n","          min_resource=max(1, n_splits),\n","          max_resource=n_splits * n_repeats,\n","          reduction_factor=2\n","      ),\n","      sampler=optuna.samplers.TPESampler(\n","          n_startup_trials=max(1, int(n_trials * 0.2)),\n","          seed=RANDOM_STATE,\n","          multivariate=True\n","      )\n","  )\n","\n","  study.optimize(objective, n_trials=n_trials)\n","  log_optuna_best_trial_search_space(study)\n","\n","  # Log Optuna storage details\n","  mlflow.log_param(\"optuna_storage\", storage)\n","  mlflow.log_param(\"optuna_study_name\", study_name)\n","\n","  # Log experiment parameters\n","  mlflow.log_param(\"n_trials\", n_trials)\n","  mlflow.log_param(\"n_splits\", n_splits)\n","  mlflow.log_param(\"n_repeats\", n_repeats)\n","\n","  # Log selected features\n","  mlflow.log_param(\"selected_features\", selected_features)\n","\n","  fanova_importances = get_param_importances(\n","      study,\n","      evaluator=FanovaImportanceEvaluator(seed=RANDOM_STATE),\n","      target= (\n","          lambda t: t.value\n","          if t.state == optuna.trial.TrialState.COMPLETE\n","          else None\n","      ),\n","      normalize=True\n","  )\n","\n","  fanova_importances = {k: round(v, 4) for k, v in fanova_importances.items()}\n","\n","  for param, importance in fanova_importances.items():\n","    mlflow.log_param(f\"fanova_{param}\", importance)\n","\n","  print(f'Fanova Hyperparameter Importances (rounded): {fanova_importances}')\n","\n","  # Get best parameters\n","  best_params = study.best_params\n","\n","  for param, value in best_params.items():\n","    mlflow.log_param(param, value)\n","\n","  mlflow.log_metric(\"best_cv_rmse\", study.best_value)\n","\n","  print(\"Best Params:\", best_params)\n","  print(\"Best CV RMSE:\", study.best_value)\n","\n","  # Apply best winsorization to final training and validation sets\n","  y_train_full = np.log1p(mstats.winsorize(\n","      df_train[TARGET].to_numpy(dtype=np.float64),\n","      limits=(0, 1-best_params['winsorize_upper'])\n","  ))\n","\n","  y_val = np.log1p(np.clip(\n","      df_val[TARGET],\n","      0,\n","      np.percentile(df_train[TARGET], best_params['winsorize_upper'] * 100)\n","  ))\n","\n","  X_final = X_train_full.copy()\n","\n","  # Prepare final estimator with best parameters\n","  final_estimator = LassoLarsCV(\n","      fit_intercept=best_params['lasso_fit_intercept'],\n","      max_iter=best_params['lasso_max_iter'],\n","      max_n_alphas=best_params['lasso_max_n_alphas'],\n","      n_jobs=N_JOBS,\n","      eps=best_params['lasso_eps'],\n","      positive=best_params['lasso_positive']\n","  )\n","\n","  # Train final StackingRegressor\n","  final_model = StackingRegressor(\n","      estimators=estimators,\n","      final_estimator=final_estimator,\n","      n_jobs=N_JOBS,\n","      cv=5,\n","      passthrough=False\n","  )\n","\n","  # Add noise to final training data if selected\n","  if best_params['add_noise']:\n","    noise = np.random.normal(0, best_params['noise_std'], X_final.shape)\n","\n","    X_final = pd.DataFrame(\n","        X_final + noise, columns=X_final.columns, index=X_final.index\n","    )\n","\n","  X_final = X_final[sorted(X_final.columns)]\n","  final_model.fit(X_final, y_train_full)\n","\n","  mlflow.sklearn.log_model(final_model, name=\"StackingRegressor\")\n","\n","  X_val = X_val[sorted(X_val.columns)]\n","  y_pred = final_model.predict(X_val)\n","\n","  y_val_original, y_pred_original = evaluate_and_log_metrics(\n","      y_val,\n","      y_pred,\n","      prefix=\"val\"\n","  )\n","\n","  # SHAP analysis\n","  shap_analysis(final_model, X_final, model_type=\"kernel\")"],"metadata":{"id":"36I6PRXU8F1e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Debugging"],"metadata":{"id":"excZkbpPFZfb"}},{"cell_type":"markdown","source":["## Learning Curve"],"metadata":{"id":"lrwvolorIwe0"}},{"cell_type":"code","source":["# Plot learning curves to diagnose whether the model suffers from high bias\n","# or high variance.\n","train_sizes, train_scores, val_scores = learning_curve(\n","    final_model,\n","    X_final,\n","    y_train_full,\n","    cv=5,\n","    scoring='neg_mean_squared_error',\n","    train_sizes=np.linspace(0.1, 1.0, 10)\n",")\n","\n","plt.plot(train_sizes, np.sqrt(-train_scores.mean(axis=1)), label='Train RMSE')\n","plt.plot(\n","    train_sizes,\n","    np.sqrt(-val_scores.mean(axis=1)),\n","    label='Validation RMSE'\n",")\n","plt.xlabel('Training Set Size')\n","plt.ylabel('RMSE')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"6QzYt1QJFbdh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature Interaction Analysis"],"metadata":{"id":"SA2B_cZ_I1Ng"}},{"cell_type":"code","source":["# Use SHAP's interaction values to understand\n","# how features interact.\n","shap_interaction_values = explainer.shap_interaction_values(X_final)\n","shap.summary_plot(shap_interaction_values, X_final, plot_type=\"compact_dot\")"],"metadata":{"id":"m9ifEZ_6FgGG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Outlier Inspection"],"metadata":{"id":"qtxZfnw7I8HI"}},{"cell_type":"code","source":["# Initialize SHAP explainer\n","explainer = shap.Explainer(final_model, X_final)\n","\n","# Compute SHAP values for the input data\n","shap_values = explainer(X_final)\n","\n","# Outlier Inspection: Examine instances with high SHAP values to identify\n","# potential outliers or anomalies driving poor generalization.\n","row_importance = np.abs(shap_values.values).sum(axis=1)\n","threshold = np.percentile(row_importance, 95)\n","high_impact_instances = X_final.iloc[row_importance > threshold]\n","\n","# This is a DataFrame containing only those rows (instances) from X_final\n","# where the model's prediction was heavily influenced by the\n","# input features - potentially outliers or anomalies.\n","print(high_impact_instances.shape)\n","\n","# You may decide to remove high-impact (outlier) instances from df_train to\n","# improve the model performance."],"metadata":{"id":"4mtJZZqbFkFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Performance Plots"],"metadata":{"id":"bJo3q3uoqcHh"}},{"cell_type":"code","source":["residuals = y_val_original - y_pred_original"],"metadata":{"id":"jII_zmFlqiDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Find experiment ID by name\n","experiment_name = \"StackingRegressor_LassoLarsCV\"\n","experiment = mlflow.get_experiment_by_name(experiment_name)\n","\n","if experiment is None:\n","  raise ValueError(f\"No experiment found with name: {experiment_name}\")\n","\n","experiment_id = experiment.experiment_id\n","\n","# Step 2: Search for a specific run\n","runs = mlflow.search_runs(\n","    experiment_ids=[experiment_id],\n","    order_by=[\"metrics.test_rmse_original ASC\"]\n",")\n","\n","# Choose the top run\n","run_id = runs.iloc[0][\"run_id\"]\n","\n","# Step 3: Load the model from the run\n","model_uri = f\"runs:/{run_id}/StackingRegressor\"\n","final_model = mlflow.sklearn.load_model(model_uri)"],"metadata":{"id":"-ffFBP5gQJd1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Actual vs Predicted Plot"],"metadata":{"id":"A_yWud2HqmmC"}},{"cell_type":"markdown","source":["Key for visually assessing prediction quality."],"metadata":{"id":"Hb6m0lRFlTvZ"}},{"cell_type":"code","source":["plt.figure(figsize=(8, 6))\n","\n","sns.scatterplot(\n","    x=y_test_original,\n","    y=y_pred_original,\n","    alpha=0.7,\n","    color=\"dodgerblue\",\n","    edgecolor=\"k\"\n",")\n","\n","plt.plot(\n","    [y_test_original.min(), y_test_original.max()],\n","    [y_test_original.min(), y_test_original.max()],\n","    'r--'\n",")  # 45-degree line\n","\n","plt.xlabel('Actual Values')\n","plt.ylabel('Predicted Values')\n","\n","plt.title(\n","    f'Actual vs. Predicted '\n","    f'(RMSE = {np.sqrt(mean_squared_error(y_test_original, y_pred_original)):.2f})'\n",")\n","\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"eDFjXu_hksiO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Residual Plot (Residuals vs Predicted)"],"metadata":{"id":"UGwY7OB1q0mX"}},{"cell_type":"markdown","source":["Crucial for checking heteroscedasticity and non-linearity."],"metadata":{"id":"n4UPhW7AlYNh"}},{"cell_type":"code","source":["# Plot\n","plt.figure(figsize=(8, 6))\n","\n","sns.scatterplot(\n","    x=y_pred_original,\n","    y=residuals,\n","    color='darkorange',\n","    edgecolor='k',\n","    alpha=0.7\n",")\n","\n","plt.axhline(0, linestyle='--', color='red')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals (Actual - Predicted)')\n","plt.title('Residuals vs. Predicted Values')\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Q1PUM0T6o421"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q-Q Plot (Quantile-Quantile Plot)"],"metadata":{"id":"4JVZjxWUs-5B"}},{"cell_type":"markdown","source":["Best way to check if residuals are normally distributed."],"metadata":{"id":"pHcLs8RKlbNh"}},{"cell_type":"code","source":["plt.figure(figsize=(8, 6))\n","stats.probplot(residuals, dist=\"norm\", plot=plt)\n","plt.title(\"Q-Q Plot of Residuals\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"GSgZqipQsxnJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bar Plot of Error Metrics"],"metadata":{"id":"0aSsrd_oIjCY"}},{"cell_type":"markdown","source":["Clear summary of model performance across metrics."],"metadata":{"id":"9pil88BJlig5"}},{"cell_type":"code","source":["# Define actual and predicted values\n","y_train = df_train[TARGET]\n","X_train = df_train[selected_features]\n","y_test = y_test_original\n","y_pred = y_pred_original\n","\n","# Calculate metrics for train\n","mae_train = mean_absolute_error(y_train, final_model.predict(X_train))\n","rmse_train = np.sqrt(mean_squared_error(y_train, final_model.predict(X_train)))\n","r2_train = r2_score(y_train, final_model.predict(X_train))\n","\n","# Calculate metrics for test\n","mae_test = mean_absolute_error(y_test, y_pred)\n","rmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\n","r2_test = r2_score(y_test, y_pred)\n","\n","# Store metrics\n","metrics = ['MAE', 'RMSE', 'R2']\n","train_scores = [mae_train, rmse_train, r2_train]\n","test_scores = [mae_test, rmse_test, r2_test]\n","\n","# Compute percentage differences\n","percentage_diff = [\n","    (\n","        ((test - train) / train * 100)\n","        if metric != 'R2'\n","        else ((test - train) / abs(train) * 100)\n","    )\n","    for train, test, metric in zip(train_scores, test_scores, metrics)\n","]\n","\n","# Create a DataFrame for display\n","df_metrics = pd.DataFrame({\n","    'Metric': metrics,\n","    'Train': train_scores,\n","    'Test': test_scores,\n","    'Test vs Train % Diff': [f\"{diff:+.2f}%\" for diff in percentage_diff]\n","})\n","\n","# Print nicely\n","print(df_metrics.to_string(index=False))\n","\n","# Plot\n","x = np.arange(len(metrics))\n","width = 0.35\n","\n","plt.figure(figsize=(8, 6))\n","plt.bar(x - width/2, train_scores, width, label='Train', color='skyblue')\n","plt.bar(x + width/2, test_scores, width, label='Test', color='salmon')\n","\n","plt.xticks(x, metrics)\n","plt.ylabel('Score')\n","plt.title('Error Metrics Comparison (Train vs Test)')\n","plt.legend()\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"2J6o7L7ionDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Partial Dependence Plots (PDP)"],"metadata":{"id":"7k1_P2ErRrfQ"}},{"cell_type":"markdown","source":["Great for model interpretability, showing marginal effects of features."],"metadata":{"id":"Oi69IH0zlkjK"}},{"cell_type":"code","source":["X_val = df_val[selected_features]\n","\n","# Plot PDP for one or more features\n","features = selected_features\n","PartialDependenceDisplay.from_estimator(final_model, X_val, features)"],"metadata":{"id":"EF1hm_fNRWFi"},"execution_count":null,"outputs":[]}]}